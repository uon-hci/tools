{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Courses list Autumn Compulsory courses Course Credits Design Ethnography 20 Studying Human Performance 20 Optional courses Course Credits Cognitive Ergonomics in Design 10 Simulation and Digital Human Modeling 10 Spring Compulsory courses Course Credits Human-Computer Systems 10 Contemporary Issues in Human Factors and Interactive Systems 10 Mixed Reality Technologies 20 Optional courses Course Credits Fundamentals of Information Visualisation 10 Information Visualisation Project 10 Summer Course Credits Individual Project: Human-Computer Interaction 60","title":"Home"},{"location":"#courses-list","text":"","title":"Courses list"},{"location":"#autumn","text":"","title":"Autumn"},{"location":"#compulsory-courses","text":"Course Credits Design Ethnography 20 Studying Human Performance 20","title":"Compulsory courses"},{"location":"#optional-courses","text":"Course Credits Cognitive Ergonomics in Design 10 Simulation and Digital Human Modeling 10","title":"Optional courses"},{"location":"#spring","text":"","title":"Spring"},{"location":"#compulsory-courses_1","text":"Course Credits Human-Computer Systems 10 Contemporary Issues in Human Factors and Interactive Systems 10 Mixed Reality Technologies 20","title":"Compulsory courses"},{"location":"#optional-courses_1","text":"Course Credits Fundamentals of Information Visualisation 10 Information Visualisation Project 10","title":"Optional courses"},{"location":"#summer","text":"Course Credits Individual Project: Human-Computer Interaction 60","title":"Summer"},{"location":"CEID-course/","text":"Cognitive Ergonomics in Design COURSE Introduction The origins of ergonomics relate to fitting the task to the person Cognitive ergonomics is about considering the impact of how people think, reason, understand and act in design of all aspects of life, and use it to make better decisions when creating products. Taking cognitive factors into account can prevent incidents and errors in systems. The example of the Three Mile Island incident demontrastes the important of human factors. In march 1979, bad design and bad user interfaces lead to human errors and then a nuclear incident. It was the most significant accident in the U.S commercial nuclear power plant history (see chapter Human error and automation for more details). The course is outlined with the following chapters: Memory and attention Human workload Situation awareness Mental models Human error and automation Decision making and expertise Joint cognitive systems Memory Memory structures When trying to describe how the memory works, two types of structures can be noted: Spatial : memories are stored in a specific location, and remembering is a retrieval process involving a specific spatial search through the mind, Parallel distributed processing : memories stored in the form of connections among units and not stored in a single place. Memory models More precisely, the memory presents many states, components and other elements that makes it more complex. Psychologist have been trying to explain it through diverse models for many years. However critized, these models have influenced memory research. Multi-store model Proposed in 1968 by Richard Atkinson and Richard Siffrin , the multi-store model (or modal model ) asserts that the memory is split in three separate stores: Sensory , Short-term and Long-term . The sensory stores describes the very brief availibility of memories related to envionmental stimulus. While it is generally agreed that there is a sensory store for each sense, most of the research in the area has focused on the visual and auditory systems. The iconic store refers to the visual system (delay of ~0.5 seconds, limited to the field of vision) and the echoic store focuses on the auditory system (delay of ~2 seconds). Unless attended, most of the information in sensory stores decays and is quickly forgotten. When sensory memory is attended, it is transferred to the short-term store. This store holds information for a longer duration, but has a very limited capacity: seven, plus or minus two items. However the short-term memory is susceptible to loss of information when distracted. The duration can be extended if the information is rehearsed . If that information is rehearsed thoroughly for a longer period of time, it is transferred to the long-term store. This store is more or less a permanent store that can hold information over extremely long periods of time. It is in fact assumed to be nearly limitless in its duration and capacity. Information stored there can be transferred back to the short-term store where it can be attended to and manipulated. Criticism : the modal model has been intensively critized over the following aspects: Over simplified, Evidence that the short-term probably behaves differently for different senses, Fails to explain how information is really stored in the long-term store, Focuses more on the structure instead of the actual process involved in memory and learning. Working memory model Proposed in 1974 by Alan Baddely and Graham Hitch proposed the working memory model in attempt to present a more accurate model than the modal model by introducing a three part working memory replacing the short-term store. In 2000, they added a fourth component in the model. These components are: Central executive : acts as a supervisory system controlling the flow of information from and to its slave systems: the phonological loop and the visuo-spatial sketchpad, Phonological loop : stores information in a phonological form (speech-based, verbal content), Visuo-visual sketch : stores the spatial and/or visual data, Episodic buffer : conjoins information from the phonological loop and the visuo-visual sketch (visual, spatial and verbal information) with long-term memory (chronological information) into a single episodic representation (e.g. the memory of a story or a movie scene). Advantages : Explains real world memory tasks (e.g. mental arithmetic, verbal reasoning), Explains better the experiences of brain damaged patients, Less focused on verbal rehearsal for retention. Disadvantage : Difficulty to measure the capacity of the central executive. Long-term memory The long-term memory does not store memories in one unified structure, as might be seen in a computer's hard disk drive. Instead, the long-term memory is typically divided into two major components: explicit memory and implicit memory. Explicit memory Also called the declarative memory , the explicit memory refers to all memories that are consciously available. It is the conscious, intentional recollection of factual information, previous experiences and concepts. People use explicit memory throughout the day, such as remembering the time of an appointment or recollecting an event from years ago. The explicit memory is divided in four major memory types: Episodic : storage and recollection of observational information attached to specific life-events ( someone's name , the memory of watching a movie , the memory of meeting someone ), Semantic : general world knowledge (facts, ideas, meaning and concepts) that can be articulated and is independant of personal experience ( languages , structures , classifications , objects ), Autobiographical : combination of episodic and semantic memory to contain the information on what the self is, what the self was, and what the self can be, Spatial : memory responsible for the recording of information about the environment and spatial orientation ( navigation , recognition of familiar places , map reading ). Implicit memory Also called the procedural memory , the implicity memory refers to the use of objects, or the movements of the body, generally to skills. It is acquired and used unconsciously, and can affect thoughts and behaviours. People use implicit memory every day such as tying their shoes or riding a bicycle, without consciously thinking about these activities. False memories Evidence and studies have shown that although people are highly confident in their memories, the details of the memories can be forgotten and altered. A false memory is a phenomenon where a person recalls something that did not happen or differently from the way it happened. This phenomenon was initially investigated by the psychological pioneers Pierre Janet and Sigmund Freud . Elizabeth Loftus has since been a lead researcher in memory recovery and false memories. False memories can be created by suggestibility, activation of associated information, the incorporation of misinformation or source misattribution. A common example to explain false memories is the case of the car crash and the distorted perceptions of it. In 1974, Loftus and Palmer conducted an experiment where they showed participants the video of a car crash. Later, they were asked questions about that same accident. The questions were asked differently to each participants, where for example a word in the question would change: \" About how fast were the cars going when they smashed/collided/bumped/hit/contacted each other? \". The results showed that the estimated speed was affected by the verb used. In addition, a week later, a question about the presence of broken glass on the accident was asked, and revealed that more participants that were asked the first question with the verb \" smashed \" remembered broken glass, although there was none. Flashbulb memory A flashbulb memory is highly detailed, exceptionnally vivid \" snapshot \" of the moment and circumstances in which a piece of surprising and consequential news was heard. This type of memory include six main characteristic features: place , ongoing activity , informant , own effect , other effect and aftermath . Generally, the determinants of a flashbulb memory are a high level of surprise, a high level of consequentiality, and perhaps emotional arousal. An example of such a memory is the generally very detailed memories of the events of 9/11, and how many people remember exactly what they were doing the moment they learned about these events. Flashbulb memories however remain a controversial idea among psychologists, with some believing that these memories are not different from any other autobiographical memory because they rely on elements of personal importance, consequentiality, emotion and surprise. Other researchers believe it is an entirely distinct type of memory that forms differently and perhaps in different parts of the brain. Misplacing objects Misplacing objects is something that happens very often to most individuals (Sarah Brill if you read this, big up to your Burgerking experience). Four main cognitive errors leading to misplaced objects are: Absent-mindness : mental condition in which a person experiences low levels of attention and frequent distraction (object put in unusual places), Updating errors : when a person cannot remember which of several usual places an object is in, Detection failures : when a person has an object in front of them but can't detect it, Context effects : distortions from other elements within the environment. Common techniques to locate missing objects are: Action replay : reconstruct sequence of actions, Mental walk : visualise possible object locations, Reality monitoring : generate images of placing the object in various locations and decide whether these correspond to reality, Physical search . Attention Attention is the behavioral and cognitive process of selectively concentrating on a discrete aspect of information, whether deemed subjective or objective, while ignoring other perceivable information. Also described as the allocation of limited cognitive processing resources, attention remains a major area of investigation to determine the source of the sensor cues and signals that generate attention and the relationship between ofhter concepts like working memory and psychological vigilance. Types of attention Attention is commonly split into four different types: Selective : ability to direct attention on several sources of information to determine whether a particular event has occured ( a doctor reading a hospital screen ), Focused : ability to direct attention on a single source of information without interruption or interference from either external or internal factors or stimuli ( a tennis court supervisor ), Divided : ability to direct attention to two or more separated tasks performed simultaneously ( cooking a meal ), Sustained : ability to continuously maintain focus on a task or event over a long period of time ( monitoring camera screen for security measures ). Attention models Broadbent's filter model In 1958, Broadbent proposed an attention model that postulates that physicial characteristics of messages are used to select one message for further processing and that all others are lost. This models proposes that information from all the stimuli at any given time enters an unilimited capacity sensory buffer, and is then filtered based on physical characteristics ( pitch , color , loudness , direction ). Because the human brain has a limited capacity to process information, this filter is designed to prevent the information-processing system from becoming overloaded. According to Broadbent the meaning of any of the messages is not taken into account at all by the filter. All semantic processing is carried out after the filter has selected the message to pay attention to. So every message that is not attended would not be understood. Criticism : In addition to several criticism regarding the experiments that lead to the Broadbent's model (possible other explanations than the filter theory), a major issue with this model is that it does not account for the Cocktail Party Effect , because unattended messages are filtered out before the meaning can be processed (hearing someone call your name when not focused on it should then be impossible). Treisman's attenuation model In 1964, Treisman completes Broadbent's theory by specifying that the filter attenuates rather than eliminates the unattended material. This attenuation is like turning down the volume, so that if you have 4 sources of sound in one room, you can turn down or attenuate 3 in order to attend the fourth. This means that people can still process the meaning of unattended messages. Criticism : This model overcomes some of the Broadbent's model (the Cocktail Party Effect), but has been criticized regarding the lack of precision on the nature of the attenuation process and Treisman does not explain how exactly semantic analysis work. Late selection model In 1963, Deutsch Deutsch proposed a model where all stimuli get processed in full, with the most important or relevant stimulus determining the response. In other words, they suggested that the selection does not occus on the basis of an early-selection filter, but after stimuli have already been identified. This theory hence locates the attentional filter later in the processing, after which all material processed upto this point ad judged to be most important is elaborated more fully. Criticism : The main criticism received for this theory is how wasteful it appears, with its thorough processing of all information before selection of admittance into working memory. Capacity model In 1973, Kahneman proposed a theory which is based around the idea of mental efforts. It proposes that some activities are more demanding and therefore require more mental effort than others. Kahneman thus believes in the existence of a Central Processor which operates a Central Allocation Policy, constantly evaluating the demands made by each task and adjusting attention accordingly. In addition, the total available processing capacities could be increased or decreased by factors such as arousal Criticism : The main criticism for this theory ist that by developing skills, it would become impossible to accurately evaluate the demands required for each task and hence adjust attention accordingly. Perceptual load model In 1995, Lavie presented the perceptual load theory as a potential resolution to the early/late selection debate. Lavie attempts to resolve this debate by stating that both early and late selection occur varying on the stimulus presented. This variation in stimilus is explained as a notion of high or low perceptual load. This perceptual load refers to the complexity of the physical stimuli (particularly the distractor stimuli). Because Lavie gives the assumption that all of the attentional resources naturally have to be used up, a low load task would process more of the distractors to exhaust mental resources, and therefore the distractors would cause a greatee inference. On the other hand, in high load situations, as all the attentional resources are used up, distractors would cause less inference, if not at all. In other words, if the task-relevant stimulus uses all of the attentional resources, then none of the task-irrelevant stimuli (distractors) will be processed. Criticism : This theory has criticized regarding several points, notably how the notion of perceptual load is in fact dilution, and that this theory is not a solution t the early/late debate. A main critique is also how a visual cue can eliminate the inference effect supposedly created by perceptual load.","title":"CEID"},{"location":"CEID-course/#cognitive-ergonomics-in-design","text":"COURSE","title":"Cognitive Ergonomics in Design"},{"location":"CEID-course/#introduction","text":"The origins of ergonomics relate to fitting the task to the person Cognitive ergonomics is about considering the impact of how people think, reason, understand and act in design of all aspects of life, and use it to make better decisions when creating products. Taking cognitive factors into account can prevent incidents and errors in systems. The example of the Three Mile Island incident demontrastes the important of human factors. In march 1979, bad design and bad user interfaces lead to human errors and then a nuclear incident. It was the most significant accident in the U.S commercial nuclear power plant history (see chapter Human error and automation for more details). The course is outlined with the following chapters: Memory and attention Human workload Situation awareness Mental models Human error and automation Decision making and expertise Joint cognitive systems","title":"Introduction"},{"location":"CEID-course/#memory","text":"","title":"Memory"},{"location":"CEID-course/#memory-structures","text":"When trying to describe how the memory works, two types of structures can be noted: Spatial : memories are stored in a specific location, and remembering is a retrieval process involving a specific spatial search through the mind, Parallel distributed processing : memories stored in the form of connections among units and not stored in a single place.","title":"Memory structures"},{"location":"CEID-course/#memory-models","text":"More precisely, the memory presents many states, components and other elements that makes it more complex. Psychologist have been trying to explain it through diverse models for many years. However critized, these models have influenced memory research.","title":"Memory models"},{"location":"CEID-course/#multi-store-model","text":"Proposed in 1968 by Richard Atkinson and Richard Siffrin , the multi-store model (or modal model ) asserts that the memory is split in three separate stores: Sensory , Short-term and Long-term . The sensory stores describes the very brief availibility of memories related to envionmental stimulus. While it is generally agreed that there is a sensory store for each sense, most of the research in the area has focused on the visual and auditory systems. The iconic store refers to the visual system (delay of ~0.5 seconds, limited to the field of vision) and the echoic store focuses on the auditory system (delay of ~2 seconds). Unless attended, most of the information in sensory stores decays and is quickly forgotten. When sensory memory is attended, it is transferred to the short-term store. This store holds information for a longer duration, but has a very limited capacity: seven, plus or minus two items. However the short-term memory is susceptible to loss of information when distracted. The duration can be extended if the information is rehearsed . If that information is rehearsed thoroughly for a longer period of time, it is transferred to the long-term store. This store is more or less a permanent store that can hold information over extremely long periods of time. It is in fact assumed to be nearly limitless in its duration and capacity. Information stored there can be transferred back to the short-term store where it can be attended to and manipulated. Criticism : the modal model has been intensively critized over the following aspects: Over simplified, Evidence that the short-term probably behaves differently for different senses, Fails to explain how information is really stored in the long-term store, Focuses more on the structure instead of the actual process involved in memory and learning.","title":"Multi-store model"},{"location":"CEID-course/#working-memory-model","text":"Proposed in 1974 by Alan Baddely and Graham Hitch proposed the working memory model in attempt to present a more accurate model than the modal model by introducing a three part working memory replacing the short-term store. In 2000, they added a fourth component in the model. These components are: Central executive : acts as a supervisory system controlling the flow of information from and to its slave systems: the phonological loop and the visuo-spatial sketchpad, Phonological loop : stores information in a phonological form (speech-based, verbal content), Visuo-visual sketch : stores the spatial and/or visual data, Episodic buffer : conjoins information from the phonological loop and the visuo-visual sketch (visual, spatial and verbal information) with long-term memory (chronological information) into a single episodic representation (e.g. the memory of a story or a movie scene). Advantages : Explains real world memory tasks (e.g. mental arithmetic, verbal reasoning), Explains better the experiences of brain damaged patients, Less focused on verbal rehearsal for retention. Disadvantage : Difficulty to measure the capacity of the central executive.","title":"Working memory model"},{"location":"CEID-course/#long-term-memory","text":"The long-term memory does not store memories in one unified structure, as might be seen in a computer's hard disk drive. Instead, the long-term memory is typically divided into two major components: explicit memory and implicit memory.","title":"Long-term memory"},{"location":"CEID-course/#explicit-memory","text":"Also called the declarative memory , the explicit memory refers to all memories that are consciously available. It is the conscious, intentional recollection of factual information, previous experiences and concepts. People use explicit memory throughout the day, such as remembering the time of an appointment or recollecting an event from years ago. The explicit memory is divided in four major memory types: Episodic : storage and recollection of observational information attached to specific life-events ( someone's name , the memory of watching a movie , the memory of meeting someone ), Semantic : general world knowledge (facts, ideas, meaning and concepts) that can be articulated and is independant of personal experience ( languages , structures , classifications , objects ), Autobiographical : combination of episodic and semantic memory to contain the information on what the self is, what the self was, and what the self can be, Spatial : memory responsible for the recording of information about the environment and spatial orientation ( navigation , recognition of familiar places , map reading ).","title":"Explicit memory"},{"location":"CEID-course/#implicit-memory","text":"Also called the procedural memory , the implicity memory refers to the use of objects, or the movements of the body, generally to skills. It is acquired and used unconsciously, and can affect thoughts and behaviours. People use implicit memory every day such as tying their shoes or riding a bicycle, without consciously thinking about these activities.","title":"Implicit memory"},{"location":"CEID-course/#false-memories","text":"Evidence and studies have shown that although people are highly confident in their memories, the details of the memories can be forgotten and altered. A false memory is a phenomenon where a person recalls something that did not happen or differently from the way it happened. This phenomenon was initially investigated by the psychological pioneers Pierre Janet and Sigmund Freud . Elizabeth Loftus has since been a lead researcher in memory recovery and false memories. False memories can be created by suggestibility, activation of associated information, the incorporation of misinformation or source misattribution. A common example to explain false memories is the case of the car crash and the distorted perceptions of it. In 1974, Loftus and Palmer conducted an experiment where they showed participants the video of a car crash. Later, they were asked questions about that same accident. The questions were asked differently to each participants, where for example a word in the question would change: \" About how fast were the cars going when they smashed/collided/bumped/hit/contacted each other? \". The results showed that the estimated speed was affected by the verb used. In addition, a week later, a question about the presence of broken glass on the accident was asked, and revealed that more participants that were asked the first question with the verb \" smashed \" remembered broken glass, although there was none.","title":"False memories"},{"location":"CEID-course/#flashbulb-memory","text":"A flashbulb memory is highly detailed, exceptionnally vivid \" snapshot \" of the moment and circumstances in which a piece of surprising and consequential news was heard. This type of memory include six main characteristic features: place , ongoing activity , informant , own effect , other effect and aftermath . Generally, the determinants of a flashbulb memory are a high level of surprise, a high level of consequentiality, and perhaps emotional arousal. An example of such a memory is the generally very detailed memories of the events of 9/11, and how many people remember exactly what they were doing the moment they learned about these events. Flashbulb memories however remain a controversial idea among psychologists, with some believing that these memories are not different from any other autobiographical memory because they rely on elements of personal importance, consequentiality, emotion and surprise. Other researchers believe it is an entirely distinct type of memory that forms differently and perhaps in different parts of the brain.","title":"Flashbulb memory"},{"location":"CEID-course/#misplacing-objects","text":"Misplacing objects is something that happens very often to most individuals (Sarah Brill if you read this, big up to your Burgerking experience). Four main cognitive errors leading to misplaced objects are: Absent-mindness : mental condition in which a person experiences low levels of attention and frequent distraction (object put in unusual places), Updating errors : when a person cannot remember which of several usual places an object is in, Detection failures : when a person has an object in front of them but can't detect it, Context effects : distortions from other elements within the environment. Common techniques to locate missing objects are: Action replay : reconstruct sequence of actions, Mental walk : visualise possible object locations, Reality monitoring : generate images of placing the object in various locations and decide whether these correspond to reality, Physical search .","title":"Misplacing objects"},{"location":"CEID-course/#attention","text":"Attention is the behavioral and cognitive process of selectively concentrating on a discrete aspect of information, whether deemed subjective or objective, while ignoring other perceivable information. Also described as the allocation of limited cognitive processing resources, attention remains a major area of investigation to determine the source of the sensor cues and signals that generate attention and the relationship between ofhter concepts like working memory and psychological vigilance.","title":"Attention"},{"location":"CEID-course/#types-of-attention","text":"Attention is commonly split into four different types: Selective : ability to direct attention on several sources of information to determine whether a particular event has occured ( a doctor reading a hospital screen ), Focused : ability to direct attention on a single source of information without interruption or interference from either external or internal factors or stimuli ( a tennis court supervisor ), Divided : ability to direct attention to two or more separated tasks performed simultaneously ( cooking a meal ), Sustained : ability to continuously maintain focus on a task or event over a long period of time ( monitoring camera screen for security measures ).","title":"Types of attention"},{"location":"CEID-course/#attention-models","text":"","title":"Attention models"},{"location":"CEID-course/#broadbents-filter-model","text":"In 1958, Broadbent proposed an attention model that postulates that physicial characteristics of messages are used to select one message for further processing and that all others are lost. This models proposes that information from all the stimuli at any given time enters an unilimited capacity sensory buffer, and is then filtered based on physical characteristics ( pitch , color , loudness , direction ). Because the human brain has a limited capacity to process information, this filter is designed to prevent the information-processing system from becoming overloaded. According to Broadbent the meaning of any of the messages is not taken into account at all by the filter. All semantic processing is carried out after the filter has selected the message to pay attention to. So every message that is not attended would not be understood. Criticism : In addition to several criticism regarding the experiments that lead to the Broadbent's model (possible other explanations than the filter theory), a major issue with this model is that it does not account for the Cocktail Party Effect , because unattended messages are filtered out before the meaning can be processed (hearing someone call your name when not focused on it should then be impossible).","title":"Broadbent's filter model"},{"location":"CEID-course/#treismans-attenuation-model","text":"In 1964, Treisman completes Broadbent's theory by specifying that the filter attenuates rather than eliminates the unattended material. This attenuation is like turning down the volume, so that if you have 4 sources of sound in one room, you can turn down or attenuate 3 in order to attend the fourth. This means that people can still process the meaning of unattended messages. Criticism : This model overcomes some of the Broadbent's model (the Cocktail Party Effect), but has been criticized regarding the lack of precision on the nature of the attenuation process and Treisman does not explain how exactly semantic analysis work.","title":"Treisman's attenuation model"},{"location":"CEID-course/#late-selection-model","text":"In 1963, Deutsch Deutsch proposed a model where all stimuli get processed in full, with the most important or relevant stimulus determining the response. In other words, they suggested that the selection does not occus on the basis of an early-selection filter, but after stimuli have already been identified. This theory hence locates the attentional filter later in the processing, after which all material processed upto this point ad judged to be most important is elaborated more fully. Criticism : The main criticism received for this theory is how wasteful it appears, with its thorough processing of all information before selection of admittance into working memory.","title":"Late selection model"},{"location":"CEID-course/#capacity-model","text":"In 1973, Kahneman proposed a theory which is based around the idea of mental efforts. It proposes that some activities are more demanding and therefore require more mental effort than others. Kahneman thus believes in the existence of a Central Processor which operates a Central Allocation Policy, constantly evaluating the demands made by each task and adjusting attention accordingly. In addition, the total available processing capacities could be increased or decreased by factors such as arousal Criticism : The main criticism for this theory ist that by developing skills, it would become impossible to accurately evaluate the demands required for each task and hence adjust attention accordingly.","title":"Capacity model"},{"location":"CEID-course/#perceptual-load-model","text":"In 1995, Lavie presented the perceptual load theory as a potential resolution to the early/late selection debate. Lavie attempts to resolve this debate by stating that both early and late selection occur varying on the stimulus presented. This variation in stimilus is explained as a notion of high or low perceptual load. This perceptual load refers to the complexity of the physical stimuli (particularly the distractor stimuli). Because Lavie gives the assumption that all of the attentional resources naturally have to be used up, a low load task would process more of the distractors to exhaust mental resources, and therefore the distractors would cause a greatee inference. On the other hand, in high load situations, as all the attentional resources are used up, distractors would cause less inference, if not at all. In other words, if the task-relevant stimulus uses all of the attentional resources, then none of the task-irrelevant stimuli (distractors) will be processed. Criticism : This theory has criticized regarding several points, notably how the notion of perceptual load is in fact dilution, and that this theory is not a solution t the early/late debate. A main critique is also how a visual cue can eliminate the inference effect supposedly created by perceptual load.","title":"Perceptual load model"},{"location":"DE-course/","text":"Design Ethnography COURSE Introduction Design ethnography is about understanding people in a sociological perspective. Ethnography comes from the greek ethnos (people) and grapho (study). It dates back to the 18th century (colonialism) as the study of people who lived in faraway places (initially tribal societies) to understand them. The social order is a major concern in ethnography studies to understand the character of everyday life. This course focuses on the empircal aspect of ethnography: the fieldwork . It is the immersion in the field to observe and study but not only from the perspective of an outsider, but from within the midst of everyday life. Also called mucking-in , the goal is to develop an appreciation of people as they really are and apprehend the ordinariness of life. Design Ethnography is relevant to design through computer systems, applications and services that are now embedded in socially organised activities. This social organisation is taken for granted (the mundane interactions: buying a coffee , taking the bus , cooking dinner ). To understand how everyday life gets done, Ethnomethodology is used: it is not a research method, but an analytic orientation . The course is outlined with the following chapters: The focus of fieldwork Doing fieldwork Analytic orientaton to the field Analysing interactional order Representing interactional order Assumptions testing Informing design The focus of fieldwork Ethnometodology is not a method, instead this course focuses on the members's methods used to conduct and organise the social activities, they are called the interactional methods . To describe and understands these methods, the empirical approach of this course requires to collect ethnographic data , or fieldwork data. This data is usually photos, videos, audio recordings or digital prints. Using all this data to give sense to the social order of an activity is also called presenting a praxeological account . In addition to interactional methods, humans make use of a repertoire of interactional resources to achieve social order. These resources are tools, not only drawned from the resource of speech but also from bodily conduct : gaze, body orientation, body motion, gesture. Doing fieldwork For any type of activity, doing fieldwork involves choosing the correct tools and resources to collect data, analyse the member's interactional methods and assemble an ethnographic record . Doing fielwork can be split in three parts: Immersion : witnessing first hand what and how interactions are made (accounts of in and of ), Documentation : recording, transcribing and describing actions and interactions, Examination : identifying what and how interactions are made, providing accounts. Tools and resources Conducting fieldwork can be achieved with the following tools and resources: Notes : writing down observations, thoughts and reflections, snippets of conservations, etc. While observing the activity, it is important to look out for the following details: Ecology : where the action takes place, layout of the space, location of actors and roles, equipments and artefacts used, Formal organisation : plans or procedures, Transitions events : important transition moments (arriving, leaving, etc), Arrangements of collaboration : interactions between members (who talks to whom about what), Audio-visual data indexing : times, sequence, making sense of digital data. Interviews : contextual interviews (not scripted), conversations about the the action conducted while observing it being done, Aduio-visual recordings : video cameras, audio recorders, still photography. Digital logs : messages, social media, conversations, systems, etc. Activity map An activity map is an overview of the phases and activites comprising the activity, it is an index into the ethnographic record of materials to show how each is done in detail. This slicing map the connection between activities, the sequential order, where the action starts, ends, with what, etc. In and of interaction While collecting data and mapping the conduct of an activity answers the question What? , a more thorough work regarding the How? must be done. The accounts of in and of interaction goes beyond the mere description of what is done, but addresses the interactional resources, how digital and physical resources are used, how decisions are made: the aim is to understand what is done in the doing , the members' methods. When studying activities in organisations, it is important to develop an appreciation of working knowledge . This addresses how abstractions are put into practice (plans, procedures, rules, routines, etc) and goes beyond abstract descriptions of method. If one can declare the ability to reproduce an activity in practice , that individual is said to have developed a vulgar competence . Vulgar competence The vulgar competence is an ordinary expertise to understand the ordinary things that people know to do any routine or procedure happen again. It is about recognising what is heard and seen by members of the setting studied, knowing what members' expertise consists of in action. It is about knowing what the members know and describe its methodical character by focusing on their accounts . Members' accounts are the particular things they say , do and the particular way they use resources . It makes what is being done and how it is organised observable-and-reportable . Machinery of interaction The member's accounts provide an accountable interactional order , revealing the machinery of interaction that addresses the taken for granted orderliness of a familiair world. The aim is to transform the simple view of what happened from a matter of particular interaction done by particular people, to a matter of interactions as products of a machinery. This accountable order of interactional work is not person specific, the machinery is the generalisable features of the interaction. Analytic orientation to the field The taken-for-granted orderliness of everyday life When doing analysis of an activity, one must ignore the feeling of normality and obviousness of an social interaction. By finding a scene unremarkable and not worth of analysis, it becomes difficult to notice the accounts (bodily, verbal and artefactual) involved. Analysts must treat everything as anthropologically strange , something they are not familiar with, as if they came from another planet and discovered earth and its inhabitants for the first time, trying to understand everything they say and do. They must proceed with indifference and ignore personal assumptions. Finding the machinery of interaction Finding the acountable interactional order and hence uncovering the machinery of interaction can be done with the following steps: What is being done? : Slice, define, filter and find the order of the accounts as well as their nature (verbal, embodied or artefactual), How is it done? : Elaborate the accounts by describing how what is done is being done, Machinery : Extract a distinctive set of methods for assembling the accounts, that describe Interaction analysis There are specific features of interaction that help unpacking member's methods: Sequentiality Temporal organisation Participation structures Trouble and repair Spatial organisation Tools, artificats and documents . FOCI Sequentiality Sequentiality addresses the context of an interaction, by finding its beginnings, middles and endings. It describes how the interaction is occasioned, or avoided. The goal is to understand how the members recognise and maintain the different phases, how they reach the boundaries or how the transition are managed. The sequentiality can be represented with an activity map. Temporal organisation The temportal orderliness is about orienting the activity to time-tables, deadlines, schedules, calendars, etc. By defining the routines, rythms, repetitions and periodicity of an activity, it is possible to look for the expected and the unexpected, and observe how it is dealt with. Participation structures The participation structures address the extent to which members share a common task orientation and attentional focus. The main aspects in this notion are how do members achieve and maintain a common understanding (embodied interactional resources, i.e pointing , eye-contact ), and the technological resources used to achieve said collaboration. Trouble and repair Trouble defines the notion of a routine feature that occurs frequently, and that although can be subtle, potentially causes misunderstanding if not repaired. Repairing is done through clarifying, repeating, rephrasing, pointing, demonstrating, etc. Spatial organisation The spatial organisation of an activity concerns how the members, resources, tools, artificats and documents are distributed in space. In a social activity, members are co-oriented in space, based on the social norms: personal space , public spaces or private spaces . The spatial configuration of a setting may constrain or enable certain interactions, and elements might get moved around. It is aslo important to address the work involved in preparing the activity, or putting back the setting to its original organisation. Tools, artifacts and documents Resources employed in the interaction, both physical and digital are useful to analyse social organisation. Documents can be used as ordering devices (.i.e shopping list ). Collaborative documents can often be indexical to interaction and demonstrative to how they were produced. Conversation analysis Conversation analysis seeks to describe the underlying social organization through which orderly and intelligible social interaction is made possible. Adjency pairs Talk tends to occur in responsive pairs; however, the pairs may be split over a sequence of turns. Two different speakers, in adjacent positions, are arranged in terms of first and second pair part . Examples can be Question/Answer, Greeting/Response, Invitation/Acceptance-Refusal, Offering/Taking. While adjencent pairing was brought in the context of talk analysis, it is actually the building block of many forms of modalities of interaction. Not only talk, but embodied interaction, physical interaction and digital interaction are of importance in adjacent pairs. Adjacent pairing helps to l ocate the procedural ways in which the work of the setting is ordered. Turn taking Turn taking is an interactional exchange system, used for the ordering of many activities. It addresses embodied resources that members employ to take turn in interaction, such as verbal (talk, intonation, tone, pitch, pacing) and non-verbal (gaze, glance, bodily orientation, gestures, pointing) or the use of artefacts. So how do members know when it's their turn? Turn-taking is plit in two components: Turn-Constructional Units (TCUs): phrases, sentences, clauses, etc, Turn allocation : occurs at transitions through speaker-selection or self-selection . If the current speaker stops or pauses, these behaviors can occur: if the current speaker nominates the next speaker, the latter has to right and is obliged to speak, if the current speaker doesn't nominate the next speaker, other speakers may self-select, or the current speaker can continue. Representing interactional order Representing interactional order consists of: Representing the sequential organisation in an activity map This represents the phases and activities, WHAT is done, also called accounts about interaction Unpacking the world in and through praxeological accounts This represents HOW it is done, also called accounts in and of interaction , Praxeological accounts = Vivid exhibits + Thick description Vivid exhibits : photos, sequences of images, transcripts, Thick description : higlighting the member's methods implicated in the production and recognition of accounts in and of interaction Assembling the collection of members' methods to describe the machinery of interaction","title":"DE"},{"location":"DE-course/#design-ethnography","text":"COURSE","title":"Design Ethnography"},{"location":"DE-course/#introduction","text":"Design ethnography is about understanding people in a sociological perspective. Ethnography comes from the greek ethnos (people) and grapho (study). It dates back to the 18th century (colonialism) as the study of people who lived in faraway places (initially tribal societies) to understand them. The social order is a major concern in ethnography studies to understand the character of everyday life. This course focuses on the empircal aspect of ethnography: the fieldwork . It is the immersion in the field to observe and study but not only from the perspective of an outsider, but from within the midst of everyday life. Also called mucking-in , the goal is to develop an appreciation of people as they really are and apprehend the ordinariness of life. Design Ethnography is relevant to design through computer systems, applications and services that are now embedded in socially organised activities. This social organisation is taken for granted (the mundane interactions: buying a coffee , taking the bus , cooking dinner ). To understand how everyday life gets done, Ethnomethodology is used: it is not a research method, but an analytic orientation . The course is outlined with the following chapters: The focus of fieldwork Doing fieldwork Analytic orientaton to the field Analysing interactional order Representing interactional order Assumptions testing Informing design","title":"Introduction"},{"location":"DE-course/#the-focus-of-fieldwork","text":"Ethnometodology is not a method, instead this course focuses on the members's methods used to conduct and organise the social activities, they are called the interactional methods . To describe and understands these methods, the empirical approach of this course requires to collect ethnographic data , or fieldwork data. This data is usually photos, videos, audio recordings or digital prints. Using all this data to give sense to the social order of an activity is also called presenting a praxeological account . In addition to interactional methods, humans make use of a repertoire of interactional resources to achieve social order. These resources are tools, not only drawned from the resource of speech but also from bodily conduct : gaze, body orientation, body motion, gesture.","title":"The focus of fieldwork"},{"location":"DE-course/#doing-fieldwork","text":"For any type of activity, doing fieldwork involves choosing the correct tools and resources to collect data, analyse the member's interactional methods and assemble an ethnographic record . Doing fielwork can be split in three parts: Immersion : witnessing first hand what and how interactions are made (accounts of in and of ), Documentation : recording, transcribing and describing actions and interactions, Examination : identifying what and how interactions are made, providing accounts.","title":"Doing fieldwork"},{"location":"DE-course/#tools-and-resources","text":"Conducting fieldwork can be achieved with the following tools and resources: Notes : writing down observations, thoughts and reflections, snippets of conservations, etc. While observing the activity, it is important to look out for the following details: Ecology : where the action takes place, layout of the space, location of actors and roles, equipments and artefacts used, Formal organisation : plans or procedures, Transitions events : important transition moments (arriving, leaving, etc), Arrangements of collaboration : interactions between members (who talks to whom about what), Audio-visual data indexing : times, sequence, making sense of digital data. Interviews : contextual interviews (not scripted), conversations about the the action conducted while observing it being done, Aduio-visual recordings : video cameras, audio recorders, still photography. Digital logs : messages, social media, conversations, systems, etc.","title":"Tools and resources"},{"location":"DE-course/#activity-map","text":"An activity map is an overview of the phases and activites comprising the activity, it is an index into the ethnographic record of materials to show how each is done in detail. This slicing map the connection between activities, the sequential order, where the action starts, ends, with what, etc.","title":"Activity map"},{"location":"DE-course/#in-and-of-interaction","text":"While collecting data and mapping the conduct of an activity answers the question What? , a more thorough work regarding the How? must be done. The accounts of in and of interaction goes beyond the mere description of what is done, but addresses the interactional resources, how digital and physical resources are used, how decisions are made: the aim is to understand what is done in the doing , the members' methods. When studying activities in organisations, it is important to develop an appreciation of working knowledge . This addresses how abstractions are put into practice (plans, procedures, rules, routines, etc) and goes beyond abstract descriptions of method. If one can declare the ability to reproduce an activity in practice , that individual is said to have developed a vulgar competence .","title":"In and of interaction"},{"location":"DE-course/#vulgar-competence","text":"The vulgar competence is an ordinary expertise to understand the ordinary things that people know to do any routine or procedure happen again. It is about recognising what is heard and seen by members of the setting studied, knowing what members' expertise consists of in action. It is about knowing what the members know and describe its methodical character by focusing on their accounts . Members' accounts are the particular things they say , do and the particular way they use resources . It makes what is being done and how it is organised observable-and-reportable .","title":"Vulgar competence"},{"location":"DE-course/#machinery-of-interaction","text":"The member's accounts provide an accountable interactional order , revealing the machinery of interaction that addresses the taken for granted orderliness of a familiair world. The aim is to transform the simple view of what happened from a matter of particular interaction done by particular people, to a matter of interactions as products of a machinery. This accountable order of interactional work is not person specific, the machinery is the generalisable features of the interaction.","title":"Machinery of interaction"},{"location":"DE-course/#analytic-orientation-to-the-field","text":"","title":"Analytic orientation to the field"},{"location":"DE-course/#the-taken-for-granted-orderliness-of-everyday-life","text":"When doing analysis of an activity, one must ignore the feeling of normality and obviousness of an social interaction. By finding a scene unremarkable and not worth of analysis, it becomes difficult to notice the accounts (bodily, verbal and artefactual) involved. Analysts must treat everything as anthropologically strange , something they are not familiar with, as if they came from another planet and discovered earth and its inhabitants for the first time, trying to understand everything they say and do. They must proceed with indifference and ignore personal assumptions.","title":"The taken-for-granted orderliness of everyday life"},{"location":"DE-course/#finding-the-machinery-of-interaction","text":"Finding the acountable interactional order and hence uncovering the machinery of interaction can be done with the following steps: What is being done? : Slice, define, filter and find the order of the accounts as well as their nature (verbal, embodied or artefactual), How is it done? : Elaborate the accounts by describing how what is done is being done, Machinery : Extract a distinctive set of methods for assembling the accounts, that describe","title":"Finding the machinery of interaction"},{"location":"DE-course/#interaction-analysis","text":"There are specific features of interaction that help unpacking member's methods: Sequentiality Temporal organisation Participation structures Trouble and repair Spatial organisation Tools, artificats and documents .","title":"Interaction analysis"},{"location":"DE-course/#foci","text":"","title":"FOCI"},{"location":"DE-course/#sequentiality","text":"Sequentiality addresses the context of an interaction, by finding its beginnings, middles and endings. It describes how the interaction is occasioned, or avoided. The goal is to understand how the members recognise and maintain the different phases, how they reach the boundaries or how the transition are managed. The sequentiality can be represented with an activity map.","title":"Sequentiality"},{"location":"DE-course/#temporal-organisation","text":"The temportal orderliness is about orienting the activity to time-tables, deadlines, schedules, calendars, etc. By defining the routines, rythms, repetitions and periodicity of an activity, it is possible to look for the expected and the unexpected, and observe how it is dealt with.","title":"Temporal organisation"},{"location":"DE-course/#participation-structures","text":"The participation structures address the extent to which members share a common task orientation and attentional focus. The main aspects in this notion are how do members achieve and maintain a common understanding (embodied interactional resources, i.e pointing , eye-contact ), and the technological resources used to achieve said collaboration.","title":"Participation structures"},{"location":"DE-course/#trouble-and-repair","text":"Trouble defines the notion of a routine feature that occurs frequently, and that although can be subtle, potentially causes misunderstanding if not repaired. Repairing is done through clarifying, repeating, rephrasing, pointing, demonstrating, etc.","title":"Trouble and repair"},{"location":"DE-course/#spatial-organisation","text":"The spatial organisation of an activity concerns how the members, resources, tools, artificats and documents are distributed in space. In a social activity, members are co-oriented in space, based on the social norms: personal space , public spaces or private spaces . The spatial configuration of a setting may constrain or enable certain interactions, and elements might get moved around. It is aslo important to address the work involved in preparing the activity, or putting back the setting to its original organisation.","title":"Spatial organisation"},{"location":"DE-course/#tools-artifacts-and-documents","text":"Resources employed in the interaction, both physical and digital are useful to analyse social organisation. Documents can be used as ordering devices (.i.e shopping list ). Collaborative documents can often be indexical to interaction and demonstrative to how they were produced.","title":"Tools, artifacts and documents"},{"location":"DE-course/#conversation-analysis","text":"Conversation analysis seeks to describe the underlying social organization through which orderly and intelligible social interaction is made possible.","title":"Conversation analysis"},{"location":"DE-course/#adjency-pairs","text":"Talk tends to occur in responsive pairs; however, the pairs may be split over a sequence of turns. Two different speakers, in adjacent positions, are arranged in terms of first and second pair part . Examples can be Question/Answer, Greeting/Response, Invitation/Acceptance-Refusal, Offering/Taking. While adjencent pairing was brought in the context of talk analysis, it is actually the building block of many forms of modalities of interaction. Not only talk, but embodied interaction, physical interaction and digital interaction are of importance in adjacent pairs. Adjacent pairing helps to l ocate the procedural ways in which the work of the setting is ordered.","title":"Adjency pairs"},{"location":"DE-course/#turn-taking","text":"Turn taking is an interactional exchange system, used for the ordering of many activities. It addresses embodied resources that members employ to take turn in interaction, such as verbal (talk, intonation, tone, pitch, pacing) and non-verbal (gaze, glance, bodily orientation, gestures, pointing) or the use of artefacts. So how do members know when it's their turn? Turn-taking is plit in two components: Turn-Constructional Units (TCUs): phrases, sentences, clauses, etc, Turn allocation : occurs at transitions through speaker-selection or self-selection . If the current speaker stops or pauses, these behaviors can occur: if the current speaker nominates the next speaker, the latter has to right and is obliged to speak, if the current speaker doesn't nominate the next speaker, other speakers may self-select, or the current speaker can continue.","title":"Turn taking"},{"location":"DE-course/#representing-interactional-order","text":"Representing interactional order consists of: Representing the sequential organisation in an activity map This represents the phases and activities, WHAT is done, also called accounts about interaction Unpacking the world in and through praxeological accounts This represents HOW it is done, also called accounts in and of interaction , Praxeological accounts = Vivid exhibits + Thick description Vivid exhibits : photos, sequences of images, transcripts, Thick description : higlighting the member's methods implicated in the production and recognition of accounts in and of interaction Assembling the collection of members' methods to describe the machinery of interaction","title":"Representing interactional order"},{"location":"SDHM-course/","text":"Simulation and Digital Human modeling COURSE 1. Introduction: Simulation Simulation is giving a false impression of.. , reproducing the conditions of.. or looking and acting like.. . In modern life it is the science of make-believe , artificial and virtual constructions. Dimensions It has three different dimensions : Live : Real people using simulated machines in real environment, Virtual : Real people using simulated machines in simulated environment, Constructive : Simulated people using simulated machine in simulated environment. Areas and Industries In modern life, simulation is involved in 5 key application areas: Training and education , Design , Entertainement , Criminal investigations , Research . As well as 7 key industries: Transport , Media , Defense , Emergency services , Healthcare , Manufacturing , Energy . Advantages and Disadvantages The advantages of simulation are: Resource savings : reduced need for real objects, quicker development time, it's not the \"real\" thing, Utility : trying and experimenting easily, target specific user groups, repeat exercises, assess performances easily, maintain control over variables, investigate issues hard to come by in the real world, Safety : reduced potential harm, consider extreme issues (a plane crash), good for novice users. The disadvantages of simulation are: Validity : potential failure to reflect real-world performance/behaviour, Sickness : people getting sick in the simulator, Costs : start-up, maintenance, Software : difficult to use/learn. 2. Sickness in Simulators Symptoms of sickness Symptoms of sickness during a simulation can be: Eye strain , Headache , Diziness Sweating , Disorientation , Drowsiness/Fatigue , Vertigo , Pallor , Nausea , Vomiting . Importance in understanding sickness Understanding sickness in a simulator is very important for the following reasons: Ethics : subject participants to discomfort, Representativeness of the sample : defining who can/can't use/participate in the simulator and/or the product, Negative transfer of training : sickness implies difficult teaching/learning conditions, Validity of results : sick participants can interfere with practical performance results, Design : a simulator that makes most people sick needs to be redesigned (so is the product). Sensor Conflict Theory During a simulator session, three major spatial senses are challenged: the Visual System , the Vestibular System , the Non-Vestibular System . If these senses are conflicted, sickness is bound to appear. There are two types of conflict: Visual-inertial : conflict between the Visual System and the Vestibular/Non-Vestibular System, Canal-otolith : conflict within the Vestibular System. For most simulations, the Visual-inertial conflict is the main consideration. Measuring sickness Sickness can be observed with three major techniques: Asking participants : using questionnaires such as the Simulation Sickness Questionnaire (SSQ, Kennedy), Observing participants : watch for signs of drowsiness, skin pallor, Measuring participants : measure postural instability and physiological characteristics. 3. Fidelity of Simulation Fidelity is the degree of exactness with which something is copied or reproduced . In simulators, it is the degree to which a model or simulation reproduces the state and behaviour of a real world object or perception of a real world object, feature, condition or standard in a measurable or perceivable manner. Importance of fidelity Fidelity in a simulator is an important aspect for the following reasons: Cost : more fidelity requires more cost, Motivation : more fidelity will improve motivation, performance and general behavior to the simulations. Types of fidelity There are two types of fidelity in a simulator: Physical : accuracy with which the simulation provides input to human senses (visual/audio, inertial), Psychological : replication of the relevant cognitive factors involved. Measuring fidelity Measuring fidelity boils down to making comparisons with reality. There are three major ways to do that: Mathematical modeling : simple and strict, but too subjective and arbitrary, Benefits : a relationship between the quantity of benefits (utility, validity, transfer training..etc) and the fidelity, Theoretical perspectives : the number of common elements between the simulation and the real world ; and the accuracy of the mapping between the task stimuli and the response. 4. Benefits Simulators should provide advantages and benefits such as validity, transfer of training, usability and more. Validity There are 5 types of validity: Face : extent to which a simulation appears to be real, particularly relevant to a participant's motivation, Internal : extent to which changes in a dependent variable can be related to an independent variable, Ecological : extent to which the results can be applied in real-world situations, Absolute : measured derived from the simulator are the same/similar as in the real world, Relative : relative effect of a condition is the same in magnitude and direction as in the real word (more used than absolute). Transfer of Training The transfer of training in a simulator is a very important aspect in novice users training for real world situations. This transfer can be either positive or negative: Positive : after training with the simulator, the person correctly applies the acquired knowledge, skills, and abilities in real word situations, Negative : after training with the simulator, the person does not apply or incorrectly applies the proper knowledge, skills, and abilities in real word situations.","title":"SDHM"},{"location":"SDHM-course/#simulation-and-digital-human-modeling","text":"COURSE","title":"Simulation and Digital Human modeling"},{"location":"SDHM-course/#1-introduction-simulation","text":"Simulation is giving a false impression of.. , reproducing the conditions of.. or looking and acting like.. . In modern life it is the science of make-believe , artificial and virtual constructions.","title":"1. Introduction: Simulation"},{"location":"SDHM-course/#dimensions","text":"It has three different dimensions : Live : Real people using simulated machines in real environment, Virtual : Real people using simulated machines in simulated environment, Constructive : Simulated people using simulated machine in simulated environment.","title":"Dimensions"},{"location":"SDHM-course/#areas-and-industries","text":"In modern life, simulation is involved in 5 key application areas: Training and education , Design , Entertainement , Criminal investigations , Research . As well as 7 key industries: Transport , Media , Defense , Emergency services , Healthcare , Manufacturing , Energy .","title":"Areas and Industries"},{"location":"SDHM-course/#advantages-and-disadvantages","text":"The advantages of simulation are: Resource savings : reduced need for real objects, quicker development time, it's not the \"real\" thing, Utility : trying and experimenting easily, target specific user groups, repeat exercises, assess performances easily, maintain control over variables, investigate issues hard to come by in the real world, Safety : reduced potential harm, consider extreme issues (a plane crash), good for novice users. The disadvantages of simulation are: Validity : potential failure to reflect real-world performance/behaviour, Sickness : people getting sick in the simulator, Costs : start-up, maintenance, Software : difficult to use/learn.","title":"Advantages and Disadvantages"},{"location":"SDHM-course/#2-sickness-in-simulators","text":"","title":"2. Sickness in Simulators"},{"location":"SDHM-course/#symptoms-of-sickness","text":"Symptoms of sickness during a simulation can be: Eye strain , Headache , Diziness Sweating , Disorientation , Drowsiness/Fatigue , Vertigo , Pallor , Nausea , Vomiting .","title":"Symptoms of sickness"},{"location":"SDHM-course/#importance-in-understanding-sickness","text":"Understanding sickness in a simulator is very important for the following reasons: Ethics : subject participants to discomfort, Representativeness of the sample : defining who can/can't use/participate in the simulator and/or the product, Negative transfer of training : sickness implies difficult teaching/learning conditions, Validity of results : sick participants can interfere with practical performance results, Design : a simulator that makes most people sick needs to be redesigned (so is the product).","title":"Importance in understanding sickness"},{"location":"SDHM-course/#sensor-conflict-theory","text":"During a simulator session, three major spatial senses are challenged: the Visual System , the Vestibular System , the Non-Vestibular System . If these senses are conflicted, sickness is bound to appear. There are two types of conflict: Visual-inertial : conflict between the Visual System and the Vestibular/Non-Vestibular System, Canal-otolith : conflict within the Vestibular System. For most simulations, the Visual-inertial conflict is the main consideration.","title":"Sensor Conflict Theory"},{"location":"SDHM-course/#measuring-sickness","text":"Sickness can be observed with three major techniques: Asking participants : using questionnaires such as the Simulation Sickness Questionnaire (SSQ, Kennedy), Observing participants : watch for signs of drowsiness, skin pallor, Measuring participants : measure postural instability and physiological characteristics.","title":"Measuring sickness"},{"location":"SDHM-course/#3-fidelity-of-simulation","text":"Fidelity is the degree of exactness with which something is copied or reproduced . In simulators, it is the degree to which a model or simulation reproduces the state and behaviour of a real world object or perception of a real world object, feature, condition or standard in a measurable or perceivable manner.","title":"3. Fidelity of Simulation"},{"location":"SDHM-course/#importance-of-fidelity","text":"Fidelity in a simulator is an important aspect for the following reasons: Cost : more fidelity requires more cost, Motivation : more fidelity will improve motivation, performance and general behavior to the simulations.","title":"Importance of fidelity"},{"location":"SDHM-course/#types-of-fidelity","text":"There are two types of fidelity in a simulator: Physical : accuracy with which the simulation provides input to human senses (visual/audio, inertial), Psychological : replication of the relevant cognitive factors involved.","title":"Types of fidelity"},{"location":"SDHM-course/#measuring-fidelity","text":"Measuring fidelity boils down to making comparisons with reality. There are three major ways to do that: Mathematical modeling : simple and strict, but too subjective and arbitrary, Benefits : a relationship between the quantity of benefits (utility, validity, transfer training..etc) and the fidelity, Theoretical perspectives : the number of common elements between the simulation and the real world ; and the accuracy of the mapping between the task stimuli and the response.","title":"Measuring fidelity"},{"location":"SDHM-course/#4-benefits","text":"Simulators should provide advantages and benefits such as validity, transfer of training, usability and more.","title":"4. Benefits"},{"location":"SDHM-course/#validity","text":"There are 5 types of validity: Face : extent to which a simulation appears to be real, particularly relevant to a participant's motivation, Internal : extent to which changes in a dependent variable can be related to an independent variable, Ecological : extent to which the results can be applied in real-world situations, Absolute : measured derived from the simulator are the same/similar as in the real world, Relative : relative effect of a condition is the same in magnitude and direction as in the real word (more used than absolute).","title":"Validity"},{"location":"SDHM-course/#transfer-of-training","text":"The transfer of training in a simulator is a very important aspect in novice users training for real world situations. This transfer can be either positive or negative: Positive : after training with the simulator, the person correctly applies the acquired knowledge, skills, and abilities in real word situations, Negative : after training with the simulator, the person does not apply or incorrectly applies the proper knowledge, skills, and abilities in real word situations.","title":"Transfer of Training"},{"location":"SHP-SPSS/","text":"Studying Human Performance - SPSS COURSE Descriptive statistics Descriptive statistics are used to describe and summarise datasets , and SPSS is very useful for the quick calculation of descriptive statistics. These can then be used for further calculations or displayed in a table or graph. The common descriptive statistics are mean , mode , median , sum , maximum , minimum , standard deviation , range , variance , standard error of the mean , skweness or kurtosis , quartimes , percentiles . Assumptions tests Levene Levene 's test is an inferential statistic used to assess the equality of variances. It tests the null hypothesis that the population variances are equal (called homogeneity of variance or homoscedasticity). If the resulting p-value of Levene's test is less than some significance level (typically 0.05), the obtained differences in sample variances are unlikely to have occurred based on random sampling from a population with equal variances. Thus, the null hypothesis of equal variances is rejected and it is concluded that there is a difference between the variances in the population. Mauchly Mauchly's sphericity is an important assumption of a repeated-measures ANOVA. It refers to the condition where the variances of the differences between all possible pairs of within-subject conditions are equal. If sphericity is violated, then the variance calculations may be distorted, which would result in an F-ratio that would be inflated. Sphericity can be evaluated when there are three or more levels of a repeated measure factor and, with each additional repeated measures factor, the risk for violating sphericity increases. To know if the sphericity is violated, the significance level is noted: if \\(p 0.05\\) the assumption of sphericity is reject . If the sphericity is violated, there are two main corrections to use to interpret the results: Greenhouse-Geisser , Huynh-Feldt . A general rule of thumb presents the choices as follows: Normality An assessment of the normality of data is a prerequisite for many statistical tests because normal data is an underlying assumption in parametric testing. There are two main methods of assessing normality: graphically and numerically. Numerically, the tests Kolmogorov-Smirnov and Shapiro-Wilk are used: Use Shapiro-Wilk for small samples ( 50) and Kolmogorov-Smirnov otherwise. To interpret both tests, read the value of the significance value, if p 0.05, the data significantly deviates from a normal distribution . Tests Wilcoxon Just like when doing the calculations by hand, SPSS first ouputs a table with sum of negative and positive ranks. The cell Ties indicates how many subjects have been removed from the calculations because their rank difference was null. Then the next table shows that SPSS calculated what is called \\(W\\) when doing by hand, but has converted it to a z-score . In this case, because the significance level is superior to 0.05, we fail to reject the null hypothesis. REPORTING : First conclude on the research question based on the significance level. Then present the results as such: z = -1.807, p = 0.071 , and finally use descriptive statistics to support the result: the median. Mann-Whitney Similarly to Wilcoxon, the first table for Mann-Whitney outputs the ranks, but for this test they are split by group . This first table shows if there are differences between groups, that can afterwards be validated if the result is significant. The next table shows the actual computation of the value \\(U\\): In this case, because the significance level is inferior to 0.05, the null hypothesis can be rejected. REPORTING : First conclude on the research question, then present the results using the p-value such as (p = 0.014) and use the ranks to support the result. Friedman The first table output is the mean ranks for each group/condition: The next table shows the actual computation of the Friedman test, with the values Chi Square, dg, and the significance level: In this case, because the significance level is inferior to 0.05, the null hypothesis can be rejected. REPORTING : First conclude on the research question based on the significance level, then present the results in this format: \\(\\chi^2(df) = ..., p = ...\\) such as \\(\\chi^2\\) (2) = 7.600, p = 0.022 . Use the median of each condition to support the results. Finally, if the results are significant, a note on possible post hoc tests should be done. Kruskal-Wallis This test's ouput is very similar to Friedman's, except it shows the group distributions. In this case, because the significance level is inferior to 0.05, the null hypothesis can be rejected. REPORTING : First conclude on the research question based on the significance level, then present the results in this format: \\(\\chi^2(df) = ..., p = ...\\) such as \\(\\chi^2\\) (2) = 8.520, p = 0.014 . Use the mean rank of each condition to support the results. Finally, if the results are significant, a note on possible post hoc tests should be done. T-Test (Within) Also called T-Test Dependant , the first output of this test gives the paired sample statistics : Then, the paired samples test table outputs the actual results of the T-Test: In this case, because the significance level is inferior to 0.05, the null hypothesis can be rejected. REPORTING : First conclude on the research question based on the significance level, then present the results in this format: t(df) = t_value, p = p-value, such as t(19) = -4.773, p 0.0005 in this case. Use the descriptive statistics to support the results. T-Test (Between) Also called T-Test Independant , the first output of this test gives the group statistics : Thenext table independant sample test provides the actual results of the T-test: With a p-value of 0.579 for the Levene's test, the samples have homogeneity of variance. Regarding the T-test, the significance level of 0.020 indicates that the null hypothesis can be rejected. REPORTING : First conclude on the research question based on the significance level,then present the results in this format: t(df) = t_value, p = p-value, such as t(38) = 2.428, p 0.020 in this case. Use the descriptive statistics to support the results. One-Way ANOVA (Between) First, the test outputs a table of descriptive statistics that gives valuable information: Then, the ANOVA table is presented: This table shows the main effect , that there is a significant difference between the group means. Here with p = 0.021 ( 0.05), that is validated and therefore there is a difference in the means. While this is important, the simple effects tells us more about which of the specific groups differ (done either with planned contrats or post hocs). The multiple comparisons shows this: This table show the significances values between each group and here we can see that Beginner differs from Intermediate and Advanced (p = 0.46 and p = 0.34), while Indermediate and Advanced present no significant difference (p = 0.989). REPORTING : First conclude on the research question based on the significance level and the F-ratio as follows: F(df_between, df_within) = F_value, p = p_value, such as F(2, 27) = 4.467, p = 0.021 . Present the results found with constrasts/post hocs using the p-values and use the mean to support it. One-Way ANOVA (Within, Repeated Measures) The first table output is the within-subjects factors which confirms the study design of our analysis, these are the experimental conditions: Then, just like for the Between ANOVA, a table of descriptive statistics is shown: The next stable is the actual results of the test: It is important (using mauchly's test) to know which result source to read from. Here, because the assumption of sphericity is violated, the Greenhouse-Geisser correction is used. With a significance level of 0.000-, the null hypothesis can be rejected. While this is important, the simple effects tells us more about which of the specific level differ (done either with planned contrats or post hocs). The pairwise comparisons shows this: Again, the significance values shows where differences are present between the conditions. REPORTING : First conclude on the research question based on the significance level and the F-ratio as follows: F(df_between, df_within) = F_value, p = p_value, such as F(2, 27) = 4.467, p = 0.021 . Present the results found with constrasts/post hocs using the p-values and use the mean to support it. Two-Way ANOVA (Between/Within) These tests are similar and the process is similar, only with more IVs, hence more combinations and more possible comparisons.","title":"SHP - SPSS"},{"location":"SHP-SPSS/#studying-human-performance-spss","text":"COURSE","title":"Studying Human Performance - SPSS"},{"location":"SHP-SPSS/#descriptive-statistics","text":"Descriptive statistics are used to describe and summarise datasets , and SPSS is very useful for the quick calculation of descriptive statistics. These can then be used for further calculations or displayed in a table or graph. The common descriptive statistics are mean , mode , median , sum , maximum , minimum , standard deviation , range , variance , standard error of the mean , skweness or kurtosis , quartimes , percentiles .","title":"Descriptive statistics"},{"location":"SHP-SPSS/#assumptions-tests","text":"","title":"Assumptions tests"},{"location":"SHP-SPSS/#levene","text":"Levene 's test is an inferential statistic used to assess the equality of variances. It tests the null hypothesis that the population variances are equal (called homogeneity of variance or homoscedasticity). If the resulting p-value of Levene's test is less than some significance level (typically 0.05), the obtained differences in sample variances are unlikely to have occurred based on random sampling from a population with equal variances. Thus, the null hypothesis of equal variances is rejected and it is concluded that there is a difference between the variances in the population.","title":"Levene"},{"location":"SHP-SPSS/#mauchly","text":"Mauchly's sphericity is an important assumption of a repeated-measures ANOVA. It refers to the condition where the variances of the differences between all possible pairs of within-subject conditions are equal. If sphericity is violated, then the variance calculations may be distorted, which would result in an F-ratio that would be inflated. Sphericity can be evaluated when there are three or more levels of a repeated measure factor and, with each additional repeated measures factor, the risk for violating sphericity increases. To know if the sphericity is violated, the significance level is noted: if \\(p 0.05\\) the assumption of sphericity is reject . If the sphericity is violated, there are two main corrections to use to interpret the results: Greenhouse-Geisser , Huynh-Feldt . A general rule of thumb presents the choices as follows:","title":"Mauchly"},{"location":"SHP-SPSS/#normality","text":"An assessment of the normality of data is a prerequisite for many statistical tests because normal data is an underlying assumption in parametric testing. There are two main methods of assessing normality: graphically and numerically. Numerically, the tests Kolmogorov-Smirnov and Shapiro-Wilk are used: Use Shapiro-Wilk for small samples ( 50) and Kolmogorov-Smirnov otherwise. To interpret both tests, read the value of the significance value, if p 0.05, the data significantly deviates from a normal distribution .","title":"Normality"},{"location":"SHP-SPSS/#tests","text":"","title":"Tests"},{"location":"SHP-SPSS/#wilcoxon","text":"Just like when doing the calculations by hand, SPSS first ouputs a table with sum of negative and positive ranks. The cell Ties indicates how many subjects have been removed from the calculations because their rank difference was null. Then the next table shows that SPSS calculated what is called \\(W\\) when doing by hand, but has converted it to a z-score . In this case, because the significance level is superior to 0.05, we fail to reject the null hypothesis. REPORTING : First conclude on the research question based on the significance level. Then present the results as such: z = -1.807, p = 0.071 , and finally use descriptive statistics to support the result: the median.","title":"Wilcoxon"},{"location":"SHP-SPSS/#mann-whitney","text":"Similarly to Wilcoxon, the first table for Mann-Whitney outputs the ranks, but for this test they are split by group . This first table shows if there are differences between groups, that can afterwards be validated if the result is significant. The next table shows the actual computation of the value \\(U\\): In this case, because the significance level is inferior to 0.05, the null hypothesis can be rejected. REPORTING : First conclude on the research question, then present the results using the p-value such as (p = 0.014) and use the ranks to support the result.","title":"Mann-Whitney"},{"location":"SHP-SPSS/#friedman","text":"The first table output is the mean ranks for each group/condition: The next table shows the actual computation of the Friedman test, with the values Chi Square, dg, and the significance level: In this case, because the significance level is inferior to 0.05, the null hypothesis can be rejected. REPORTING : First conclude on the research question based on the significance level, then present the results in this format: \\(\\chi^2(df) = ..., p = ...\\) such as \\(\\chi^2\\) (2) = 7.600, p = 0.022 . Use the median of each condition to support the results. Finally, if the results are significant, a note on possible post hoc tests should be done.","title":"Friedman"},{"location":"SHP-SPSS/#kruskal-wallis","text":"This test's ouput is very similar to Friedman's, except it shows the group distributions. In this case, because the significance level is inferior to 0.05, the null hypothesis can be rejected. REPORTING : First conclude on the research question based on the significance level, then present the results in this format: \\(\\chi^2(df) = ..., p = ...\\) such as \\(\\chi^2\\) (2) = 8.520, p = 0.014 . Use the mean rank of each condition to support the results. Finally, if the results are significant, a note on possible post hoc tests should be done.","title":"Kruskal-Wallis"},{"location":"SHP-SPSS/#t-test-within","text":"Also called T-Test Dependant , the first output of this test gives the paired sample statistics : Then, the paired samples test table outputs the actual results of the T-Test: In this case, because the significance level is inferior to 0.05, the null hypothesis can be rejected. REPORTING : First conclude on the research question based on the significance level, then present the results in this format: t(df) = t_value, p = p-value, such as t(19) = -4.773, p 0.0005 in this case. Use the descriptive statistics to support the results.","title":"T-Test (Within)"},{"location":"SHP-SPSS/#t-test-between","text":"Also called T-Test Independant , the first output of this test gives the group statistics : Thenext table independant sample test provides the actual results of the T-test: With a p-value of 0.579 for the Levene's test, the samples have homogeneity of variance. Regarding the T-test, the significance level of 0.020 indicates that the null hypothesis can be rejected. REPORTING : First conclude on the research question based on the significance level,then present the results in this format: t(df) = t_value, p = p-value, such as t(38) = 2.428, p 0.020 in this case. Use the descriptive statistics to support the results.","title":"T-Test (Between)"},{"location":"SHP-SPSS/#one-way-anova-between","text":"First, the test outputs a table of descriptive statistics that gives valuable information: Then, the ANOVA table is presented: This table shows the main effect , that there is a significant difference between the group means. Here with p = 0.021 ( 0.05), that is validated and therefore there is a difference in the means. While this is important, the simple effects tells us more about which of the specific groups differ (done either with planned contrats or post hocs). The multiple comparisons shows this: This table show the significances values between each group and here we can see that Beginner differs from Intermediate and Advanced (p = 0.46 and p = 0.34), while Indermediate and Advanced present no significant difference (p = 0.989). REPORTING : First conclude on the research question based on the significance level and the F-ratio as follows: F(df_between, df_within) = F_value, p = p_value, such as F(2, 27) = 4.467, p = 0.021 . Present the results found with constrasts/post hocs using the p-values and use the mean to support it.","title":"One-Way ANOVA (Between)"},{"location":"SHP-SPSS/#one-way-anova-within-repeated-measures","text":"The first table output is the within-subjects factors which confirms the study design of our analysis, these are the experimental conditions: Then, just like for the Between ANOVA, a table of descriptive statistics is shown: The next stable is the actual results of the test: It is important (using mauchly's test) to know which result source to read from. Here, because the assumption of sphericity is violated, the Greenhouse-Geisser correction is used. With a significance level of 0.000-, the null hypothesis can be rejected. While this is important, the simple effects tells us more about which of the specific level differ (done either with planned contrats or post hocs). The pairwise comparisons shows this: Again, the significance values shows where differences are present between the conditions. REPORTING : First conclude on the research question based on the significance level and the F-ratio as follows: F(df_between, df_within) = F_value, p = p_value, such as F(2, 27) = 4.467, p = 0.021 . Present the results found with constrasts/post hocs using the p-values and use the mean to support it.","title":"One-Way ANOVA (Within, Repeated Measures)"},{"location":"SHP-SPSS/#two-way-anova-betweenwithin","text":"These tests are similar and the process is similar, only with more IVs, hence more combinations and more possible comparisons.","title":"Two-Way ANOVA (Between/Within)"},{"location":"SHP-course/","text":"Studying Human Performance - Methods COURSE Introduction Ergonomics is the scientific discipline cocerned with the understanding of interactions among humans and other elements of a system, in order to optimise human well-being and overall system performance. The aim of this course is to be able to analyse products and workplaces using a range of different methods, to compare and contrast different methodological approaches , to evaluate methods and statistical techniques, to demonstrate the application of specific methods in practical contexts ; in order to solve Human Factors-related problems. Choosing methods The understanding of methods is the core matter of this subject. A method is a \" way of proceeding or doing something \", and in Human Factors, allow for the collection of data. Methods need to be clear , structured and repeatable . The context of the studies in which methods are used define purposes, roles and data that make up for many different methods. It is hence very important to understand the context of these studies, and how it impacts the use of methods. Study types Three study types are observed: Laboratory , Simulation and Field . These types are compared with the following table: Laboratory/Simulation Field Advantages More control Real task Disadvantages Limited scope and accuracy Unplanned events (danger, interruptions) Affecting factors In addition to the study type, there are numerous factors affecting the choice of methods: Amount of participants, Motivation ( research , consultancy ), Location, Objective, Types of desired results, Stakeholders, sponsors and funders. Good methods The choice of a method significantly depends on the context of the study. However it doesn't define the quality of a method. What is a good method? Five requirements define the quality of a method: Validity : does the method measure what it is supposed to measure? Reliability : is the method stable? (consistent over various measurements, between/within researchers), Sensitivity : is the method able to detect effects of interest? (the more sensible the more accurate), Usability : is the method easy to learn and implement? Resources : how expensive and demanding (cost, expertise) is the method? SOURCES : \"The notion of reliability thus includes both the characteristics of the instrument and the conditions under which it is administered \u2014 both must be consistent.\" (Oppenheim), \"If the reliability of a scale or other measure drops below .80 this means that repeated administrations will cover less than 64 per cent of the same ground, and that the error component is more than one-third; such a measure will come in for serious criticism and might well have to be discarded or rebuilt.\" (Oppenheim, where 0.8 is a correlation coefficient) Reliability may be measured in several different ways: test-retest , internal consistency , split-half or parallel-form ; each method using correlation coefficients. (Oppenheims) There are different types of validity: content validity (the content is well-balanced in the domain to be measured), concurrent validity (the method correlates with other well-validated measures of the same topic, administered at about the same time); construct validity (base a method on the fulfilment of well established predictions and theoretical assumptions), predictive validity (forecast some future criterion related to similar concepts). Types of measure When doing a study, the type of measures defines the methods used and the data collected. In most ergonomics works, the following measure types are assessed: Performance : Task achivement, errors, Demands : Task time, physical and mental workload, Behaviour : Body movements, Knowledge : Subjective responses to a test situation, Opinions : Ratings, preferences. Method types A simple classification splits all available methods into three categories : Observation, Self report, Use of archival data. For all methods, the type of output and validity are also differentiated. There is first the matter of objective or subjective validity: Objective : clamining the describe a true and correct reality, independent of those involved in the research process, Subjective : based on data derived from observations of events as they occur, or from interviews. Finally, the data can be split in two categories: Qualitative : data classification of objects based on attributes and properties, Quantitative : data measuread and expressed numerically. Planning and designing studies Defining the problem, aim and hypotheses The first step in designing a study is to clearly define the problem and issues that require investigation, and what will be achived by doing it. This is usually represented by a system of hypotheses , or statements of the predicted outcome . As studies manipulate variables and data, the aim of these hypotheses is to predict, understand and validate the relationships between said variables. Conventionally, there are two types of hypothesis: Experimental , or H1 , that predicts a dependent relationship between variables, Null , or H0 , that states that variables are not dependent (one does not impact the other). Identify the research variables The said variables are split in three categories: Independant variables ( IV ): variables manipulated by the experimented, related to the individual, task, system or environment, Dependant variables ( DV ): variables being measured in the experiment, not under the control of the experimenter, possibly affected by the IVs, Controlled variables : variables that need to be kept constant during the experiment. Choose the research setting The research setting (previously mentioned as study type ), needs to be considered through the advantages and disadvantages between Laboratory and Field studies, as well as if it is actually possible to implement one based on the nature of the study. The following table describes the key elements for choosing: Field Advantages Disadvantage Realistic Hard to set up Easier to generalise results May take longer Real participants Difficulty to access real participants -- Many uncontrolled variables -- Harder to control variables Laboratory Advantages Disadvantage Easy to set up Artificial Shorter Participants less motivated Easy to control variables May not be able to controll all extraneous variables Easy to perform comparative tests May be expensive Select the participants sample It is important to properly select the sample , the small group that is representative of the population . This sample must be the most representative of the population possible, in order to make more general statements about the results. There are multiple types of sampling, depending on if the notion of probability accuracy is desired. The major representative sampling techniques are: Random ( sampling frame ): selecting random participants, with each participant having an equal chance of being selected, Systematic : selecting every nth participant, Stratified random : first diving the population into meaningful groups, then selecting randomly, Cluster : dividing the population into a number of units, or clusters, each of which contains individuals having a range of characteristics (can be extended to a multi-stage sampling). In other situtions, the participants are selected in a more favored way, when the basic selection techniques seen previously are not precise enough and the sample needs to be very specific. The non-probability sample techniques are: Quota : obtaining representatives of the various elements of a population, usually in the relative proportions in which they occur in the population (can be extended to a dimensional sampling), Purposive : satisfying the specific needs of a project, Conveniance sampling : nearest, easiest, cheapest, most convenient, Snowball : identifying one or more individuals from the population of interest, and then use them as informants to identify other members of the population, and so on. \" Convenience sampling is sometimes used as a cheap and dirty way of doing a sample survey. You do not know whether or not findings are representative \" (Robson). There are additional special sampling methods: Time samples , Homogeneous samples , Heterogeneous samples , Extreme case samples and Rare element samples . Finally, the resources and costs required to mobilize a sample need to be balanced with the potential results, and the confidence that the results will be significant. Defining the experimental design The number and arrangement of the variables is an important part of the study. The aim is to ensure that the IVs are the only systematic difference between the experimental groups, also called the internal validity . It is also important to consider the measurement of the DVs, such as the data is commonly quantitative, easy to collect, valid, reliable, senstive and ethical. Allocate participants to the experimental conditions There are two ways to allocate and distribute the participants across the IVs: Within : every participant completes every experimental condition, Between : different participants are allocated to each experimental condition. Ethics When doing studies with real participants, ethics play an important part in carrying out the experiments. It is required that they should give informed consent , based on sufficient information on the study (goals, duration, instruction, type of data collected, use of data). They should also be ensured protection from harm, privacy and a right to withdraw at any moment. Conducting the study Preparing the experiment Planning an preparing before conducting the study can be a lengthy process, but is necessary. Once designed, piloting the study with trial members allows to make sure everything is in order and as desired. The practical details such as appointements, timescales or idenfication must be thoroughly planned. Finally the various types of equipment required for the experiments must be properly prepared in order to conduct the study with consistency. Analyzing the data Once the data is collected, analyzing it is a complex process. A common way to do so is the following: Transcribe , examine and outline the data using tools such as charts, tables ; observing patterns, distributions and tendencies, Identify the type of data: nominal (categories), ordinal (ordered) or interval (ordered with equal intervals). Statiscally test the data based on the purpose of the study and the experimental design, Draw conclusions . Presenting results and drawing conclusions Results can be represented visually through charts, figures and tables. Descriptive and inferential statistics are also commonly used. Based on the drawn conslusions ( reject , fail to reject ), a wider discussion must be made regarding the strength of the findings, and how they can be generalized to other contexts. Self report methods Questionnaire Questionnaires are good tools to survey large populations , while ensuring a high response rate . It is however a complex method that requires careful planning and proper question wording. Designing a questionnaire implies the following aspects to consider: Planning : as the questionnaire is an indirect method (no face to face), it is crucial to properly plan it to make the most of it, Sampling : thoroughly target the correct sample to optimize the results on the research question, General instructions : clear outline of what is expected from people filling the questionnaire, Question wording : make questions simple, specific, unambiguous, clear, not misleading, Type of response required : clarity on the structuring of responses (open, categories, multiple-choice, rating), Sequencing of questions : intuitive flow on the sequence of questions, Layout : easy of use, clear and appealing, Demographic details : defining what personal information is really needed, Piloting : trials to identify ambiguities, issues and errors, Administration : process to get access to the desired sample (distribution), Data collection : process to collect the completed questionnaires (postage, internet), Data entry and screening : conversion from questionnaire to usable data. There are also many factors to take into consideration to make sure people fill the questionnaire appropriately to obtain good data. Generally, people might not be aware of specific aspects of a task (they don't realize everything they do, and how) or not recall certain information about a task (frequency, exposure). The time since exposure may also affect what can be observed as memory could be altered. Finally, observers and participants can have differences in their understanding and perception of certain aspects. SOURCE : \"Most practitioners and researchers describe questionnaires as tools to gather information that is not readily accessible by examining an individual\u2019s performance or behavior\" (Moroney and Cameron, 2016), Example of question wording: impact of the verb (context of a car accident: smashed/collided/bumped/hit/contacted, each lead to different interpretation of the speed of the cars when the accident occured, impact on the mental model), (Loftus and Palmer, 1974), Perceived affordances and cultural conventions associated with Web site use, Kilograms/Pounds confusion in top 10 patient safety concern by the ECRI Institute (2015) \" If a questionnaire is to be a conversation, the designer and the respondent need to establish a relationship \" (Moroney and Cameron, 2016) Interview Interviews, unlike questionnaires, happen face to face (or through vocal exchange) with a participant. It is a useful tool to collect detailed task related information, and thoroughly collect data with flexibility . However this method requires more time, and can be costly. Interviews are usually targeted to smaller samples . The aspects to consider for an interview are very similar to those for the questionnaires, but from a more practical point of view. Generally, an interview will require different planning regarding the instructions, the administration, and the data collection. Setting up the meeting, the location and the recording is important. It is also essential to prepare for the interaction with the participant, and behave appropriately to make sure the results are optimized. Finally, choosing the right type of interview is crucial. There are four types of interview: Exploratory : free-style in-depth, Structured : each interview is presented with exactly the same questions in the same order (no diversion), Semi-structured : open, allowing new ideas to be brought up during the interview, generally through a framework of themes to be explored, Group : multiple participants at once. Exploratory Exploratory interviews are used to \" probe \" participants and develop concepts by understanding how people think and feel about specific topics. These interviews are \" free-style \" and spontaneous, with minimal intervention from the researcher. Structured A structured interview is the most quantitative type of interview, providing a strong confidence in the results. By asking the exact same questions to every interviewee, a consistency is kept and the risk of diverting is the lowest. Semi-structured Semi-structured interviews address general themes and core questions, but allow for the interview to divert to broader and external contexts. Generally these interviews still require a set of questions regarding certain topics to always ensure a beneficial method. Being less \" accurate \", these interviews are widely used in qualitative research. Group Group interviews gather multiple participants, and sometimes multiple interviewers. The first benefit in doing such an interview is the obvious gain of time by combining participants. Of course a potential issue with this is the lack of depth, and the difficulty to maintain a structured interview. It is hence very important to limit the group interview to a maximum amount of participants and interviewers. The practical aspects must also be considered: group disposition, turn taking and general order. Other interview techniques Critical incident technique : set of procedures used for collecting direct observations of human behavior that have critical significance and meet methodically defined criteria. These observations are used to solve practical problems and develop broad psychological principles. A critical incident can be described as one that makes a contribution, either positively or negatively, to an activity. Critical decision method : retrospective interview method that employs a set of cognitive probes to non-routine incidents that required expert judgment or decision making. Verbal protocol reports : concurrent and retrospective protocol reports used with cognitive tasks, to determine what workers are thinking about during tasks. Eliciting preferences Understanding the way people perceive, compare and prefer systems and products is an essential part of a design process. There are tools and methods used to identify these aspects in diverse contexts. Paired comparisons Henry Scheff\u00e9 When comparing multiple systems, the paired comparisons method can be used to rank and identify general user preferences . This method consists in presenting all possible pairs of the system (product) to every participant, in a random sequence. Each time, the participant must indicate which system was preferred between the two. SOURCE : used to compare car seat comfort, through a subjective scale (-3 to +3). Significant results were found regarding the stiffness and foam hardness. (Kazushige Ebe Micheal J. Griffin (2001)). Identifying the pairs For \\(n\\) objects, there are \\( \\frac{1}{2} \\cdot n \\cdot (n - 1)\\) possible pairs. In the example of 4 chairs, there a \\(1/2 * 4 * 3 = 6\\) possible pairs, that can be \\({AB, AC, BD, CB, CD, DA}.\\) Filling the preference matrix The preference matrix represents every pair and how many participants prefered one object to another (\\(x\\) prefered to \\(y\\)). The following preference matrix for the chairs, with 14 participants, is given: A B C D A - 6 8 12 B 8 - 10 14 C 6 4 - 13 D 2 0 1 - Computing the probability matrix To obtain results and identify the most (or least) preferred object, the probability matrix must be computed. From the preference matrix, every item must be divided by the total number of participants \\(N\\), and empty items (same objects) replaced by \\(0.50\\). A B C D A 0.50 0.43 0.57 0.86 B 0.57 0.50 0.71 1.00 C 0.43 0.29 0.50 0.93 D 0.14 0.00 0.07 0.50 \\(\\sum\\) 1.64 1.22 1.85 3.29 Using the last row, the sum of probabilities, the most preferred chair is \\(D\\), and the least preferred one is \\(B\\). Relative ranking To rank the objects in relative levels, the use of standard scores, or z-scores , is required. To do so, the distance from the mean, here \\(0.50\\) must be extracted and used in the normal distribution table to get the z-score . If an item is above the mean, the z-score is positive, if it is under the mean the z-score is negative, and null otherwise. A B C D A 0.00 -0.18 0.18 1.06 B 0.18 0.00 0.55 4.00 C -0.18 -0.55 0.00 1.48 D -1.08 -4.00 -1.48 0.00 \\(\\bar{z}\\) -0.27 -1.18 -0.19 1.64 The last row, the mean of the z-scores , represents a relative ranking: \\[ B(-1.18) A(-0.27) C(-0.19) D(1.64) \\] Repertory grid George Kelly, 1955 The repertory grid is a tool to identify what constructs are important to a person in a system, product, job or establishement. It helps to understand and indivual's interpretation of his or her environment. Using a method of triads , this method compares elements of a topic (better, worse, extremes). These elements are put in a table that represents a person's perception of a certain topic. The data used to rate and rank the elements can be qualitative and quantitative. In practice, the repertory grid is a structured and thorough method , and can be applied to many contexts. Ideally used early in the design process, it might required more time to train the participaints and make sure the results are valid and reliable. SOURCE : In order to elicit and explore the knowledge held by rail signalling experts about the most relevant elements of the signalling system and how each element could be considered as influencing signaller workload, the Repertory Grid Technique was used (L. Pickup, J. Wilson, E. Lowe (2009)) Card sorting The technique of card sorting, also called concept sorting , aims to understand how an individual sees the relationships between a set of concepts . By sorting cards (items, elements) into piles, the participant explores the different ways in which concepts are similar or different, using diverse system of sorting (categories, ordered hierarchy). Repeated many times, it can demonstrate many views of how knowledge is organised . The results can then be examined and analyzed to reveal relationships between concepts. SOURCE : card-sorting task with experienced and less experienced air traffic controllers, in order to examine whether the range of anticipation depends on the level of experience. A set of 30 realistic and comparable traffic sutations was presented, and the controllers' task was to sort the situations according to similarity. Results: experienced and less experienced controllers showed no relevant differences in classifying the situations. (C. Niessen, K. Eyferth T. Bierwagen (1999)). Observartional methods Observing is an important skill when doing ergonomics work. It is important to understant what should be observed, and how ? What? Observation is first useful to become familiar with a task, get experience of work in a natural situation, a real setting. It is about observing the actions, postures, bahaviours, techniques, reactions and many more aspects of an activity. More than just observing the participants, it is also essential to note the task sequences, timings, equipments and diverse resources, and the interactions between people. How? Observing of an activity and collecting data can be done in many ways. An observation session can either be formal or informal , where a formal approach imposes a large amount of structure and direction on what is to be observed, whereas an informal session is less structured and allows the observer considerable freedom in what information is gathered ad how it is recorded. The observation can also be either direct or indirect . A direct observation is when the observer is present during the experiment, in real time. Indirect observation happens by accessing previous recording. The role of the observer is a complex notion that can vary from simple observer to complete participant . While sometimes frowned upon from a scientifical point of view, the observer can decide to become a participant in the experiment. Participant observation presents advantages such as real experience for the observer (subjective experience), but also presents issues such as the analysis, that should already take place while collecting data. Participant observers can either hide the fact that they are carrying out a research ( the complete participant ), or make it clear from the start what is being observed ( the participant as observer ). Other possible roles are the marginal participant , when the observer plays the role of a passive participant in the \"background\", or the observer-as-participant , where the observer does not take part in the activity, but whose status as researcher is known to the participants. Issues Whichever observation scheme is chosen, issues regarding ethics are raised. It is necessary to obtain permission from those who are being observed and recorded. In the context of covert studies (when the participants are not informed), there are obvious and strong ethical objections, that make that kind of activity more and more rare. Another issue the possible lengthy periods of observations necessary, followed by a large amount of information to analyze, requiring sufficient skills in the subject. There might also be practical issues, like getting the right point of view, access to spectific equipement, etc. Finally, the risk of a bias due to participant being aware of the experience and changing their usual methods is high. Collecting data Collecting data during the experiment can be done with the following tools: Notes : writing down observations, thoughts and reflections, snippets of conservations, etc. While observing the activity, it is important to look out for the following details: Ecology : where the action takes place, layout of the space, location of actors and roles, equipments and artefacts used, Formal organisation : plans or procedures, Transitions events : important transition moments (arriving, leaving, etc), Arrangements of collaboration : interactions between members (who talks to whom about what), Audio-visual data indexing : times, sequence, making sense of digital data. Interviews : contextual interviews (not scripted), conversations about the the action conducted while observing it being done, Aduio-visual recordings : video cameras, audio recorders, still photography. Digital logs : messages, social media, conversations, systems, etc. Analysis of work and work systems Analyzing work is another important aspect of doing ergonomics work. But why do we analyze work? It is relevant to understand the existing systems , the task demands, its description, performance and the capabilities of people, to properly design new systems . It provides different ways of looking at work and systems through what must be done ( normative ), what is actually done ( descriptive ) and what could be done ( formative ). Terminology Understanding concepts such as task , goal and operation is the first step towards work analysis: Task : a set of activities occuring about the same time, sharing some common purpose that is recognised by a task performer. Tasks are usually seen as the smallest useful description of a work activity, and can be defined as a goal to be achieved under certain conditions and with certain resources, Goal : a state to be achieved or maintained by an actor at a particular time, a goal is the object or aim of an action, Operation : units of behaviour undertaken in order to achieve a goal, specified in terms of a target system state, Function : a mode of action or activity by which a product fulfills its purpose, Activity : various actions or conduct in a given context, sometimes distinct from the term task . Task analysis A task analysis is a study of what an operator is required to do, in terms of actions and cognitive processes to achieve a system goal. It is about describing activities, concerned with the examination of human performance in systems, both from the perspective of the behaviour of the human and the factors that shape performance. The fundamental components of task analysis are: Clarifying the problem and collecting relevant information, Analazing and representing the information, Defining future use, solutions and outputs. There are multiple forms of task analysis: Data collection methods : observation, critical incident, etc, Task description : representing data in specific formats (chart, hiearichical task analysis (HTA): decomposing a task into goals, sub-goals, operations and plans), Task simulation : computer modelling, computations, walk-throughs, Task behaviour assessment : eventualities, failures, effects, Task requirements evaluation : environment, checklists, surveys. SOURCE : \" Carrying out some form of human error analysis enables us to gain an understanding of how human interaction with drug administration tasks might lead to incidents \", in the context of the study of drug dosage errors, R. Lane et al. (2006) SHERPA (systematic human error reduction and prediction approach, Embery 1986): analyses tasks and identifies potential solutions to errors in a structured manner.","title":"SHP - Methods"},{"location":"SHP-course/#studying-human-performance-methods","text":"COURSE","title":"Studying Human Performance - Methods"},{"location":"SHP-course/#introduction","text":"Ergonomics is the scientific discipline cocerned with the understanding of interactions among humans and other elements of a system, in order to optimise human well-being and overall system performance. The aim of this course is to be able to analyse products and workplaces using a range of different methods, to compare and contrast different methodological approaches , to evaluate methods and statistical techniques, to demonstrate the application of specific methods in practical contexts ; in order to solve Human Factors-related problems.","title":"Introduction"},{"location":"SHP-course/#choosing-methods","text":"The understanding of methods is the core matter of this subject. A method is a \" way of proceeding or doing something \", and in Human Factors, allow for the collection of data. Methods need to be clear , structured and repeatable . The context of the studies in which methods are used define purposes, roles and data that make up for many different methods. It is hence very important to understand the context of these studies, and how it impacts the use of methods.","title":"Choosing methods"},{"location":"SHP-course/#study-types","text":"Three study types are observed: Laboratory , Simulation and Field . These types are compared with the following table: Laboratory/Simulation Field Advantages More control Real task Disadvantages Limited scope and accuracy Unplanned events (danger, interruptions)","title":"Study types"},{"location":"SHP-course/#affecting-factors","text":"In addition to the study type, there are numerous factors affecting the choice of methods: Amount of participants, Motivation ( research , consultancy ), Location, Objective, Types of desired results, Stakeholders, sponsors and funders.","title":"Affecting factors"},{"location":"SHP-course/#good-methods","text":"The choice of a method significantly depends on the context of the study. However it doesn't define the quality of a method. What is a good method? Five requirements define the quality of a method: Validity : does the method measure what it is supposed to measure? Reliability : is the method stable? (consistent over various measurements, between/within researchers), Sensitivity : is the method able to detect effects of interest? (the more sensible the more accurate), Usability : is the method easy to learn and implement? Resources : how expensive and demanding (cost, expertise) is the method? SOURCES : \"The notion of reliability thus includes both the characteristics of the instrument and the conditions under which it is administered \u2014 both must be consistent.\" (Oppenheim), \"If the reliability of a scale or other measure drops below .80 this means that repeated administrations will cover less than 64 per cent of the same ground, and that the error component is more than one-third; such a measure will come in for serious criticism and might well have to be discarded or rebuilt.\" (Oppenheim, where 0.8 is a correlation coefficient) Reliability may be measured in several different ways: test-retest , internal consistency , split-half or parallel-form ; each method using correlation coefficients. (Oppenheims) There are different types of validity: content validity (the content is well-balanced in the domain to be measured), concurrent validity (the method correlates with other well-validated measures of the same topic, administered at about the same time); construct validity (base a method on the fulfilment of well established predictions and theoretical assumptions), predictive validity (forecast some future criterion related to similar concepts).","title":"Good methods"},{"location":"SHP-course/#types-of-measure","text":"When doing a study, the type of measures defines the methods used and the data collected. In most ergonomics works, the following measure types are assessed: Performance : Task achivement, errors, Demands : Task time, physical and mental workload, Behaviour : Body movements, Knowledge : Subjective responses to a test situation, Opinions : Ratings, preferences.","title":"Types of measure"},{"location":"SHP-course/#method-types","text":"A simple classification splits all available methods into three categories : Observation, Self report, Use of archival data. For all methods, the type of output and validity are also differentiated. There is first the matter of objective or subjective validity: Objective : clamining the describe a true and correct reality, independent of those involved in the research process, Subjective : based on data derived from observations of events as they occur, or from interviews. Finally, the data can be split in two categories: Qualitative : data classification of objects based on attributes and properties, Quantitative : data measuread and expressed numerically.","title":"Method types"},{"location":"SHP-course/#planning-and-designing-studies","text":"","title":"Planning and designing studies"},{"location":"SHP-course/#defining-the-problem-aim-and-hypotheses","text":"The first step in designing a study is to clearly define the problem and issues that require investigation, and what will be achived by doing it. This is usually represented by a system of hypotheses , or statements of the predicted outcome . As studies manipulate variables and data, the aim of these hypotheses is to predict, understand and validate the relationships between said variables. Conventionally, there are two types of hypothesis: Experimental , or H1 , that predicts a dependent relationship between variables, Null , or H0 , that states that variables are not dependent (one does not impact the other).","title":"Defining the problem, aim and hypotheses"},{"location":"SHP-course/#identify-the-research-variables","text":"The said variables are split in three categories: Independant variables ( IV ): variables manipulated by the experimented, related to the individual, task, system or environment, Dependant variables ( DV ): variables being measured in the experiment, not under the control of the experimenter, possibly affected by the IVs, Controlled variables : variables that need to be kept constant during the experiment.","title":"Identify the research variables"},{"location":"SHP-course/#choose-the-research-setting","text":"The research setting (previously mentioned as study type ), needs to be considered through the advantages and disadvantages between Laboratory and Field studies, as well as if it is actually possible to implement one based on the nature of the study. The following table describes the key elements for choosing: Field Advantages Disadvantage Realistic Hard to set up Easier to generalise results May take longer Real participants Difficulty to access real participants -- Many uncontrolled variables -- Harder to control variables Laboratory Advantages Disadvantage Easy to set up Artificial Shorter Participants less motivated Easy to control variables May not be able to controll all extraneous variables Easy to perform comparative tests May be expensive","title":"Choose the research setting"},{"location":"SHP-course/#select-the-participants-sample","text":"It is important to properly select the sample , the small group that is representative of the population . This sample must be the most representative of the population possible, in order to make more general statements about the results. There are multiple types of sampling, depending on if the notion of probability accuracy is desired. The major representative sampling techniques are: Random ( sampling frame ): selecting random participants, with each participant having an equal chance of being selected, Systematic : selecting every nth participant, Stratified random : first diving the population into meaningful groups, then selecting randomly, Cluster : dividing the population into a number of units, or clusters, each of which contains individuals having a range of characteristics (can be extended to a multi-stage sampling). In other situtions, the participants are selected in a more favored way, when the basic selection techniques seen previously are not precise enough and the sample needs to be very specific. The non-probability sample techniques are: Quota : obtaining representatives of the various elements of a population, usually in the relative proportions in which they occur in the population (can be extended to a dimensional sampling), Purposive : satisfying the specific needs of a project, Conveniance sampling : nearest, easiest, cheapest, most convenient, Snowball : identifying one or more individuals from the population of interest, and then use them as informants to identify other members of the population, and so on. \" Convenience sampling is sometimes used as a cheap and dirty way of doing a sample survey. You do not know whether or not findings are representative \" (Robson). There are additional special sampling methods: Time samples , Homogeneous samples , Heterogeneous samples , Extreme case samples and Rare element samples . Finally, the resources and costs required to mobilize a sample need to be balanced with the potential results, and the confidence that the results will be significant.","title":"Select the participants sample"},{"location":"SHP-course/#defining-the-experimental-design","text":"The number and arrangement of the variables is an important part of the study. The aim is to ensure that the IVs are the only systematic difference between the experimental groups, also called the internal validity . It is also important to consider the measurement of the DVs, such as the data is commonly quantitative, easy to collect, valid, reliable, senstive and ethical.","title":"Defining the experimental design"},{"location":"SHP-course/#allocate-participants-to-the-experimental-conditions","text":"There are two ways to allocate and distribute the participants across the IVs: Within : every participant completes every experimental condition, Between : different participants are allocated to each experimental condition.","title":"Allocate participants to the experimental conditions"},{"location":"SHP-course/#ethics","text":"When doing studies with real participants, ethics play an important part in carrying out the experiments. It is required that they should give informed consent , based on sufficient information on the study (goals, duration, instruction, type of data collected, use of data). They should also be ensured protection from harm, privacy and a right to withdraw at any moment.","title":"Ethics"},{"location":"SHP-course/#conducting-the-study","text":"","title":"Conducting the study"},{"location":"SHP-course/#preparing-the-experiment","text":"Planning an preparing before conducting the study can be a lengthy process, but is necessary. Once designed, piloting the study with trial members allows to make sure everything is in order and as desired. The practical details such as appointements, timescales or idenfication must be thoroughly planned. Finally the various types of equipment required for the experiments must be properly prepared in order to conduct the study with consistency.","title":"Preparing the experiment"},{"location":"SHP-course/#analyzing-the-data","text":"Once the data is collected, analyzing it is a complex process. A common way to do so is the following: Transcribe , examine and outline the data using tools such as charts, tables ; observing patterns, distributions and tendencies, Identify the type of data: nominal (categories), ordinal (ordered) or interval (ordered with equal intervals). Statiscally test the data based on the purpose of the study and the experimental design, Draw conclusions .","title":"Analyzing the data"},{"location":"SHP-course/#presenting-results-and-drawing-conclusions","text":"Results can be represented visually through charts, figures and tables. Descriptive and inferential statistics are also commonly used. Based on the drawn conslusions ( reject , fail to reject ), a wider discussion must be made regarding the strength of the findings, and how they can be generalized to other contexts.","title":"Presenting results and drawing conclusions"},{"location":"SHP-course/#self-report-methods","text":"","title":"Self report methods"},{"location":"SHP-course/#questionnaire","text":"Questionnaires are good tools to survey large populations , while ensuring a high response rate . It is however a complex method that requires careful planning and proper question wording. Designing a questionnaire implies the following aspects to consider: Planning : as the questionnaire is an indirect method (no face to face), it is crucial to properly plan it to make the most of it, Sampling : thoroughly target the correct sample to optimize the results on the research question, General instructions : clear outline of what is expected from people filling the questionnaire, Question wording : make questions simple, specific, unambiguous, clear, not misleading, Type of response required : clarity on the structuring of responses (open, categories, multiple-choice, rating), Sequencing of questions : intuitive flow on the sequence of questions, Layout : easy of use, clear and appealing, Demographic details : defining what personal information is really needed, Piloting : trials to identify ambiguities, issues and errors, Administration : process to get access to the desired sample (distribution), Data collection : process to collect the completed questionnaires (postage, internet), Data entry and screening : conversion from questionnaire to usable data. There are also many factors to take into consideration to make sure people fill the questionnaire appropriately to obtain good data. Generally, people might not be aware of specific aspects of a task (they don't realize everything they do, and how) or not recall certain information about a task (frequency, exposure). The time since exposure may also affect what can be observed as memory could be altered. Finally, observers and participants can have differences in their understanding and perception of certain aspects. SOURCE : \"Most practitioners and researchers describe questionnaires as tools to gather information that is not readily accessible by examining an individual\u2019s performance or behavior\" (Moroney and Cameron, 2016), Example of question wording: impact of the verb (context of a car accident: smashed/collided/bumped/hit/contacted, each lead to different interpretation of the speed of the cars when the accident occured, impact on the mental model), (Loftus and Palmer, 1974), Perceived affordances and cultural conventions associated with Web site use, Kilograms/Pounds confusion in top 10 patient safety concern by the ECRI Institute (2015) \" If a questionnaire is to be a conversation, the designer and the respondent need to establish a relationship \" (Moroney and Cameron, 2016)","title":"Questionnaire"},{"location":"SHP-course/#interview","text":"Interviews, unlike questionnaires, happen face to face (or through vocal exchange) with a participant. It is a useful tool to collect detailed task related information, and thoroughly collect data with flexibility . However this method requires more time, and can be costly. Interviews are usually targeted to smaller samples . The aspects to consider for an interview are very similar to those for the questionnaires, but from a more practical point of view. Generally, an interview will require different planning regarding the instructions, the administration, and the data collection. Setting up the meeting, the location and the recording is important. It is also essential to prepare for the interaction with the participant, and behave appropriately to make sure the results are optimized. Finally, choosing the right type of interview is crucial. There are four types of interview: Exploratory : free-style in-depth, Structured : each interview is presented with exactly the same questions in the same order (no diversion), Semi-structured : open, allowing new ideas to be brought up during the interview, generally through a framework of themes to be explored, Group : multiple participants at once.","title":"Interview"},{"location":"SHP-course/#exploratory","text":"Exploratory interviews are used to \" probe \" participants and develop concepts by understanding how people think and feel about specific topics. These interviews are \" free-style \" and spontaneous, with minimal intervention from the researcher.","title":"Exploratory"},{"location":"SHP-course/#structured","text":"A structured interview is the most quantitative type of interview, providing a strong confidence in the results. By asking the exact same questions to every interviewee, a consistency is kept and the risk of diverting is the lowest.","title":"Structured"},{"location":"SHP-course/#semi-structured","text":"Semi-structured interviews address general themes and core questions, but allow for the interview to divert to broader and external contexts. Generally these interviews still require a set of questions regarding certain topics to always ensure a beneficial method. Being less \" accurate \", these interviews are widely used in qualitative research.","title":"Semi-structured"},{"location":"SHP-course/#group","text":"Group interviews gather multiple participants, and sometimes multiple interviewers. The first benefit in doing such an interview is the obvious gain of time by combining participants. Of course a potential issue with this is the lack of depth, and the difficulty to maintain a structured interview. It is hence very important to limit the group interview to a maximum amount of participants and interviewers. The practical aspects must also be considered: group disposition, turn taking and general order.","title":"Group"},{"location":"SHP-course/#other-interview-techniques","text":"Critical incident technique : set of procedures used for collecting direct observations of human behavior that have critical significance and meet methodically defined criteria. These observations are used to solve practical problems and develop broad psychological principles. A critical incident can be described as one that makes a contribution, either positively or negatively, to an activity. Critical decision method : retrospective interview method that employs a set of cognitive probes to non-routine incidents that required expert judgment or decision making. Verbal protocol reports : concurrent and retrospective protocol reports used with cognitive tasks, to determine what workers are thinking about during tasks.","title":"Other interview techniques"},{"location":"SHP-course/#eliciting-preferences","text":"Understanding the way people perceive, compare and prefer systems and products is an essential part of a design process. There are tools and methods used to identify these aspects in diverse contexts.","title":"Eliciting preferences"},{"location":"SHP-course/#paired-comparisons","text":"Henry Scheff\u00e9 When comparing multiple systems, the paired comparisons method can be used to rank and identify general user preferences . This method consists in presenting all possible pairs of the system (product) to every participant, in a random sequence. Each time, the participant must indicate which system was preferred between the two. SOURCE : used to compare car seat comfort, through a subjective scale (-3 to +3). Significant results were found regarding the stiffness and foam hardness. (Kazushige Ebe Micheal J. Griffin (2001)).","title":"Paired comparisons"},{"location":"SHP-course/#identifying-the-pairs","text":"For \\(n\\) objects, there are \\( \\frac{1}{2} \\cdot n \\cdot (n - 1)\\) possible pairs. In the example of 4 chairs, there a \\(1/2 * 4 * 3 = 6\\) possible pairs, that can be \\({AB, AC, BD, CB, CD, DA}.\\)","title":"Identifying the pairs"},{"location":"SHP-course/#filling-the-preference-matrix","text":"The preference matrix represents every pair and how many participants prefered one object to another (\\(x\\) prefered to \\(y\\)). The following preference matrix for the chairs, with 14 participants, is given: A B C D A - 6 8 12 B 8 - 10 14 C 6 4 - 13 D 2 0 1 -","title":"Filling the preference matrix"},{"location":"SHP-course/#computing-the-probability-matrix","text":"To obtain results and identify the most (or least) preferred object, the probability matrix must be computed. From the preference matrix, every item must be divided by the total number of participants \\(N\\), and empty items (same objects) replaced by \\(0.50\\). A B C D A 0.50 0.43 0.57 0.86 B 0.57 0.50 0.71 1.00 C 0.43 0.29 0.50 0.93 D 0.14 0.00 0.07 0.50 \\(\\sum\\) 1.64 1.22 1.85 3.29 Using the last row, the sum of probabilities, the most preferred chair is \\(D\\), and the least preferred one is \\(B\\).","title":"Computing the probability matrix"},{"location":"SHP-course/#relative-ranking","text":"To rank the objects in relative levels, the use of standard scores, or z-scores , is required. To do so, the distance from the mean, here \\(0.50\\) must be extracted and used in the normal distribution table to get the z-score . If an item is above the mean, the z-score is positive, if it is under the mean the z-score is negative, and null otherwise. A B C D A 0.00 -0.18 0.18 1.06 B 0.18 0.00 0.55 4.00 C -0.18 -0.55 0.00 1.48 D -1.08 -4.00 -1.48 0.00 \\(\\bar{z}\\) -0.27 -1.18 -0.19 1.64 The last row, the mean of the z-scores , represents a relative ranking: \\[ B(-1.18) A(-0.27) C(-0.19) D(1.64) \\]","title":"Relative ranking"},{"location":"SHP-course/#repertory-grid","text":"George Kelly, 1955 The repertory grid is a tool to identify what constructs are important to a person in a system, product, job or establishement. It helps to understand and indivual's interpretation of his or her environment. Using a method of triads , this method compares elements of a topic (better, worse, extremes). These elements are put in a table that represents a person's perception of a certain topic. The data used to rate and rank the elements can be qualitative and quantitative. In practice, the repertory grid is a structured and thorough method , and can be applied to many contexts. Ideally used early in the design process, it might required more time to train the participaints and make sure the results are valid and reliable. SOURCE : In order to elicit and explore the knowledge held by rail signalling experts about the most relevant elements of the signalling system and how each element could be considered as influencing signaller workload, the Repertory Grid Technique was used (L. Pickup, J. Wilson, E. Lowe (2009))","title":"Repertory grid"},{"location":"SHP-course/#card-sorting","text":"The technique of card sorting, also called concept sorting , aims to understand how an individual sees the relationships between a set of concepts . By sorting cards (items, elements) into piles, the participant explores the different ways in which concepts are similar or different, using diverse system of sorting (categories, ordered hierarchy). Repeated many times, it can demonstrate many views of how knowledge is organised . The results can then be examined and analyzed to reveal relationships between concepts. SOURCE : card-sorting task with experienced and less experienced air traffic controllers, in order to examine whether the range of anticipation depends on the level of experience. A set of 30 realistic and comparable traffic sutations was presented, and the controllers' task was to sort the situations according to similarity. Results: experienced and less experienced controllers showed no relevant differences in classifying the situations. (C. Niessen, K. Eyferth T. Bierwagen (1999)).","title":"Card sorting"},{"location":"SHP-course/#observartional-methods","text":"Observing is an important skill when doing ergonomics work. It is important to understant what should be observed, and how ?","title":"Observartional methods"},{"location":"SHP-course/#what","text":"Observation is first useful to become familiar with a task, get experience of work in a natural situation, a real setting. It is about observing the actions, postures, bahaviours, techniques, reactions and many more aspects of an activity. More than just observing the participants, it is also essential to note the task sequences, timings, equipments and diverse resources, and the interactions between people.","title":"What?"},{"location":"SHP-course/#how","text":"Observing of an activity and collecting data can be done in many ways. An observation session can either be formal or informal , where a formal approach imposes a large amount of structure and direction on what is to be observed, whereas an informal session is less structured and allows the observer considerable freedom in what information is gathered ad how it is recorded. The observation can also be either direct or indirect . A direct observation is when the observer is present during the experiment, in real time. Indirect observation happens by accessing previous recording. The role of the observer is a complex notion that can vary from simple observer to complete participant . While sometimes frowned upon from a scientifical point of view, the observer can decide to become a participant in the experiment. Participant observation presents advantages such as real experience for the observer (subjective experience), but also presents issues such as the analysis, that should already take place while collecting data. Participant observers can either hide the fact that they are carrying out a research ( the complete participant ), or make it clear from the start what is being observed ( the participant as observer ). Other possible roles are the marginal participant , when the observer plays the role of a passive participant in the \"background\", or the observer-as-participant , where the observer does not take part in the activity, but whose status as researcher is known to the participants.","title":"How?"},{"location":"SHP-course/#issues","text":"Whichever observation scheme is chosen, issues regarding ethics are raised. It is necessary to obtain permission from those who are being observed and recorded. In the context of covert studies (when the participants are not informed), there are obvious and strong ethical objections, that make that kind of activity more and more rare. Another issue the possible lengthy periods of observations necessary, followed by a large amount of information to analyze, requiring sufficient skills in the subject. There might also be practical issues, like getting the right point of view, access to spectific equipement, etc. Finally, the risk of a bias due to participant being aware of the experience and changing their usual methods is high.","title":"Issues"},{"location":"SHP-course/#collecting-data","text":"Collecting data during the experiment can be done with the following tools: Notes : writing down observations, thoughts and reflections, snippets of conservations, etc. While observing the activity, it is important to look out for the following details: Ecology : where the action takes place, layout of the space, location of actors and roles, equipments and artefacts used, Formal organisation : plans or procedures, Transitions events : important transition moments (arriving, leaving, etc), Arrangements of collaboration : interactions between members (who talks to whom about what), Audio-visual data indexing : times, sequence, making sense of digital data. Interviews : contextual interviews (not scripted), conversations about the the action conducted while observing it being done, Aduio-visual recordings : video cameras, audio recorders, still photography. Digital logs : messages, social media, conversations, systems, etc.","title":"Collecting data"},{"location":"SHP-course/#analysis-of-work-and-work-systems","text":"Analyzing work is another important aspect of doing ergonomics work. But why do we analyze work? It is relevant to understand the existing systems , the task demands, its description, performance and the capabilities of people, to properly design new systems . It provides different ways of looking at work and systems through what must be done ( normative ), what is actually done ( descriptive ) and what could be done ( formative ).","title":"Analysis of work and work systems"},{"location":"SHP-course/#terminology","text":"Understanding concepts such as task , goal and operation is the first step towards work analysis: Task : a set of activities occuring about the same time, sharing some common purpose that is recognised by a task performer. Tasks are usually seen as the smallest useful description of a work activity, and can be defined as a goal to be achieved under certain conditions and with certain resources, Goal : a state to be achieved or maintained by an actor at a particular time, a goal is the object or aim of an action, Operation : units of behaviour undertaken in order to achieve a goal, specified in terms of a target system state, Function : a mode of action or activity by which a product fulfills its purpose, Activity : various actions or conduct in a given context, sometimes distinct from the term task .","title":"Terminology"},{"location":"SHP-course/#task-analysis","text":"A task analysis is a study of what an operator is required to do, in terms of actions and cognitive processes to achieve a system goal. It is about describing activities, concerned with the examination of human performance in systems, both from the perspective of the behaviour of the human and the factors that shape performance. The fundamental components of task analysis are: Clarifying the problem and collecting relevant information, Analazing and representing the information, Defining future use, solutions and outputs. There are multiple forms of task analysis: Data collection methods : observation, critical incident, etc, Task description : representing data in specific formats (chart, hiearichical task analysis (HTA): decomposing a task into goals, sub-goals, operations and plans), Task simulation : computer modelling, computations, walk-throughs, Task behaviour assessment : eventualities, failures, effects, Task requirements evaluation : environment, checklists, surveys. SOURCE : \" Carrying out some form of human error analysis enables us to gain an understanding of how human interaction with drug administration tasks might lead to incidents \", in the context of the study of drug dosage errors, R. Lane et al. (2006) SHERPA (systematic human error reduction and prediction approach, Embery 1986): analyses tasks and identifies potential solutions to errors in a structured manner.","title":"Task analysis"},{"location":"SHP-stats/","text":"Studying Human Performance - Statistics COURSE Basics Data types There are three types of data to consider when doing statistics: Categorical : data classed into categories, Ordinal : data that can only be classed by order, Ratio/Interval : data that can be put on a scale where the relationship between each point is known. Measures of central tendency There are three measures of central tendency to take into account: mean , median and mode . The mean is the average of all numbers and is sometimes called the arithmetic mean , and is generally best for interval data. In a sample with \\(n\\) elements such as \\(x_1, x_2,..., x_n\\), the mean noted \\(\\bar{x}\\) is the sum of all the values divided by the numbers of items: \\[\\bar{x} = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} x_i\\] The median is the value separating the higher half from the lower half of a data sample, commonly thought of as the \" middle \" value. Finding the median in a sample only requires the sample to be ranked (sorted, ordered). The median is best to use with ordinal data . The basic advantage of the median compared to the mean is that it is not skewed so much by extremely large or small values, and so it may give a better idea of a \" typical \" value. The mode of a set of values is the value that appears most often, the most common entry, best measure for categorical data . Normal distribution Definition The normal distribution (also called Gaussian or the bell curve ), is a very common continuous probability distribution. Normal distributions are very important in statistics and are often used to represent real-valued random variables whose distributions are not known. The normal distribution is useful because of the central limit theorem stating that averages of samples converge in distribution to the normal, that is, they become normally distributed when the number of observations is sufficiently large. In simpler words, when observing data from large samples, in many cases, the data tends to be around a central value with no bias left or right. This distribution is found in many naturally occurring phenomena such as heights of people , size of objects produced by machines , errors in measurements , blood pressure , marks on a test and many more. The properties of a normal distribution allow to easiliy manipulate variables and compute significant results. A normally distributed sample is a sample with equal measures of central tendecy, that is when \\(mean = median = mode\\). When this is not the case, the data sample is said to be skewed . A sample can be negatively skewed (\\(mean median mode\\)) or positively skewed (\\(mean median mode\\)). It is not appropriate to perform various statistical tests on a skewed sample. When the sample is normally distributed with a perfect symetry, exactly 50% of the values are lower than the mean, and exactly 50% are higher. A normal distribution is noted \\(\\mathcal{N}(\\mu, \\sigma)\\) with \\(\\mu\\) the mean, \\(\\sigma\\) the standard deviation and \\(\\sigma^2\\) the variance. Standard deviation The standard deviation, note \\(\\sigma\\), is a measure defining how spread out the numbers of a sample are. A low standard deviation indicates that the data points tend to be close to the mean , while a high standard deviation indicates that the data points are spread out over wider range of values . This measure is the square root of the variance . But what is the variance? The variance is defined as the squared differences from the mean . Informally, the variance measures how far a set of numbers are spread out from their average value. For a sample with \\(n\\) values, it is calculated with the following formula: \\[\\sigma^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\mu)^2}{n-1}\\] The standard deviation is therefore calculated with: \\[\\sigma = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\mu)^2}{n-1}}\\] Once the mean and the standard deviation are known, a perfect description of the distribution is given. Generally we find that: Standard scores The standard deviation is a useful tool to generally examine a sample and contextualize values in 1 to 3 standard deviation units from the mean. However, how to know how many standard deviations a specific value is from the mean? To convert a value to a standard deviation value (also called a standard score or z-score ): Substract the mean to the value ( how far from the mean? ), Divided by the standard deviation ( how many standard deviations? ). Hence for a value \\(x\\), the z-score \\(z\\) is: \\[z = \\frac{x - \\mu}{\\sigma}\\] Doing that operation is called standardizing , or converting a normal distribution into a standard normal distribution . Usually, the entire sample is standardized at once to simplify the calculations by simply looking up standardized pre-computed tables. The table D. Normal distribution table allows to convert from standard scores to percents, and vice versa. Statistical tests When doing an experiment, choosing the correct statistical test is very important to obtain significant results. The first step in doing so is to understand every factor affecting the choice, and how to implement study design . Experiment outline An experiment, (or study or research), can be outlined by the following steps: Research question : objective, aim, question asked, Study design : describing the experiment through hypothesis, variables, data types and participants, Conducting the experiment : doing the described experiment and collecting data, Test selection : choosing the correct test(s) based on the study design, Doing the test : using the data to obtain results through the selected test(s), Interpretation and conclusion : making sense of the result and concluding on the research question. Study design Defining the problem, aim and hypotheses The first step in designing a study is to clearly define the problem and issues that require investigation, and what will be achived by doing it. This is usually represented by a system of hypotheses , or statements of the predicted outcome . As studies manipulate variables and data, the aim of these hypotheses is to predict, understand and validate the relationships between said variables. Conventionally, there are two types of hypothesis: Experimental , or H1 , that predicts a dependent relationship between variables, Null , or H0 , that states that variables are not dependent (one does not impact the other). Identify the research variables The said variables are split in three categories: Independant variables ( IV ): variables manipulated by the experimented, related to the individual, task, system or environment, Dependant variables ( DV ): variables being measured in the experiment, not under the control of the experimenter, possibly affected by the IVs, Controlled variables : variables that need to be kept constant during the experiment. Allocate participants to the experimental conditions There are two ways to allocate and distribute the participants across the IVs: Within : every participant completes every experimental condition, Between : different participants are allocated to each experimental condition. Assumptions and planned comparissons The experimental hypothesis predicts a dependant relationship between variables, or in other words, independant variables impacting dependant variables. When a significant interaction is found, it is called a main effect . A study design can however further predictions by using assumptions, planned comparisons and post hocs . A one-tailed hypothesis predicts how and in which direction an independant variable impacts a dependant variable. For example, if an experimental hypothesis predicts that music (IV) impacts performance (DV), a one-tailed extension would add that music improves performance , or that music impairs performance . One-tailed tests must be justified: it is for example justified to predict that constant loud noise impairs performance. In addition, planned comparisons and post hoc are other comparisons that can be integrated to a study design. Unlike one-tailed assumptions that predict in which direction an independant variable affects a dependant variable, these computations compare the levels of an independant variable. For example, if an experimental hypothesis predicts that music (IV) impacts performance (DV), and that this independant variable has three levels ( classical , rock , rap ), contrasts could be that classical music affects performance differently than rap music . These potential interactions are called simple effects . These comparisons can be done in two ways: Contrasts : they are planned comparisons, with hypotheses in mind, a guess in which direction the comparisons will result. They are specifically planned before the tests , Post hoc : if no specific prediction is made, these comparisons are made after the test, comparing everything to possibly detect significant differences. Data types and analysis Based on the data types and the variables, a test can either be parametric or non-parametric . A parametric test is a test that is carried out with the assumption that the data collected follows a well-known distribution (usually the normal distribution), which can be boiled down to the knowledge of just a couple of parameters . A parametric test provides generalisations for making statements about the mean of the parent population. Because parametric test are based on a distribution, the measurement of variables on interval or ratio level, with the mean used as the measure of central tendency. On the other hand, a non-parametric test has no known information about the population and is not based on any known distribution. Because non-parametric tests don't require assumptions about the nature of their distributions, they are also called distribution-free . These tests hence assume that the variables are measured on a nominal (categorical) or ordinal level, with the median as the measure of central tendency. Significance Statistical tests try to answer a question, based on the available data. When rejecting a null hypothesis (observing that there is indeed a relation between variables), there is a probability that this rejection is due to chance , that the random variables happen to misrepresent the reality . This probability, called p-value , represents the chance to reject a null hypothesis when it is actually true . Subsequently, the lower the p-value, the more meaningful the result because it is less likely to be caused by noise . A statistically significant result is one in which we believe the result was as a result of the experimental manipulation rather than chance alone. The action to reject the null hypothesis when it is actually true is called an incorrect conclusion and more precisely a Type I error . It is an experimental error that analysts wants to avoid in priority, to not make any false claim. To that end, before conducting the test, a significance threshold called \\(\\alpha\\) is set, that if exceeded, indicates a non-significant result. That threshold is usually \\(\\alpha = 0.05\\), but particular contexts and fields of study often require a more strict value, such as \\(\\alpha = 0.001\\). Of course, the opposite action of failing to reject a null hypothesis when it is actually false is another experimental error called Type II error , but is often seen as less serious than the Type I error. The chance to make a Type II error is represented by the probability called \\(\\beta\\). The statistical power is the probability of correctly rejecting the null hypothesis, when it is indeed false . Statistical power Statistical power is the probability of correctly rejecting a false null hypothesis. Statistical power is inversely related to \\(\\beta\\) or the probability of making a Type II error . In short, \\(power = 1 - \u03b2\\). In simpler words, statistical power is the likelihood that a study will detect an effect when there is an effect there to be detected. If statistical power is high, the probability of making a Type II error, or concluding there is no effect when, in fact, there is one, goes down. As a rule of thumb, ideal statistical power is assumed when \\(\\geq 0.8\\). Statistical power power nearly always depends on the following three factors: the statistical significance criterion used in the test: usually named \\(\\alpha\\) and set at 0.05, this criterion represent the likelihood to reject a null hypothesis when it is actually true, the magnitude of the effect: the effect size, is a quantitative measure of the magnitude of a phenomenon. Examples of effect sizes are the correlation between two variables, the regression coefficient in a regression, the mean difference, etc. In simple words, while the significance provides the knowledge of the presence of a phenomenon, the effect size describes how much is that phenomenon occuring, the sample size : it determines the amount of sampling error inherent in a test result. The statistical power is therefore often used to determine the ideal sample size (larger effect and sample sizes improve power). In addition, the type of test can play a part in the power, as parametric tests are generally more powerful than non-parametrics ones. Correlation, regression Most tests aim to know if an independant variable has an impact on a dependant variable, but doesn't describe how . The study of correlation aims to measure the association or the absence of relationship between two variables, without taking into account if they are independant or dependant. Regression analysis on the other hand predicts the value of a dependant variable based on the known value of an independant variable. Correlation is represented by a coefficient \\(r\\). This coefficient can vary from \\(-1\\) to \\(+1\\). A \\(-1\\) indicates a perfect negative correlation , while a \\(+1\\) indicates a perfect positive correlation . A correlation of \\(0\\) means there is no relationship between the two variables. When there is a negative correlation between two variables, as the value of one variable increases, the value of the other variable decreases, and vise versa. In other words, for a negative correlation, the variables work opposite each other . When there is a positive correlation between two variables, as the value of one variable increases, the value of the other variable also increases, the variables move together . A rule of thumb to interpret \\(r\\) is: \\(+-1\\): perfect relationship, \\(+-0.7\\): strong relationship, \\(+-0.5\\): moderate relationship, \\(+-0.3\\): weak relation ship, \\(0\\): no relationship. While \\(r\\) represents the strength of the linear relationship between two variables, \\(r^2\\) is generally better to report correlation in ergonomics. Known as the coefficient of determination , \\(r^2\\) represents the proportion of variance of one variable that is predicted from another variable (e.g. 80% of variation in Y can be explained by its relationship with X, while 20% remain unexplained). The coefficient \\(r\\) can be calculated with: Spearman test (non-parametric), Pearson test (parametric). In simple linear regression , we predict scores of one variable from the scores of a second variable. The variable we are predicting is called the criterion variable and is referred to as \\(Y\\). The variable we are basing our predictions on is called the predictor variable and is referred to as \\(X\\). When there is only one predictor variable, the prediction method is called simple regression . In simple linear regression, the predictions of Y when plotted as a function of X form a straight line. Linear regression consists of finding the best-fitting straight line through the points, called a regression line. Causality It is very important to understand that correlation does not imply causation . When two variables are found to be correlated, it is tempting to assume that this shows that one variable causes the other. It is however not the case, and this assumption is considered a questionable cause logical fallacy . For any two correlated variables \\(A\\) and \\(B\\), the different possible relationships include: \\(A\\) causes \\(B\\) (direct causation), \\(B\\) causes \\(A\\) (reverse causation), \\(A\\) and \\(B\\) are consequences of a common cause, but do not cause each other, \\(A\\) and \\(B\\) both cause \\(C\\), which is explicitely of implicitely conditioned on, \\(A\\) causes \\(B\\) and \\(B\\) causes \\(A\\) (bidirectional or cyclic causation), \\(A\\) causes \\(C\\) which causes \\(B\\) (indirect causation), There is no connection between \\(A\\) and \\(B\\), the correlation is a coincidence. Thus there can be no conclusion made regarding the existence of a cause-and-effect relationship only from the fact that A and B are correlated. Determining whether there is an actual cause-and-effect relationship requires further investigation , even when the relationship between A and B is statistically significant, a large effect size is observed, or a large part of the variance is explained. Non-parametric tests Wilcoxon One IV, Two Levels, Within A set of \\(N = 12\\) subjects are asked to rate two designs of screwdriver handle for ease of use on a seven point ordinal scale (7 = easiest, 1 = most difficult) and the data obtained are shown in the table below: Subject Handle A (\\(X_1\\)) Handle B (\\(X_2\\)) S1 6 3 S2 5 6 S3 7 4 S4 4 4 S5 6 5 S6 4 1 S7 3 5 S8 5 2 S9 7 2 S10 5 2 S11 6 2 S12 6 4 First start by calculating the differences between each pair of scores : Subject Handle A (\\(X_1\\)) Handle B (\\(X_2\\)) \\(X_1\\) - \\(X_2\\) S1 6 3 3 S2 5 6 -1 S3 7 4 3 S4 4 4 0 S5 6 5 1 S6 4 1 3 S7 3 5 -2 S8 5 2 3 S9 7 2 5 S10 5 2 3 S11 6 2 4 S12 6 4 2 The next step is to ignore null differences (such as for S4), and hence decrease to \\(N = 11\\) subjects. The differences must then be ranked in order of magnitude (take mean for similar differences) in \\([1, N]\\), as follows: Order 1 2 3 4 5 6 7 8 9 10 11 Difference 1 1 2 2 3 3 3 3 3 4 5 Rank 1.5 3.5 7 10 11 Then, assign in the previous table the rank to each difference, conserving their sign : Subject Handle A (\\(X_1\\)) Handle B (\\(X_2\\)) \\(X_1\\) - \\(X_2\\) Rank S1 6 3 3 7 (+) S2 5 6 -1 1.5 (-) S3 7 4 3 7 (+) S4 4 4 0 S5 6 5 1 1.5 (+) S6 4 1 3 7 (+) S7 3 5 -2 3.5 (-) S8 5 2 3 7 (+) S9 7 2 5 11 (+) S10 5 2 3 7(+) S11 6 2 4 10 (+) S12 6 4 2 3.5 (+) Calculate the total of negative ranks \\(T_n\\) and the total of positive ranks \\(T_p\\): \\(T_n = 5\\) \\(T_p = 61\\) Finally, select from these two totals the lowest one , that is the value \\(W_{obs}\\). In this example \\(min(T_n, T_p) = 5\\), therefore \\(W_{obs} = 5\\). In the table A , search for \\(N = 11\\), with a level of significance for a two-tailed test of \\(\\alpha = 0.05\\), that value is \\(W_{crit} = 11\\). Because \\(W_{obs} W_{crit}\\), the null hypothesis is rejected (there is a difference of ease of use between the two designs of handles). Mann-Whitney One IV, Two Levels, Between Two groups of 7 subjects (total of \\(N = 14\\)) are asked to complete a maintenance task using either a paper based manual for instructions or a head mounted display on a \u201cprivate eye\u201d. After the task they rate how easy they found several aspects of the task, giving an overall usability rating of between 4 and 20 (20 = easiest to use, 4 = most difficult) . Subject Paper based Subect Private eye S1 8 S8 11 S2 10 S9 15 S3 6 S10 18 S4 15 S11 14 S5 14 S12 16 S6 9 S13 12 S7 10 S14 9 First rank the scores by order of magnitude (take mean for similar differences) in \\([1, N]\\) as follows: Order 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Score 6 8 9 9 10 10 11 12 14 14 15 15 16 18 Rank 1 2 3.5 5.5 7 8 9.5 11.5 13 14 Assign the rank to each score in the previous table and compute the totals: Subject Paper based Rank Subect Private eye Rank S1 8 2 S8 11 7 S2 10 5.5 S9 15 11.5 S3 6 1 S10 18 14 S4 15 11.5 S11 14 9.5 S5 14 9.5 S12 16 13 S6 9 3.5 S13 12 8 S7 10 5.5 S14 9 3.5 TOTAL 38.5 66.5 The total rank for paper based is \\(T_{pb} = 38.5\\), and the total rank for private eye is \\(T_{pe} = 66.5\\). Select from the two rank totals the highest one and assign to the variable \\(T_x\\). \\[T_x = max(T_{pb}, T_{pe}) = 66.5\\] Using the following formulate, calculate the value of \\(U_{obs}\\): \\[U_{obs} = n_1 \\cdot n_2 + \\frac{n_x \\cdot (n_x + 1)}{2} - T_x\\] where: \\(n_1\\) is the number of subjects in the group 1 (here \\(n_1 = 7\\)), \\(n_2\\) is the number of subjects in the group 2 (here \\(n_2 = 7\\)), \\(n_x\\) is the number of subjects in the group with the the highest rank total (here \\(n_x = 7\\)). Therefore: \\[U_{obs} = 7 \\cdot 7 + \\frac{7 \\cdot (7 + 1)}{2} - 66.5 = 10.5\\] In the table B , search for \\(n_1 = n_2 = 7\\), with a level of significance for a two-tailed test of \\(\\alpha = 0.05\\), that value is \\(U_{crit} = 8\\). Because \\(U_{obs} U_{crit}\\), we fail to reject the null hypothesis (it can't be said that there is a difference between the two conditions). Friedman One IV, Three+ levels, Within A group of \\(n = 6\\) students are asked to use three different types of computer interface one which is solely command line based, one which uses a combination of command line and pull down menus, and one which only uses pull down menus . They are then asked to rate the usability of these interfaces on a five point scale (5 = most usable) . The following scores are obtained: Subject Cmd line Combination Menus S1 2 4 2 S2 1 5 3 S3 3 5 2 S4 2 3 3 S5 2 4 3 S6 1 3 4 For each separate subect, rank the three scores by order of magnitude as follows for Subject 1 (S1) : Order 1 2 3 Score 2 2 4 Rank 1.5 3 Assign the ranks to every score in the previous table and compute the rank totals: Cmd line Combination Menus Subject Score Rank Score Rank Score Rank S1 2 1.5 4 3 2 1.5 S2 1 1 5 3 3 2 S3 3 2 5 3 2 1 S4 2 1 3 2.5 3 2.5 S5 2 1 4 3 3 2 S6 1 1 3 2 4 3 Total 7.5 16.5 12 We note the rank totals for each condition \\(T_{c1} = 7.5\\), \\(T_{c2} = 16.5\\), \\(T_{c3} = 12\\). The next step is to calculate the value \\(Xr^2_{obs}\\) with the following formula: \\[Xr^2_{obs} = \\bigg[ \\frac{12}{N \\cdot C \\cdot (C + 1)} \\cdot \\sum T_c^2 \\bigg] - 3 \\cdot N \\cdot (C + 1)\\] where: \\(N\\) is the number of subjects (here \\(N = 6\\)), \\(C\\) is the number of levels (here \\(C = 3\\)), \\(\\sum T_c^2\\) is the sum of the square rank totals for each condition. Therefore: \\[\\sum T_c^2 = T_{c1}^2 + T_{c2}^2 + T_{c3}^2 = 7.5^2 + 16.5^2 + 12^2 = 472.5\\] \\[Xr^2_{obs} = \\bigg[ \\frac{12}{6 \\cdot 3 \\cdot (3 + 1)} \\cdot 472.5 \\bigg] - 3 \\cdot 6 \\cdot (3 + 1) = 6.75\\] In the table C , search for \\(N = 6\\) and \\(C = 3\\), that value is \\(Xr^2_{crit} = 6.33\\). Because \\(Xr^2_{obs} Xr^2_{crit}\\), we can reject the null hypothesis (there is a difference between the three conditions). Kruskal-Wallis One IV, Three+ levels, Between Three groups of seven subjects (total of \\(N = 21\\)) were asked to rate any discomfort they experienced after typing on a keyboard for 20 minutes. The keyboards were set up so that they required three different levels of force to be exerted onto the keys. Therefore the three conditions were low force , medium force and high force . The subjects rated their discomfort on a 9 point scale, where a high score indicated a greater degree of discomfort . Subject Low force Subect Medium force Subject High force S1 4 S8 5 S15 6 S2 3 S9 4 S16 8 S3 1 S10 2 S17 9 S4 2 S11 5 S18 7 S5 2 S12 4 S19 9 S6 3 S13 3 S20 9 S7 1 S14 6 S21 8 First rank the scores by order of magnitude (take mean for similar differences) in \\([1, N]\\) as follows: Order 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Score 1 1 2 2 2 3 3 3 4 4 4 5 5 6 6 7 8 8 9 9 9 Rank 1.5 4 7 10 12.5 14.5 16 17.5 20 Assign the rank to each score in the previous table and compute the totals: Low force Medium force High force Subject Score Rank Subject Score Rank Subject Score Rank S1 4 10 S8 5 12.5 S15 6 14.5 S2 3 7 S9 4 10 S16 8 17.5 S3 1 1.5 S10 2 4 S17 9 20 S4 2 4 S11 5 12.5 S18 7 16 S5 2 4 S12 4 10 S19 9 20 S6 3 7 S13 3 7 S20 9 20 S7 1 1.5 S14 6 14.5 S21 8 17.5 TOTAL 35 70.5 125.5 We note the rank totals for each condition \\(T_{c1} = 35\\), \\(T_{c2} = 70.5\\), \\(T_{c3} = 125.5\\), and the number of subjects for each condition \\(n_1 = n_2 = n_3 = 7\\). The next step is to calculate the value \\(H_{obs}\\) with the following formula: \\[H_{obs} = \\bigg[ \\frac{12}{N \\cdot (N + 1)} \\cdot \\sum \\frac{T_c^2}{n_c} \\bigg] - 3 \\cdot (N + 1)\\] where: \\(N\\) is the total number of subjects (here \\(N = 21\\)), \\(\\sum \\frac{T_c^2}{n_c}\\) is the sum of squared rank totals for each condition divided by the number of participants in that condition. \\[\\sum \\frac{T_c^2}{n_c} = \\frac{35^2}{7} + \\frac{70.5^2}{7} + \\frac{125.5^2}{7} = 3135.08\\] Therefore: \\[H_{obs} = \\bigg[ \\frac{12}{21 \\cdot (21 + 1)} \\cdot 3135.08 \\bigg] - 3 \\cdot (21 + 1) = 15.51\\] Finally the number of degrees of freedom must be computed as \\(df = C - 1 = 3 - 1 = 2\\), where \\(C\\) is the number of conditions (levels). To find \\(H_{crit}\\) the number of participants in each group determines the table to use: if all groups have more than 5 subjects, the Chi-square table should be used, otherwise the Kruskal-Wallis one. Here \\(n_1 = n_2 = n_3 5\\), so the Chi-square table should be used. In the table D , search for \\(df = 2\\) and a significance level of \\(\\alpha = 0.05\\), that value is \\(H_{crit} = 5.99\\). Because \\(H_{obs} H_{crit}\\), we can reject the null hypothesis (keyboard force does affect comfort). One-Way Chi Square One IV, Two+ Levels The Chi Square is the only test for categorical data . Usually measuring frequencies of events (how many subjects made an error, how many passed a test etc) in different conditions, this test aims to uncover if the condition had an impact on the frequencies, or if the null hypothesis is true, that is that each condition should be almost equally distributed. A test was performed to compare the efficiency of different methods of learning for retention of items after \u201cKim\u2019s game\u201d. The following figures show the number of people who remembered more than 50% of items for each type of mnemonic: Pictorial mnemonic Word mnemonic No mnemonic Musical mnemonic 39 42 26 3 The null hypothesis predicts that the expected frequencies should be even, such as: Pictorial mnemonic Word mnemonic No mnemonic Musical mnemonic Observed 39 42 26 3 Expected 27.5 27.5 27.5 27.5 First calculate Chi Squared with the following formula: \\[\\chi^2_{obs} = \\sum \\frac{(O - E)^2}{E}\\] with: \\(O\\) are the observed values, \\(E\\) are the expected values (\\(H_0\\)). Therefore: \\[\\chi^2_{obs} = \\bigg( \\frac{(39 - 27.5)^2}{27.5} + \\frac{(42 - 27.5)^2}{27.5} + \\frac{(26 - 27.5)^2}{27.5} + \\frac{(3 - 27.5)^2}{27.5} \\bigg) = 34.37\\] Finally, calculate the degrees of freedom as \\(df = C - 1 = 4 - 1 = 3\\), where \\(C\\) is the number of different conditions (events, cells). In the table D , search for the \\(df = 3\\) with a significance level of \\(\\alpha = 0.05\\), that value is \\(\\chi^2_{crit} = 7.82\\). Because \\(\\chi^2_{obs} \\chi^2_{crit}\\), we can reject the null hypothesis (the type of mnemonic has an impact on the numbers of items remembered). Two-Way Chi Square Two IVs, Two+ Levels (symmetrical) It is observed whether male and female drivers differ in their likelihood to stop at an amber light: Female Male TOTAL Stopped 90 88 178 Didn't stop 56 89 145 TOTAL 146 177 323 First start to calculate the expected frequencies for each cell with the following formula: \\[E = \\frac{totalRow \\cdot totalColumn}{grandTotal}\\] Therefore: Female Male TOTAL Stopped \\(O = 90, E = \\frac{146 \\cdot 178}{323}\\) \\(O = 88, E = \\frac{177 \\cdot 178}{323}\\) 178 Didn't stop \\(O = 56, E = \\frac{146 \\cdot 145}{323}\\) \\(O = 89, E = \\frac{177 \\cdot 145}{323}\\) 145 TOTAL 146 177 323 With the final values: Female Male TOTAL Stopped \\(O = 90, E = 80.46\\) \\(O = 88, E = 97.54\\) 178 Didn't stop \\(O = 56, E = 65.54\\) \\(O = 89, E = 79.46\\) 145 TOTAL 146 177 323 Next calculate Chi Squared with the following formula: \\[\\chi^2_{obs} = \\sum \\frac{(O - E)^2}{E}\\] with: \\(O\\) are the observed values, \\(E\\) are the expected values (\\(H_0\\)). Therefore: \\[\\chi^2_{obs} = \\bigg( \\frac{(90 - 80.46)^2}{80.46} + \\frac{(88 - 97.54)^2}{97.54} + \\frac{(56 - 65.54)^2}{65.54} + \\frac{(89 - 79.46)^2}{79.46} \\bigg) = 4.6\\] Finally, calculate the degrees of freedom as \\(df = (r - 1) \\cdot (c - 1) = (2 - 1) \\cdot (2 - 1) = 1\\), where \\(c\\) is the number of columns and \\(r\\) the number of rows. In the table D , search for the \\(df = 1\\) with a significance level of \\(\\alpha = 0.05\\), that value is \\(\\chi^2_{crit} = 3.84\\). Because \\(\\chi^2_{obs} \\chi^2_{crit}\\), we can reject the null hypothesis (there is a difference between male and female when it comes to stopping at an amber light). Spearman This test is used to know if there is a correlation between two variables. If the null hypothesis is rejected, than a correlation is found (the strength of that correlation is determined by the coefficient \\(r\\)). A survey is conducted to identify whether there is any relationship between job satisfaction and days absent. For \\(N = 12\\) participants, job satisfaction is measured using an 11 point ordinal scale, where a high rating indicates a high level of satisfaction. Number of days absent is monitored for 1 year. Subject Days absent Job satisfaction S1 3 10 S2 6 8 S3 0 11 S4 2 9 S5 1 10 S6 8 5 S7 15 3 S8 33 2 S9 6 7 S10 1 10 S11 2 10 S12 0 10 First rank the scores by order of magnitude (take mean for similar differences) in \\([1, N]\\) as follows: Days absent Order 1 2 3 4 5 6 7 8 9 10 11 12 Score 0 0 1 1 2 2 3 6 6 8 15 33 Rank 1.5 3.5 5.5 7 8.5 10 11 12 Job satisfaction Order 1 2 3 4 5 6 7 8 9 10 11 12 Score 2 3 5 7 8 9 10 10 10 10 10 11 Rank 1 2 3 4 5 6 9 12 Fill these ranks in the previous table: Subject Days absent (\\(X\\)) Job satisfaction (\\(Y\\)) Rank \\(X\\) Rank \\(Y\\) S1 3 10 7 9 S2 6 8 8.5 5 S3 0 11 1.5 12 S4 2 9 5.5 6 S5 1 10 3.5 9 S6 8 5 10 3 S7 15 3 11 2 S8 33 2 12 1 S9 6 7 8.5 4 S10 1 10 3.5 9 S11 2 10 5.5 9 S12 0 10 1.5 9 Next, calculate the differences of the rank for each subject, as well as the square of that difference and the totals: Subject Days absent (\\(X\\)) Job satisfaction (\\(Y\\)) Rank \\(X\\) Rank \\(Y\\) \\(d\\) \\(d^2\\) S1 3 10 7 9 -2 4 S2 6 8 8.5 5 3.5 12.25 S3 0 11 1.5 12 -10.5 110.25 S4 2 9 5.5 6 -0.5 0.25 S5 1 10 3.5 9 -5.5 30.25 S6 8 5 10 3 7 49 S7 15 3 11 2 9 81 S8 33 2 12 1 11 121 S9 6 7 8.5 4 4.5 20.25 S10 1 10 3.5 9 -5.5 30.25 S11 2 10 5.5 9 -3.5 12.25 S12 0 10 1.5 9 -7.5 56.25 TOTAL 527 Next calculate the coefficient \\(r_{obs}\\) with the following formula: \\[r_{obs} = 1 - \\frac{6 \\cdot \\sum d^2}{N \\cdot (N^2 - 1)}\\] Therefore: \\[r_{obs} = 1 - \\frac{6 \\cdot 527}{12 \\cdot (12^2 - 1)} = -0.843\\] \\(r_{obs}\\) first shows a linear relationship close to -1, which means a strong negative correlation. Let's however find \\(r_{crit}\\) with a significance level of \\(\\alpha = 0.05\\). In the table J , search for \\(N = 12\\) with a significance level of \\(\\alpha = 0.05\\), that value is \\(r_{crit} = 0.591\\) Because \\(abs(r_{obs}) r_{crit}\\), we can reject the null hypothesis (there is a negative correlation between the number of days absent and job satisfaction). Negative correlation indicates that the less absent employees are, the more satisfied they are. That doesn't mean however causality, we don't know if being absent causes employees to be more satisfied, just that they are correlated. A likely guess is that employees that love their jobs are less likely to want to miss it (maybe a reverse causation?). Further investigation is required. Parametric tests T-Test (Between) One IV, Two Levels, Between The efficiency of two different methods of assembling an electronic component was compared. Two groups of subjects are tested, one using Method A and one using Method B. The time taken for the assembly task to be completed is measured in seconds. The following results were obtained: Method A Method B Subject Score Subject Score S1 72 S11 65 S2 65 S12 56 S3 89 S13 67 S4 87 S14 45 S5 65 S15 47 S6 67 S16 68 S7 54 S17 72 S8 65 S18 58 S9 79 S19 81 S10 80 S20 62 Start by calculating the squared scores for each score as follows: Method A Method B Subject Score Score\u00b2 Subject Score Score\u00b2 S1 72 5184 S11 65 4225 S2 65 4225 S12 56 3136 S3 89 7921 S13 67 4489 S4 87 7569 S14 45 2025 S5 65 4225 S15 47 2209 S6 67 4489 S16 68 4624 S7 54 2916 S17 72 5184 S8 65 4225 S18 58 3364 S9 79 6241 S19 81 6561 S10 80 6400 S20 62 3844 Then, calculate the total for the scores and the squared scores, as well the scores' means. Assign variables names to each condition, such as \\(X_1\\) and \\(X_2\\): Method A Method B Subject Score (\\(X_1\\)) Score\u00b2 (\\(X_1^2\\)) Subject Score (\\(X_2\\)) Score\u00b2 (\\(X_2^2\\)) S1 72 5184 S11 65 4225 S2 65 4225 S12 56 3136 S3 89 7921 S13 67 4489 S4 87 7569 S14 45 2025 S5 65 4225 S15 47 2209 S6 67 4489 S16 68 4624 S7 54 2916 S17 72 5184 S8 65 4225 S18 58 3364 S9 79 6241 S19 81 6561 S10 80 6400 S20 62 3844 TOTAL 723 53395 621 39661 MEAN 72.3 62.1 The next step is to calculate the value \\(t_{obs}\\) with the following formula: \\[t_{obs} = \\frac{\\overline{X_1} - \\overline{X_2}}{\\sqrt{\\frac{\\bigg[ \\sum X_1^2 - \\frac{(\\sum X_1)^2}{n_1} \\bigg] + \\bigg[\\sum X_2^2 - \\frac{(\\sum X_2)^2}{n_2} \\bigg] }{(n_1 - 1) + (n_2 -1)} \\cdot (\\frac{1}{n_1} + \\frac{1}{n_2})}}\\] where: \\(n_1\\) and \\(n_2\\) are the number of subjects in each group (here \\(n_1 = n_2 = 10\\)), \\(\\overline{X_1}\\) and \\(\\overline{X_2}\\) are the mean for each group (here \\(\\overline{X_1} = 72.3, \\overline{X_2} = 62.1\\)), \\(\\sum X_1^2\\) is the sum of squared scores for the group 1 (here \\(\\sum X_1^2 = 53395\\)), \\(\\sum X_2^2\\) is the sum of squared scores for the group 2 (here \\(\\sum X_2^2 = 39661\\)), \\((\\sum X_1)^2\\) is the squared total of scores for the group 1 (here \\((\\sum X_1^2) = 723^2 = 522729\\)), \\((\\sum X_2)^2\\) is the squared total of scores for the group 2 (here \\((\\sum X_2^2) = 621^2 = 385641\\)). Therefore: \\[t_{obs} = \\frac{72.3 - 62.1}{\\sqrt{\\frac{\\bigg[ 53395 - \\frac{522729}{10} \\bigg] + \\bigg[39661 - \\frac{385641}{10} \\bigg] }{(10 - 1) + (10 -1)} \\cdot (\\frac{1}{10} + \\frac{1}{10})}} = 2.056\\] Finally, compute the degrees of freedom \\(df = (n_1 - 1) + (n_2 - 1) = 18\\). In the table H , search for \\(df = 18\\) with a level of significance of \\(\\alpha = 0.05\\), that value is \\(t_{crit} = 2.101\\). Because \\(t_{obs} t_{crit}\\), we fail to reject the null hypothesis (it cannot be said that the method has an impact on performance). T-Test (Within) One IV, Two Levels, Within The appropriate height for a workstation is assessed by an experiment where the time taken to complete the task at two different workstation heights is measured (in seconds) and compared. The order of presentation of the two conditions is balanced to compensate for any practice effect that might be witnessed, and eight subjects are asked to complete the task at both of the workstation heights. The following results are obtained: Subject High workstation Low workstation S1 45 32 S2 34 37 S3 46 43 S4 48 48 S5 42 38 S6 38 31 S7 37 33 S8 46 43 First, calculate the difference between each subject's scores as the variable \\(d\\): Subject High workstation Low workstation \\(d\\) S1 45 32 13 S2 34 37 -3 S3 46 43 3 S4 48 48 0 S5 42 38 4 S6 38 31 7 S7 37 33 4 S8 46 43 3 Square these differences: Subject High workstation Low workstation \\(d\\) \\(d^2\\) S1 45 32 13 169 S2 34 37 -3 9 S3 46 43 3 9 S4 48 48 0 0 S5 42 38 4 16 S6 38 31 7 49 S7 37 33 4 16 S8 46 43 3 9 Total the differences \\(d\\) and total the squared differences \\(d^2\\): Subject High workstation Low workstation \\(d\\) \\(d^2\\) S1 45 32 13 169 S2 34 37 -3 9 S3 46 43 3 9 S4 48 48 0 0 S5 42 38 4 16 S6 38 31 7 49 S7 37 33 4 16 S8 46 43 3 9 TOTAL 31 277 The next step is to calculate the value \\(t_{obs}\\) with the following formula: \\[t_{obs} = \\frac{\\sum d}{\\sqrt{\\frac{N \\cdot \\sum d^2 - (\\sum d)^2}{N - 1}}}\\] where: \\(N\\) is the number of subjects (here \\(N = 8\\)), \\(\\sum d\\) is the total of differences (here \\(\\sum d = 31\\)), \\(\\sum d^2\\) is the total of squared differences (here \\(\\sum d^2 = 277\\)), \\((\\sum d)^2\\) is the squared total of differences (here \\((\\sum d)^2 = 31^2 = 961\\)). Therefore: \\[t_{obs} = \\frac{31}{\\sqrt{\\frac{8 \\cdot 277 - 961}{8 - 1}}} = 2.315\\] Finally, compute the degrees of freedom \\(df = N - 1 = 7\\). In the table H , search for \\(df = 7\\) with a level of significance of \\(\\alpha = 0.05\\), that value is \\(t_{crit} = 2.365\\). Because \\(t_{obs} t_{crit}\\), we fail to reject the null hypothesis (it cannot be said that the workstation height has an impact on performance). One-Way ANOVA (Between) One IV, Three+ Levels, Between An experiment is performed to see if there is a difference between the ability to perform tasks depending on the amount of grip provided by the glove worn. The following results are obtained: High grip Medium grip Low grip Subject Score Subject Score Subject Score S1 16 S6 2 S11 4 S2 18 S7 10 S12 6 S3 10 S8 9 S13 8 S4 12 S9 13 S14 10 S5 19 S10 11 S15 2 First, square each score and calculate the totals: High grip Medium grip Low grip Subject Score \\(X_1\\) \\(X_1^2\\) Subject Score \\(X_2\\) \\(X_2^2\\) Subject Score \\(X_3\\) \\(X_3^2\\) S1 16 256 S6 2 4 S11 4 16 S2 18 324 S7 10 100 S12 6 36 S3 10 100 S8 9 81 S13 8 64 S4 12 144 S9 13 169 S14 10 100 S5 19 361 S10 11 121 S15 2 4 TOTAL 75 1185 45 475 30 220 We note the following values: \\(\\sum T_c^2\\) is the sum of squared totals for each condition, here: \\[\\sum T_c^2 = (\\sum X_1)^2 + (\\sum X_2)^2 + (\\sum X_3)^2 = 75^2 + 45^2 + 30^2 = 8550\\] \\(\\sum X^2\\) is the sum of all squared scores, here: \\[\\sum X^2 = \\sum X_1^2 + \\sum X_2^2 + \\sum X_3^2 = 1185 + 475 + 220 = 1880\\] \\((\\sum X)^2\\) is the grand total squared, here: \\[(\\sum X)^2 = (\\sum X_1 + \\sum X_2 + \\sum X_3)^2 = (75 + 45 + 30)^2 = 22500\\] \\(N\\) is the total number of scores (subjects), here \\(N = 15\\), \\(n\\) is the number of subjects in each condition, here \\(n = 5\\), \\(C\\) is the number of conditions (levels), here \\(C = 3\\). From this table, the ANOVA Table must be computed, with the following values: \\(SS_{bet} = \\frac{\\sum T_c^2}{n} - \\frac{(\\sum X)^2}{N}\\) \\(SS_{tot} = \\sum X^2 - \\frac{(\\sum X)^2}{N}\\) \\(SS_{error} = SS_{tot} - SS_{bet}\\) \\(df_{bet} = C - 1\\) \\(df_{tot} = N - 1\\) \\(df_{error} = df_{tot} - df_{bet}\\) Replacing with the values gives: \\[SS_{bet} = \\frac{8550}{5} - \\frac{22500}{15} = 210\\] \\[SS_{tot} = 1880 - \\frac{22500}{15} = 380\\] \\[SS_{error} = 380 - 210 = 170\\] \\[df_{bet} = 3 - 1 = 2\\] \\[df_{tot} = 15 - 1 = 14\\] \\[df_{error} = 14 - 2 = 12\\] Next, replace the values in the following table: Source of Variance \\(SS\\) \\(df\\) \\(MS\\) F ratio Variable A \\(SS_{bet}\\) \\(df_{bet}\\) \\(SS_{bet} / df_{bet}\\) \\(MS_{bet} / MS_{error}\\) Error \\(SS_{error}\\) \\(df_{error}\\) \\(SS_{error} / df_{error}\\) -- TOTAL \\(SS_{tot}\\) \\(df_{tot}\\) -- -- Which gives us: Source of Variance \\(SS\\) \\(df\\) \\(MS\\) F ratio Variable A 210 2 105 7.41 Error 170 12 14.17 -- TOTAL 380 14 -- -- We note the value \\(F_{obs} = 7.41\\). In the table(s) I , search for \\(v_1 = df_{bet} = 2\\) and \\(v_2 = df_{error} = 12\\), with a significance level of \\(\\alpha = 0.05\\), that value is \\(F_{crit} = 3.89\\). Because \\(F_{obs} F_{crit}\\), we can reject the null hypothesis (the amount of grip impacts the performance). One-Way ANOVA (Within) One IV, Three+ Levels, Within An experiment is performed to see if there is a difference between the number of correct responses (deciding whether each stimulus is a word or a non-word) for different sizes of text. The following results are obtained: Subject Small Medium Large S1 6 8 9 S2 7 12 13 S3 9 11 12 S4 6 15 16 First square every score and compute the totals: Small Medium Large \\(\\sum T_s\\) Subject Score \\(X_1\\) \\(X_1^2\\) Score \\(X_2\\) \\(X_2^2\\) Score \\(X_1\\) \\(X_3^2\\) S1 6 36 8 64 9 81 23 S2 7 49 12 144 13 169 32 S3 9 81 11 121 12 144 32 S4 6 36 15 225 16 256 37 TOTAL 28 202 46 554 50 650 124 We note the following values: \\(\\sum T_c^2\\) is the sum of squared totals for each condition, here: \\[\\sum T_c^2\\ = (\\sum X_1)^2 + (\\sum X_2)^2 + (\\sum X_3)^2 = 28^2 + 46^2 + 50^2 = 5400\\] \\(\\sum T_s^2\\) is the sum of squared totals for each subject, here: \\[\\sum T_s^2\\ = (\\sum T_{s1})^2 + (\\sum T_{s2})^2 + (\\sum T_{s3})^2 + (\\sum T_{s4})^2 = 23^2 + 32^2 + 32^2 + 37^2 = 3946\\] \\(\\sum X^2\\) is the sum of all squared scores, here: \\[\\sum X^2 = \\sum X_1^2 + \\sum X_2^2 + \\sum X_3^2 = 202 + 554 + 650 = 1406\\] \\((\\sum X)^2\\) is the grand total squared, here: \\[(\\sum X)^2 = (\\sum X_1 + \\sum X_2 + \\sum X_3)^2 = (28 + 46 + 50)^2 = 15376\\] \\(N\\) is the total number of scores (subjects), here \\(N = 12\\), \\(n\\) is the number of subjects in each condition, here \\(n = 4\\), \\(C\\) is the number of conditions (levels), here \\(C = 3\\). From this table, the ANOVA Table must be computed, with the following values: \\(SS_{bet} = \\frac{\\sum T_c^2}{n} - \\frac{(\\sum X)^2}{N}\\) \\(SS_{subj} = \\frac{\\sum T_s^2}{c} - \\frac{(\\sum X)^2}{N}\\) \\(SS_{tot} = \\sum X^2 - \\frac{(\\sum X)^2}{N}\\) \\(SS_{error} = SS_{tot} - SS_{bet} - SS_{subj}\\) \\(df_{bet} = C - 1\\) \\(df_{subj} = n - 1\\) \\(df_{tot} = N - 1\\) \\(df_{error} = df_{tot} - df_{bet} - df_{subj}\\) Replacing with the values gives: \\[SS_{bet} = \\frac{5400}{4} - \\frac{15376}{12} = 68.67\\] \\[SS_{subj} = \\frac{3946}{3} - \\frac{15376}{12} = 34\\] \\[SS_{tot} = 1406 - \\frac{15376}{12} = 124.67\\] \\[SS_{error} = 124.67 - 68.67 - 34 = 22\\] \\[df_{bet} = 3 - 1 = 2\\] \\[df_{subj} = 4 - 1 = 3\\] \\[df_{tot} = 12 - 1 = 11\\] \\[df_{error} = 11 - 3 - 2 = 6\\] Source of Variance \\(SS\\) \\(df\\) \\(MS\\) F ratio Variable A \\(SS_{bet}\\) \\(df_{bet}\\) \\(SS_{bet} / df_{bet}\\) \\(MS_{bet} / MS_{error}\\) Subjects \\(SS_{subj}\\) \\(df_{subj}\\) \\(SS_{subj} / df_{subj}\\) \\(MS_{subj} / MS_{error}\\) Error \\(SS_{error}\\) \\(df_{error}\\) \\(SS_{error} / df_{error}\\) -- TOTAL \\(SS_{tot}\\) \\(df_{tot}\\) -- -- Which gives us: Source of Variance \\(SS\\) \\(df\\) \\(MS\\) F ratio Variable A 68.7 2 34.34 9.38 Subjects 34 3 11.33 3.10 Error 22 6 3.66 -- TOTAL 124.67 11 -- -- We note the value \\(F_{obs} = 9.38\\). In the table(s) I , search for \\(v_1 = df_{bet} = 2\\) and \\(v_2 = df_{error} = 6\\), with a significance level of \\(\\alpha = 0.05\\), that value is \\(F_{crit} = 5.14\\). Because \\(F_{obs} F_{crit}\\), we can reject the null hypothesis (the text size impacts the performance). Pearson This test is used to know if there is a correlation between two variables. If the null hypothesis is rejected, than a correlation is found (the strength of that correlation is determined by the coefficient \\(r\\)). An ergonomist wishes to discover whether there is any association between the time twelve typists have spent working at a computer without a break and the number of typing errors made by them in a ten minute interval. The following results are obtained: Subject Time (\\(X\\)) Errors (\\(Y\\)) S1 45 10 S2 61 14 S3 52 13 S4 73 16 S5 46 9 S6 32 6 S7 21 4 S8 19 3 S9 70 18 S10 86 21 S11 53 15 S12 18 5 First start by computing \\(X^2\\), \\(Y^2\\) and \\(X \\cdot Y\\) for each subject, as well as the totals: Subject Time (\\(X\\)) Errors (\\(Y\\)) \\(X^2\\) \\(Y^2\\) \\(X \\cdot Y\\) S1 45 10 2025 100 450 S2 61 14 3721 196 854 S3 52 13 2704 169 676 S4 73 16 5329 256 1168 S5 46 9 2116 81 414 S6 32 6 1024 36 192 S7 21 4 441 16 84 S8 19 3 361 9 57 S9 70 18 4900 324 1260 S10 86 21 7396 441 1806 S11 53 15 2809 225 795 S12 18 5 324 25 90 TOTAL 576 134 33150 1880 7846 Then calculate the coefficient \\(r_{obs}\\) with the following formula: \\[r_{obs} = \\frac{N \\cdot \\sum X \\cdot Y - (\\sum X)\\cdot (\\sum Y)}{\\sqrt{(N \\cdot \\sum X^2 - (\\sum X)^2) \\cdot (N \\cdot \\sum Y^2 - (\\sum Y)^2)}}\\] with \\(N\\) the number of subjects, here \\(N = 12\\). Therefore: \\[r_{obs} = \\frac{12 \\cdot 7846 - 576 \\cdot 134}{\\sqrt{(12 \\cdot 33150 - 576^2) \\cdot (12 \\cdot 1880 - 134^2)}} = 0.973\\] \\(r_{obs}\\) first shows a linear relationship close to 1, which in general means a strong correlation. Let's however find \\(r_{crit}\\) with a significance level of \\(\\alpha = 0.05\\). Calculate the degrees of freedom as \\(df = N - 2 = 12 - 2 = 10\\). In the tale K , search for \\(df = 10\\) with a significance level of \\(\\alpha = 0.05\\), that value is \\(r_{crit} = 0.5760\\). Because \\(r_{obs} r_{crit}\\) we can reject the null hypothesis (the time spent without break and the number of errors are correlated). In addition, \\(r_{obs}^2 = 0.95\\) which means that 95% of the variance in the nuber of errors can be accounted for by the amount of time spent without a break.","title":"SHP - Statistics"},{"location":"SHP-stats/#studying-human-performance-statistics","text":"COURSE","title":"Studying Human Performance - Statistics"},{"location":"SHP-stats/#basics","text":"","title":"Basics"},{"location":"SHP-stats/#data-types","text":"There are three types of data to consider when doing statistics: Categorical : data classed into categories, Ordinal : data that can only be classed by order, Ratio/Interval : data that can be put on a scale where the relationship between each point is known.","title":"Data types"},{"location":"SHP-stats/#measures-of-central-tendency","text":"There are three measures of central tendency to take into account: mean , median and mode . The mean is the average of all numbers and is sometimes called the arithmetic mean , and is generally best for interval data. In a sample with \\(n\\) elements such as \\(x_1, x_2,..., x_n\\), the mean noted \\(\\bar{x}\\) is the sum of all the values divided by the numbers of items: \\[\\bar{x} = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} x_i\\] The median is the value separating the higher half from the lower half of a data sample, commonly thought of as the \" middle \" value. Finding the median in a sample only requires the sample to be ranked (sorted, ordered). The median is best to use with ordinal data . The basic advantage of the median compared to the mean is that it is not skewed so much by extremely large or small values, and so it may give a better idea of a \" typical \" value. The mode of a set of values is the value that appears most often, the most common entry, best measure for categorical data .","title":"Measures of central tendency"},{"location":"SHP-stats/#normal-distribution","text":"","title":"Normal distribution"},{"location":"SHP-stats/#definition","text":"The normal distribution (also called Gaussian or the bell curve ), is a very common continuous probability distribution. Normal distributions are very important in statistics and are often used to represent real-valued random variables whose distributions are not known. The normal distribution is useful because of the central limit theorem stating that averages of samples converge in distribution to the normal, that is, they become normally distributed when the number of observations is sufficiently large. In simpler words, when observing data from large samples, in many cases, the data tends to be around a central value with no bias left or right. This distribution is found in many naturally occurring phenomena such as heights of people , size of objects produced by machines , errors in measurements , blood pressure , marks on a test and many more. The properties of a normal distribution allow to easiliy manipulate variables and compute significant results. A normally distributed sample is a sample with equal measures of central tendecy, that is when \\(mean = median = mode\\). When this is not the case, the data sample is said to be skewed . A sample can be negatively skewed (\\(mean median mode\\)) or positively skewed (\\(mean median mode\\)). It is not appropriate to perform various statistical tests on a skewed sample. When the sample is normally distributed with a perfect symetry, exactly 50% of the values are lower than the mean, and exactly 50% are higher. A normal distribution is noted \\(\\mathcal{N}(\\mu, \\sigma)\\) with \\(\\mu\\) the mean, \\(\\sigma\\) the standard deviation and \\(\\sigma^2\\) the variance.","title":"Definition"},{"location":"SHP-stats/#standard-deviation","text":"The standard deviation, note \\(\\sigma\\), is a measure defining how spread out the numbers of a sample are. A low standard deviation indicates that the data points tend to be close to the mean , while a high standard deviation indicates that the data points are spread out over wider range of values . This measure is the square root of the variance . But what is the variance? The variance is defined as the squared differences from the mean . Informally, the variance measures how far a set of numbers are spread out from their average value. For a sample with \\(n\\) values, it is calculated with the following formula: \\[\\sigma^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\mu)^2}{n-1}\\] The standard deviation is therefore calculated with: \\[\\sigma = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\mu)^2}{n-1}}\\] Once the mean and the standard deviation are known, a perfect description of the distribution is given. Generally we find that:","title":"Standard deviation"},{"location":"SHP-stats/#standard-scores","text":"The standard deviation is a useful tool to generally examine a sample and contextualize values in 1 to 3 standard deviation units from the mean. However, how to know how many standard deviations a specific value is from the mean? To convert a value to a standard deviation value (also called a standard score or z-score ): Substract the mean to the value ( how far from the mean? ), Divided by the standard deviation ( how many standard deviations? ). Hence for a value \\(x\\), the z-score \\(z\\) is: \\[z = \\frac{x - \\mu}{\\sigma}\\] Doing that operation is called standardizing , or converting a normal distribution into a standard normal distribution . Usually, the entire sample is standardized at once to simplify the calculations by simply looking up standardized pre-computed tables. The table D. Normal distribution table allows to convert from standard scores to percents, and vice versa.","title":"Standard scores"},{"location":"SHP-stats/#statistical-tests","text":"When doing an experiment, choosing the correct statistical test is very important to obtain significant results. The first step in doing so is to understand every factor affecting the choice, and how to implement study design .","title":"Statistical tests"},{"location":"SHP-stats/#experiment-outline","text":"An experiment, (or study or research), can be outlined by the following steps: Research question : objective, aim, question asked, Study design : describing the experiment through hypothesis, variables, data types and participants, Conducting the experiment : doing the described experiment and collecting data, Test selection : choosing the correct test(s) based on the study design, Doing the test : using the data to obtain results through the selected test(s), Interpretation and conclusion : making sense of the result and concluding on the research question.","title":"Experiment outline"},{"location":"SHP-stats/#study-design","text":"","title":"Study design"},{"location":"SHP-stats/#defining-the-problem-aim-and-hypotheses","text":"The first step in designing a study is to clearly define the problem and issues that require investigation, and what will be achived by doing it. This is usually represented by a system of hypotheses , or statements of the predicted outcome . As studies manipulate variables and data, the aim of these hypotheses is to predict, understand and validate the relationships between said variables. Conventionally, there are two types of hypothesis: Experimental , or H1 , that predicts a dependent relationship between variables, Null , or H0 , that states that variables are not dependent (one does not impact the other).","title":"Defining the problem, aim and hypotheses"},{"location":"SHP-stats/#identify-the-research-variables","text":"The said variables are split in three categories: Independant variables ( IV ): variables manipulated by the experimented, related to the individual, task, system or environment, Dependant variables ( DV ): variables being measured in the experiment, not under the control of the experimenter, possibly affected by the IVs, Controlled variables : variables that need to be kept constant during the experiment.","title":"Identify the research variables"},{"location":"SHP-stats/#allocate-participants-to-the-experimental-conditions","text":"There are two ways to allocate and distribute the participants across the IVs: Within : every participant completes every experimental condition, Between : different participants are allocated to each experimental condition.","title":"Allocate participants to the experimental conditions"},{"location":"SHP-stats/#assumptions-and-planned-comparissons","text":"The experimental hypothesis predicts a dependant relationship between variables, or in other words, independant variables impacting dependant variables. When a significant interaction is found, it is called a main effect . A study design can however further predictions by using assumptions, planned comparisons and post hocs . A one-tailed hypothesis predicts how and in which direction an independant variable impacts a dependant variable. For example, if an experimental hypothesis predicts that music (IV) impacts performance (DV), a one-tailed extension would add that music improves performance , or that music impairs performance . One-tailed tests must be justified: it is for example justified to predict that constant loud noise impairs performance. In addition, planned comparisons and post hoc are other comparisons that can be integrated to a study design. Unlike one-tailed assumptions that predict in which direction an independant variable affects a dependant variable, these computations compare the levels of an independant variable. For example, if an experimental hypothesis predicts that music (IV) impacts performance (DV), and that this independant variable has three levels ( classical , rock , rap ), contrasts could be that classical music affects performance differently than rap music . These potential interactions are called simple effects . These comparisons can be done in two ways: Contrasts : they are planned comparisons, with hypotheses in mind, a guess in which direction the comparisons will result. They are specifically planned before the tests , Post hoc : if no specific prediction is made, these comparisons are made after the test, comparing everything to possibly detect significant differences.","title":"Assumptions and planned comparissons"},{"location":"SHP-stats/#data-types-and-analysis","text":"Based on the data types and the variables, a test can either be parametric or non-parametric . A parametric test is a test that is carried out with the assumption that the data collected follows a well-known distribution (usually the normal distribution), which can be boiled down to the knowledge of just a couple of parameters . A parametric test provides generalisations for making statements about the mean of the parent population. Because parametric test are based on a distribution, the measurement of variables on interval or ratio level, with the mean used as the measure of central tendency. On the other hand, a non-parametric test has no known information about the population and is not based on any known distribution. Because non-parametric tests don't require assumptions about the nature of their distributions, they are also called distribution-free . These tests hence assume that the variables are measured on a nominal (categorical) or ordinal level, with the median as the measure of central tendency.","title":"Data types and analysis"},{"location":"SHP-stats/#significance","text":"Statistical tests try to answer a question, based on the available data. When rejecting a null hypothesis (observing that there is indeed a relation between variables), there is a probability that this rejection is due to chance , that the random variables happen to misrepresent the reality . This probability, called p-value , represents the chance to reject a null hypothesis when it is actually true . Subsequently, the lower the p-value, the more meaningful the result because it is less likely to be caused by noise . A statistically significant result is one in which we believe the result was as a result of the experimental manipulation rather than chance alone. The action to reject the null hypothesis when it is actually true is called an incorrect conclusion and more precisely a Type I error . It is an experimental error that analysts wants to avoid in priority, to not make any false claim. To that end, before conducting the test, a significance threshold called \\(\\alpha\\) is set, that if exceeded, indicates a non-significant result. That threshold is usually \\(\\alpha = 0.05\\), but particular contexts and fields of study often require a more strict value, such as \\(\\alpha = 0.001\\). Of course, the opposite action of failing to reject a null hypothesis when it is actually false is another experimental error called Type II error , but is often seen as less serious than the Type I error. The chance to make a Type II error is represented by the probability called \\(\\beta\\). The statistical power is the probability of correctly rejecting the null hypothesis, when it is indeed false .","title":"Significance"},{"location":"SHP-stats/#statistical-power","text":"Statistical power is the probability of correctly rejecting a false null hypothesis. Statistical power is inversely related to \\(\\beta\\) or the probability of making a Type II error . In short, \\(power = 1 - \u03b2\\). In simpler words, statistical power is the likelihood that a study will detect an effect when there is an effect there to be detected. If statistical power is high, the probability of making a Type II error, or concluding there is no effect when, in fact, there is one, goes down. As a rule of thumb, ideal statistical power is assumed when \\(\\geq 0.8\\). Statistical power power nearly always depends on the following three factors: the statistical significance criterion used in the test: usually named \\(\\alpha\\) and set at 0.05, this criterion represent the likelihood to reject a null hypothesis when it is actually true, the magnitude of the effect: the effect size, is a quantitative measure of the magnitude of a phenomenon. Examples of effect sizes are the correlation between two variables, the regression coefficient in a regression, the mean difference, etc. In simple words, while the significance provides the knowledge of the presence of a phenomenon, the effect size describes how much is that phenomenon occuring, the sample size : it determines the amount of sampling error inherent in a test result. The statistical power is therefore often used to determine the ideal sample size (larger effect and sample sizes improve power). In addition, the type of test can play a part in the power, as parametric tests are generally more powerful than non-parametrics ones.","title":"Statistical power"},{"location":"SHP-stats/#correlation-regression","text":"Most tests aim to know if an independant variable has an impact on a dependant variable, but doesn't describe how . The study of correlation aims to measure the association or the absence of relationship between two variables, without taking into account if they are independant or dependant. Regression analysis on the other hand predicts the value of a dependant variable based on the known value of an independant variable. Correlation is represented by a coefficient \\(r\\). This coefficient can vary from \\(-1\\) to \\(+1\\). A \\(-1\\) indicates a perfect negative correlation , while a \\(+1\\) indicates a perfect positive correlation . A correlation of \\(0\\) means there is no relationship between the two variables. When there is a negative correlation between two variables, as the value of one variable increases, the value of the other variable decreases, and vise versa. In other words, for a negative correlation, the variables work opposite each other . When there is a positive correlation between two variables, as the value of one variable increases, the value of the other variable also increases, the variables move together . A rule of thumb to interpret \\(r\\) is: \\(+-1\\): perfect relationship, \\(+-0.7\\): strong relationship, \\(+-0.5\\): moderate relationship, \\(+-0.3\\): weak relation ship, \\(0\\): no relationship. While \\(r\\) represents the strength of the linear relationship between two variables, \\(r^2\\) is generally better to report correlation in ergonomics. Known as the coefficient of determination , \\(r^2\\) represents the proportion of variance of one variable that is predicted from another variable (e.g. 80% of variation in Y can be explained by its relationship with X, while 20% remain unexplained). The coefficient \\(r\\) can be calculated with: Spearman test (non-parametric), Pearson test (parametric). In simple linear regression , we predict scores of one variable from the scores of a second variable. The variable we are predicting is called the criterion variable and is referred to as \\(Y\\). The variable we are basing our predictions on is called the predictor variable and is referred to as \\(X\\). When there is only one predictor variable, the prediction method is called simple regression . In simple linear regression, the predictions of Y when plotted as a function of X form a straight line. Linear regression consists of finding the best-fitting straight line through the points, called a regression line.","title":"Correlation, regression"},{"location":"SHP-stats/#causality","text":"It is very important to understand that correlation does not imply causation . When two variables are found to be correlated, it is tempting to assume that this shows that one variable causes the other. It is however not the case, and this assumption is considered a questionable cause logical fallacy . For any two correlated variables \\(A\\) and \\(B\\), the different possible relationships include: \\(A\\) causes \\(B\\) (direct causation), \\(B\\) causes \\(A\\) (reverse causation), \\(A\\) and \\(B\\) are consequences of a common cause, but do not cause each other, \\(A\\) and \\(B\\) both cause \\(C\\), which is explicitely of implicitely conditioned on, \\(A\\) causes \\(B\\) and \\(B\\) causes \\(A\\) (bidirectional or cyclic causation), \\(A\\) causes \\(C\\) which causes \\(B\\) (indirect causation), There is no connection between \\(A\\) and \\(B\\), the correlation is a coincidence. Thus there can be no conclusion made regarding the existence of a cause-and-effect relationship only from the fact that A and B are correlated. Determining whether there is an actual cause-and-effect relationship requires further investigation , even when the relationship between A and B is statistically significant, a large effect size is observed, or a large part of the variance is explained.","title":"Causality"},{"location":"SHP-stats/#non-parametric-tests","text":"","title":"Non-parametric tests"},{"location":"SHP-stats/#wilcoxon","text":"One IV, Two Levels, Within A set of \\(N = 12\\) subjects are asked to rate two designs of screwdriver handle for ease of use on a seven point ordinal scale (7 = easiest, 1 = most difficult) and the data obtained are shown in the table below: Subject Handle A (\\(X_1\\)) Handle B (\\(X_2\\)) S1 6 3 S2 5 6 S3 7 4 S4 4 4 S5 6 5 S6 4 1 S7 3 5 S8 5 2 S9 7 2 S10 5 2 S11 6 2 S12 6 4 First start by calculating the differences between each pair of scores : Subject Handle A (\\(X_1\\)) Handle B (\\(X_2\\)) \\(X_1\\) - \\(X_2\\) S1 6 3 3 S2 5 6 -1 S3 7 4 3 S4 4 4 0 S5 6 5 1 S6 4 1 3 S7 3 5 -2 S8 5 2 3 S9 7 2 5 S10 5 2 3 S11 6 2 4 S12 6 4 2 The next step is to ignore null differences (such as for S4), and hence decrease to \\(N = 11\\) subjects. The differences must then be ranked in order of magnitude (take mean for similar differences) in \\([1, N]\\), as follows: Order 1 2 3 4 5 6 7 8 9 10 11 Difference 1 1 2 2 3 3 3 3 3 4 5 Rank 1.5 3.5 7 10 11 Then, assign in the previous table the rank to each difference, conserving their sign : Subject Handle A (\\(X_1\\)) Handle B (\\(X_2\\)) \\(X_1\\) - \\(X_2\\) Rank S1 6 3 3 7 (+) S2 5 6 -1 1.5 (-) S3 7 4 3 7 (+) S4 4 4 0 S5 6 5 1 1.5 (+) S6 4 1 3 7 (+) S7 3 5 -2 3.5 (-) S8 5 2 3 7 (+) S9 7 2 5 11 (+) S10 5 2 3 7(+) S11 6 2 4 10 (+) S12 6 4 2 3.5 (+) Calculate the total of negative ranks \\(T_n\\) and the total of positive ranks \\(T_p\\): \\(T_n = 5\\) \\(T_p = 61\\) Finally, select from these two totals the lowest one , that is the value \\(W_{obs}\\). In this example \\(min(T_n, T_p) = 5\\), therefore \\(W_{obs} = 5\\). In the table A , search for \\(N = 11\\), with a level of significance for a two-tailed test of \\(\\alpha = 0.05\\), that value is \\(W_{crit} = 11\\). Because \\(W_{obs} W_{crit}\\), the null hypothesis is rejected (there is a difference of ease of use between the two designs of handles).","title":"Wilcoxon"},{"location":"SHP-stats/#mann-whitney","text":"One IV, Two Levels, Between Two groups of 7 subjects (total of \\(N = 14\\)) are asked to complete a maintenance task using either a paper based manual for instructions or a head mounted display on a \u201cprivate eye\u201d. After the task they rate how easy they found several aspects of the task, giving an overall usability rating of between 4 and 20 (20 = easiest to use, 4 = most difficult) . Subject Paper based Subect Private eye S1 8 S8 11 S2 10 S9 15 S3 6 S10 18 S4 15 S11 14 S5 14 S12 16 S6 9 S13 12 S7 10 S14 9 First rank the scores by order of magnitude (take mean for similar differences) in \\([1, N]\\) as follows: Order 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Score 6 8 9 9 10 10 11 12 14 14 15 15 16 18 Rank 1 2 3.5 5.5 7 8 9.5 11.5 13 14 Assign the rank to each score in the previous table and compute the totals: Subject Paper based Rank Subect Private eye Rank S1 8 2 S8 11 7 S2 10 5.5 S9 15 11.5 S3 6 1 S10 18 14 S4 15 11.5 S11 14 9.5 S5 14 9.5 S12 16 13 S6 9 3.5 S13 12 8 S7 10 5.5 S14 9 3.5 TOTAL 38.5 66.5 The total rank for paper based is \\(T_{pb} = 38.5\\), and the total rank for private eye is \\(T_{pe} = 66.5\\). Select from the two rank totals the highest one and assign to the variable \\(T_x\\). \\[T_x = max(T_{pb}, T_{pe}) = 66.5\\] Using the following formulate, calculate the value of \\(U_{obs}\\): \\[U_{obs} = n_1 \\cdot n_2 + \\frac{n_x \\cdot (n_x + 1)}{2} - T_x\\] where: \\(n_1\\) is the number of subjects in the group 1 (here \\(n_1 = 7\\)), \\(n_2\\) is the number of subjects in the group 2 (here \\(n_2 = 7\\)), \\(n_x\\) is the number of subjects in the group with the the highest rank total (here \\(n_x = 7\\)). Therefore: \\[U_{obs} = 7 \\cdot 7 + \\frac{7 \\cdot (7 + 1)}{2} - 66.5 = 10.5\\] In the table B , search for \\(n_1 = n_2 = 7\\), with a level of significance for a two-tailed test of \\(\\alpha = 0.05\\), that value is \\(U_{crit} = 8\\). Because \\(U_{obs} U_{crit}\\), we fail to reject the null hypothesis (it can't be said that there is a difference between the two conditions).","title":"Mann-Whitney"},{"location":"SHP-stats/#friedman","text":"One IV, Three+ levels, Within A group of \\(n = 6\\) students are asked to use three different types of computer interface one which is solely command line based, one which uses a combination of command line and pull down menus, and one which only uses pull down menus . They are then asked to rate the usability of these interfaces on a five point scale (5 = most usable) . The following scores are obtained: Subject Cmd line Combination Menus S1 2 4 2 S2 1 5 3 S3 3 5 2 S4 2 3 3 S5 2 4 3 S6 1 3 4 For each separate subect, rank the three scores by order of magnitude as follows for Subject 1 (S1) : Order 1 2 3 Score 2 2 4 Rank 1.5 3 Assign the ranks to every score in the previous table and compute the rank totals: Cmd line Combination Menus Subject Score Rank Score Rank Score Rank S1 2 1.5 4 3 2 1.5 S2 1 1 5 3 3 2 S3 3 2 5 3 2 1 S4 2 1 3 2.5 3 2.5 S5 2 1 4 3 3 2 S6 1 1 3 2 4 3 Total 7.5 16.5 12 We note the rank totals for each condition \\(T_{c1} = 7.5\\), \\(T_{c2} = 16.5\\), \\(T_{c3} = 12\\). The next step is to calculate the value \\(Xr^2_{obs}\\) with the following formula: \\[Xr^2_{obs} = \\bigg[ \\frac{12}{N \\cdot C \\cdot (C + 1)} \\cdot \\sum T_c^2 \\bigg] - 3 \\cdot N \\cdot (C + 1)\\] where: \\(N\\) is the number of subjects (here \\(N = 6\\)), \\(C\\) is the number of levels (here \\(C = 3\\)), \\(\\sum T_c^2\\) is the sum of the square rank totals for each condition. Therefore: \\[\\sum T_c^2 = T_{c1}^2 + T_{c2}^2 + T_{c3}^2 = 7.5^2 + 16.5^2 + 12^2 = 472.5\\] \\[Xr^2_{obs} = \\bigg[ \\frac{12}{6 \\cdot 3 \\cdot (3 + 1)} \\cdot 472.5 \\bigg] - 3 \\cdot 6 \\cdot (3 + 1) = 6.75\\] In the table C , search for \\(N = 6\\) and \\(C = 3\\), that value is \\(Xr^2_{crit} = 6.33\\). Because \\(Xr^2_{obs} Xr^2_{crit}\\), we can reject the null hypothesis (there is a difference between the three conditions).","title":"Friedman"},{"location":"SHP-stats/#kruskal-wallis","text":"One IV, Three+ levels, Between Three groups of seven subjects (total of \\(N = 21\\)) were asked to rate any discomfort they experienced after typing on a keyboard for 20 minutes. The keyboards were set up so that they required three different levels of force to be exerted onto the keys. Therefore the three conditions were low force , medium force and high force . The subjects rated their discomfort on a 9 point scale, where a high score indicated a greater degree of discomfort . Subject Low force Subect Medium force Subject High force S1 4 S8 5 S15 6 S2 3 S9 4 S16 8 S3 1 S10 2 S17 9 S4 2 S11 5 S18 7 S5 2 S12 4 S19 9 S6 3 S13 3 S20 9 S7 1 S14 6 S21 8 First rank the scores by order of magnitude (take mean for similar differences) in \\([1, N]\\) as follows: Order 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Score 1 1 2 2 2 3 3 3 4 4 4 5 5 6 6 7 8 8 9 9 9 Rank 1.5 4 7 10 12.5 14.5 16 17.5 20 Assign the rank to each score in the previous table and compute the totals: Low force Medium force High force Subject Score Rank Subject Score Rank Subject Score Rank S1 4 10 S8 5 12.5 S15 6 14.5 S2 3 7 S9 4 10 S16 8 17.5 S3 1 1.5 S10 2 4 S17 9 20 S4 2 4 S11 5 12.5 S18 7 16 S5 2 4 S12 4 10 S19 9 20 S6 3 7 S13 3 7 S20 9 20 S7 1 1.5 S14 6 14.5 S21 8 17.5 TOTAL 35 70.5 125.5 We note the rank totals for each condition \\(T_{c1} = 35\\), \\(T_{c2} = 70.5\\), \\(T_{c3} = 125.5\\), and the number of subjects for each condition \\(n_1 = n_2 = n_3 = 7\\). The next step is to calculate the value \\(H_{obs}\\) with the following formula: \\[H_{obs} = \\bigg[ \\frac{12}{N \\cdot (N + 1)} \\cdot \\sum \\frac{T_c^2}{n_c} \\bigg] - 3 \\cdot (N + 1)\\] where: \\(N\\) is the total number of subjects (here \\(N = 21\\)), \\(\\sum \\frac{T_c^2}{n_c}\\) is the sum of squared rank totals for each condition divided by the number of participants in that condition. \\[\\sum \\frac{T_c^2}{n_c} = \\frac{35^2}{7} + \\frac{70.5^2}{7} + \\frac{125.5^2}{7} = 3135.08\\] Therefore: \\[H_{obs} = \\bigg[ \\frac{12}{21 \\cdot (21 + 1)} \\cdot 3135.08 \\bigg] - 3 \\cdot (21 + 1) = 15.51\\] Finally the number of degrees of freedom must be computed as \\(df = C - 1 = 3 - 1 = 2\\), where \\(C\\) is the number of conditions (levels). To find \\(H_{crit}\\) the number of participants in each group determines the table to use: if all groups have more than 5 subjects, the Chi-square table should be used, otherwise the Kruskal-Wallis one. Here \\(n_1 = n_2 = n_3 5\\), so the Chi-square table should be used. In the table D , search for \\(df = 2\\) and a significance level of \\(\\alpha = 0.05\\), that value is \\(H_{crit} = 5.99\\). Because \\(H_{obs} H_{crit}\\), we can reject the null hypothesis (keyboard force does affect comfort).","title":"Kruskal-Wallis"},{"location":"SHP-stats/#one-way-chi-square","text":"One IV, Two+ Levels The Chi Square is the only test for categorical data . Usually measuring frequencies of events (how many subjects made an error, how many passed a test etc) in different conditions, this test aims to uncover if the condition had an impact on the frequencies, or if the null hypothesis is true, that is that each condition should be almost equally distributed. A test was performed to compare the efficiency of different methods of learning for retention of items after \u201cKim\u2019s game\u201d. The following figures show the number of people who remembered more than 50% of items for each type of mnemonic: Pictorial mnemonic Word mnemonic No mnemonic Musical mnemonic 39 42 26 3 The null hypothesis predicts that the expected frequencies should be even, such as: Pictorial mnemonic Word mnemonic No mnemonic Musical mnemonic Observed 39 42 26 3 Expected 27.5 27.5 27.5 27.5 First calculate Chi Squared with the following formula: \\[\\chi^2_{obs} = \\sum \\frac{(O - E)^2}{E}\\] with: \\(O\\) are the observed values, \\(E\\) are the expected values (\\(H_0\\)). Therefore: \\[\\chi^2_{obs} = \\bigg( \\frac{(39 - 27.5)^2}{27.5} + \\frac{(42 - 27.5)^2}{27.5} + \\frac{(26 - 27.5)^2}{27.5} + \\frac{(3 - 27.5)^2}{27.5} \\bigg) = 34.37\\] Finally, calculate the degrees of freedom as \\(df = C - 1 = 4 - 1 = 3\\), where \\(C\\) is the number of different conditions (events, cells). In the table D , search for the \\(df = 3\\) with a significance level of \\(\\alpha = 0.05\\), that value is \\(\\chi^2_{crit} = 7.82\\). Because \\(\\chi^2_{obs} \\chi^2_{crit}\\), we can reject the null hypothesis (the type of mnemonic has an impact on the numbers of items remembered).","title":"One-Way Chi Square"},{"location":"SHP-stats/#two-way-chi-square","text":"Two IVs, Two+ Levels (symmetrical) It is observed whether male and female drivers differ in their likelihood to stop at an amber light: Female Male TOTAL Stopped 90 88 178 Didn't stop 56 89 145 TOTAL 146 177 323 First start to calculate the expected frequencies for each cell with the following formula: \\[E = \\frac{totalRow \\cdot totalColumn}{grandTotal}\\] Therefore: Female Male TOTAL Stopped \\(O = 90, E = \\frac{146 \\cdot 178}{323}\\) \\(O = 88, E = \\frac{177 \\cdot 178}{323}\\) 178 Didn't stop \\(O = 56, E = \\frac{146 \\cdot 145}{323}\\) \\(O = 89, E = \\frac{177 \\cdot 145}{323}\\) 145 TOTAL 146 177 323 With the final values: Female Male TOTAL Stopped \\(O = 90, E = 80.46\\) \\(O = 88, E = 97.54\\) 178 Didn't stop \\(O = 56, E = 65.54\\) \\(O = 89, E = 79.46\\) 145 TOTAL 146 177 323 Next calculate Chi Squared with the following formula: \\[\\chi^2_{obs} = \\sum \\frac{(O - E)^2}{E}\\] with: \\(O\\) are the observed values, \\(E\\) are the expected values (\\(H_0\\)). Therefore: \\[\\chi^2_{obs} = \\bigg( \\frac{(90 - 80.46)^2}{80.46} + \\frac{(88 - 97.54)^2}{97.54} + \\frac{(56 - 65.54)^2}{65.54} + \\frac{(89 - 79.46)^2}{79.46} \\bigg) = 4.6\\] Finally, calculate the degrees of freedom as \\(df = (r - 1) \\cdot (c - 1) = (2 - 1) \\cdot (2 - 1) = 1\\), where \\(c\\) is the number of columns and \\(r\\) the number of rows. In the table D , search for the \\(df = 1\\) with a significance level of \\(\\alpha = 0.05\\), that value is \\(\\chi^2_{crit} = 3.84\\). Because \\(\\chi^2_{obs} \\chi^2_{crit}\\), we can reject the null hypothesis (there is a difference between male and female when it comes to stopping at an amber light).","title":"Two-Way Chi Square"},{"location":"SHP-stats/#spearman","text":"This test is used to know if there is a correlation between two variables. If the null hypothesis is rejected, than a correlation is found (the strength of that correlation is determined by the coefficient \\(r\\)). A survey is conducted to identify whether there is any relationship between job satisfaction and days absent. For \\(N = 12\\) participants, job satisfaction is measured using an 11 point ordinal scale, where a high rating indicates a high level of satisfaction. Number of days absent is monitored for 1 year. Subject Days absent Job satisfaction S1 3 10 S2 6 8 S3 0 11 S4 2 9 S5 1 10 S6 8 5 S7 15 3 S8 33 2 S9 6 7 S10 1 10 S11 2 10 S12 0 10 First rank the scores by order of magnitude (take mean for similar differences) in \\([1, N]\\) as follows: Days absent Order 1 2 3 4 5 6 7 8 9 10 11 12 Score 0 0 1 1 2 2 3 6 6 8 15 33 Rank 1.5 3.5 5.5 7 8.5 10 11 12 Job satisfaction Order 1 2 3 4 5 6 7 8 9 10 11 12 Score 2 3 5 7 8 9 10 10 10 10 10 11 Rank 1 2 3 4 5 6 9 12 Fill these ranks in the previous table: Subject Days absent (\\(X\\)) Job satisfaction (\\(Y\\)) Rank \\(X\\) Rank \\(Y\\) S1 3 10 7 9 S2 6 8 8.5 5 S3 0 11 1.5 12 S4 2 9 5.5 6 S5 1 10 3.5 9 S6 8 5 10 3 S7 15 3 11 2 S8 33 2 12 1 S9 6 7 8.5 4 S10 1 10 3.5 9 S11 2 10 5.5 9 S12 0 10 1.5 9 Next, calculate the differences of the rank for each subject, as well as the square of that difference and the totals: Subject Days absent (\\(X\\)) Job satisfaction (\\(Y\\)) Rank \\(X\\) Rank \\(Y\\) \\(d\\) \\(d^2\\) S1 3 10 7 9 -2 4 S2 6 8 8.5 5 3.5 12.25 S3 0 11 1.5 12 -10.5 110.25 S4 2 9 5.5 6 -0.5 0.25 S5 1 10 3.5 9 -5.5 30.25 S6 8 5 10 3 7 49 S7 15 3 11 2 9 81 S8 33 2 12 1 11 121 S9 6 7 8.5 4 4.5 20.25 S10 1 10 3.5 9 -5.5 30.25 S11 2 10 5.5 9 -3.5 12.25 S12 0 10 1.5 9 -7.5 56.25 TOTAL 527 Next calculate the coefficient \\(r_{obs}\\) with the following formula: \\[r_{obs} = 1 - \\frac{6 \\cdot \\sum d^2}{N \\cdot (N^2 - 1)}\\] Therefore: \\[r_{obs} = 1 - \\frac{6 \\cdot 527}{12 \\cdot (12^2 - 1)} = -0.843\\] \\(r_{obs}\\) first shows a linear relationship close to -1, which means a strong negative correlation. Let's however find \\(r_{crit}\\) with a significance level of \\(\\alpha = 0.05\\). In the table J , search for \\(N = 12\\) with a significance level of \\(\\alpha = 0.05\\), that value is \\(r_{crit} = 0.591\\) Because \\(abs(r_{obs}) r_{crit}\\), we can reject the null hypothesis (there is a negative correlation between the number of days absent and job satisfaction). Negative correlation indicates that the less absent employees are, the more satisfied they are. That doesn't mean however causality, we don't know if being absent causes employees to be more satisfied, just that they are correlated. A likely guess is that employees that love their jobs are less likely to want to miss it (maybe a reverse causation?). Further investigation is required.","title":"Spearman"},{"location":"SHP-stats/#parametric-tests","text":"","title":"Parametric tests"},{"location":"SHP-stats/#t-test-between","text":"One IV, Two Levels, Between The efficiency of two different methods of assembling an electronic component was compared. Two groups of subjects are tested, one using Method A and one using Method B. The time taken for the assembly task to be completed is measured in seconds. The following results were obtained: Method A Method B Subject Score Subject Score S1 72 S11 65 S2 65 S12 56 S3 89 S13 67 S4 87 S14 45 S5 65 S15 47 S6 67 S16 68 S7 54 S17 72 S8 65 S18 58 S9 79 S19 81 S10 80 S20 62 Start by calculating the squared scores for each score as follows: Method A Method B Subject Score Score\u00b2 Subject Score Score\u00b2 S1 72 5184 S11 65 4225 S2 65 4225 S12 56 3136 S3 89 7921 S13 67 4489 S4 87 7569 S14 45 2025 S5 65 4225 S15 47 2209 S6 67 4489 S16 68 4624 S7 54 2916 S17 72 5184 S8 65 4225 S18 58 3364 S9 79 6241 S19 81 6561 S10 80 6400 S20 62 3844 Then, calculate the total for the scores and the squared scores, as well the scores' means. Assign variables names to each condition, such as \\(X_1\\) and \\(X_2\\): Method A Method B Subject Score (\\(X_1\\)) Score\u00b2 (\\(X_1^2\\)) Subject Score (\\(X_2\\)) Score\u00b2 (\\(X_2^2\\)) S1 72 5184 S11 65 4225 S2 65 4225 S12 56 3136 S3 89 7921 S13 67 4489 S4 87 7569 S14 45 2025 S5 65 4225 S15 47 2209 S6 67 4489 S16 68 4624 S7 54 2916 S17 72 5184 S8 65 4225 S18 58 3364 S9 79 6241 S19 81 6561 S10 80 6400 S20 62 3844 TOTAL 723 53395 621 39661 MEAN 72.3 62.1 The next step is to calculate the value \\(t_{obs}\\) with the following formula: \\[t_{obs} = \\frac{\\overline{X_1} - \\overline{X_2}}{\\sqrt{\\frac{\\bigg[ \\sum X_1^2 - \\frac{(\\sum X_1)^2}{n_1} \\bigg] + \\bigg[\\sum X_2^2 - \\frac{(\\sum X_2)^2}{n_2} \\bigg] }{(n_1 - 1) + (n_2 -1)} \\cdot (\\frac{1}{n_1} + \\frac{1}{n_2})}}\\] where: \\(n_1\\) and \\(n_2\\) are the number of subjects in each group (here \\(n_1 = n_2 = 10\\)), \\(\\overline{X_1}\\) and \\(\\overline{X_2}\\) are the mean for each group (here \\(\\overline{X_1} = 72.3, \\overline{X_2} = 62.1\\)), \\(\\sum X_1^2\\) is the sum of squared scores for the group 1 (here \\(\\sum X_1^2 = 53395\\)), \\(\\sum X_2^2\\) is the sum of squared scores for the group 2 (here \\(\\sum X_2^2 = 39661\\)), \\((\\sum X_1)^2\\) is the squared total of scores for the group 1 (here \\((\\sum X_1^2) = 723^2 = 522729\\)), \\((\\sum X_2)^2\\) is the squared total of scores for the group 2 (here \\((\\sum X_2^2) = 621^2 = 385641\\)). Therefore: \\[t_{obs} = \\frac{72.3 - 62.1}{\\sqrt{\\frac{\\bigg[ 53395 - \\frac{522729}{10} \\bigg] + \\bigg[39661 - \\frac{385641}{10} \\bigg] }{(10 - 1) + (10 -1)} \\cdot (\\frac{1}{10} + \\frac{1}{10})}} = 2.056\\] Finally, compute the degrees of freedom \\(df = (n_1 - 1) + (n_2 - 1) = 18\\). In the table H , search for \\(df = 18\\) with a level of significance of \\(\\alpha = 0.05\\), that value is \\(t_{crit} = 2.101\\). Because \\(t_{obs} t_{crit}\\), we fail to reject the null hypothesis (it cannot be said that the method has an impact on performance).","title":"T-Test (Between)"},{"location":"SHP-stats/#t-test-within","text":"One IV, Two Levels, Within The appropriate height for a workstation is assessed by an experiment where the time taken to complete the task at two different workstation heights is measured (in seconds) and compared. The order of presentation of the two conditions is balanced to compensate for any practice effect that might be witnessed, and eight subjects are asked to complete the task at both of the workstation heights. The following results are obtained: Subject High workstation Low workstation S1 45 32 S2 34 37 S3 46 43 S4 48 48 S5 42 38 S6 38 31 S7 37 33 S8 46 43 First, calculate the difference between each subject's scores as the variable \\(d\\): Subject High workstation Low workstation \\(d\\) S1 45 32 13 S2 34 37 -3 S3 46 43 3 S4 48 48 0 S5 42 38 4 S6 38 31 7 S7 37 33 4 S8 46 43 3 Square these differences: Subject High workstation Low workstation \\(d\\) \\(d^2\\) S1 45 32 13 169 S2 34 37 -3 9 S3 46 43 3 9 S4 48 48 0 0 S5 42 38 4 16 S6 38 31 7 49 S7 37 33 4 16 S8 46 43 3 9 Total the differences \\(d\\) and total the squared differences \\(d^2\\): Subject High workstation Low workstation \\(d\\) \\(d^2\\) S1 45 32 13 169 S2 34 37 -3 9 S3 46 43 3 9 S4 48 48 0 0 S5 42 38 4 16 S6 38 31 7 49 S7 37 33 4 16 S8 46 43 3 9 TOTAL 31 277 The next step is to calculate the value \\(t_{obs}\\) with the following formula: \\[t_{obs} = \\frac{\\sum d}{\\sqrt{\\frac{N \\cdot \\sum d^2 - (\\sum d)^2}{N - 1}}}\\] where: \\(N\\) is the number of subjects (here \\(N = 8\\)), \\(\\sum d\\) is the total of differences (here \\(\\sum d = 31\\)), \\(\\sum d^2\\) is the total of squared differences (here \\(\\sum d^2 = 277\\)), \\((\\sum d)^2\\) is the squared total of differences (here \\((\\sum d)^2 = 31^2 = 961\\)). Therefore: \\[t_{obs} = \\frac{31}{\\sqrt{\\frac{8 \\cdot 277 - 961}{8 - 1}}} = 2.315\\] Finally, compute the degrees of freedom \\(df = N - 1 = 7\\). In the table H , search for \\(df = 7\\) with a level of significance of \\(\\alpha = 0.05\\), that value is \\(t_{crit} = 2.365\\). Because \\(t_{obs} t_{crit}\\), we fail to reject the null hypothesis (it cannot be said that the workstation height has an impact on performance).","title":"T-Test (Within)"},{"location":"SHP-stats/#one-way-anova-between","text":"One IV, Three+ Levels, Between An experiment is performed to see if there is a difference between the ability to perform tasks depending on the amount of grip provided by the glove worn. The following results are obtained: High grip Medium grip Low grip Subject Score Subject Score Subject Score S1 16 S6 2 S11 4 S2 18 S7 10 S12 6 S3 10 S8 9 S13 8 S4 12 S9 13 S14 10 S5 19 S10 11 S15 2 First, square each score and calculate the totals: High grip Medium grip Low grip Subject Score \\(X_1\\) \\(X_1^2\\) Subject Score \\(X_2\\) \\(X_2^2\\) Subject Score \\(X_3\\) \\(X_3^2\\) S1 16 256 S6 2 4 S11 4 16 S2 18 324 S7 10 100 S12 6 36 S3 10 100 S8 9 81 S13 8 64 S4 12 144 S9 13 169 S14 10 100 S5 19 361 S10 11 121 S15 2 4 TOTAL 75 1185 45 475 30 220 We note the following values: \\(\\sum T_c^2\\) is the sum of squared totals for each condition, here: \\[\\sum T_c^2 = (\\sum X_1)^2 + (\\sum X_2)^2 + (\\sum X_3)^2 = 75^2 + 45^2 + 30^2 = 8550\\] \\(\\sum X^2\\) is the sum of all squared scores, here: \\[\\sum X^2 = \\sum X_1^2 + \\sum X_2^2 + \\sum X_3^2 = 1185 + 475 + 220 = 1880\\] \\((\\sum X)^2\\) is the grand total squared, here: \\[(\\sum X)^2 = (\\sum X_1 + \\sum X_2 + \\sum X_3)^2 = (75 + 45 + 30)^2 = 22500\\] \\(N\\) is the total number of scores (subjects), here \\(N = 15\\), \\(n\\) is the number of subjects in each condition, here \\(n = 5\\), \\(C\\) is the number of conditions (levels), here \\(C = 3\\). From this table, the ANOVA Table must be computed, with the following values: \\(SS_{bet} = \\frac{\\sum T_c^2}{n} - \\frac{(\\sum X)^2}{N}\\) \\(SS_{tot} = \\sum X^2 - \\frac{(\\sum X)^2}{N}\\) \\(SS_{error} = SS_{tot} - SS_{bet}\\) \\(df_{bet} = C - 1\\) \\(df_{tot} = N - 1\\) \\(df_{error} = df_{tot} - df_{bet}\\) Replacing with the values gives: \\[SS_{bet} = \\frac{8550}{5} - \\frac{22500}{15} = 210\\] \\[SS_{tot} = 1880 - \\frac{22500}{15} = 380\\] \\[SS_{error} = 380 - 210 = 170\\] \\[df_{bet} = 3 - 1 = 2\\] \\[df_{tot} = 15 - 1 = 14\\] \\[df_{error} = 14 - 2 = 12\\] Next, replace the values in the following table: Source of Variance \\(SS\\) \\(df\\) \\(MS\\) F ratio Variable A \\(SS_{bet}\\) \\(df_{bet}\\) \\(SS_{bet} / df_{bet}\\) \\(MS_{bet} / MS_{error}\\) Error \\(SS_{error}\\) \\(df_{error}\\) \\(SS_{error} / df_{error}\\) -- TOTAL \\(SS_{tot}\\) \\(df_{tot}\\) -- -- Which gives us: Source of Variance \\(SS\\) \\(df\\) \\(MS\\) F ratio Variable A 210 2 105 7.41 Error 170 12 14.17 -- TOTAL 380 14 -- -- We note the value \\(F_{obs} = 7.41\\). In the table(s) I , search for \\(v_1 = df_{bet} = 2\\) and \\(v_2 = df_{error} = 12\\), with a significance level of \\(\\alpha = 0.05\\), that value is \\(F_{crit} = 3.89\\). Because \\(F_{obs} F_{crit}\\), we can reject the null hypothesis (the amount of grip impacts the performance).","title":"One-Way ANOVA (Between)"},{"location":"SHP-stats/#one-way-anova-within","text":"One IV, Three+ Levels, Within An experiment is performed to see if there is a difference between the number of correct responses (deciding whether each stimulus is a word or a non-word) for different sizes of text. The following results are obtained: Subject Small Medium Large S1 6 8 9 S2 7 12 13 S3 9 11 12 S4 6 15 16 First square every score and compute the totals: Small Medium Large \\(\\sum T_s\\) Subject Score \\(X_1\\) \\(X_1^2\\) Score \\(X_2\\) \\(X_2^2\\) Score \\(X_1\\) \\(X_3^2\\) S1 6 36 8 64 9 81 23 S2 7 49 12 144 13 169 32 S3 9 81 11 121 12 144 32 S4 6 36 15 225 16 256 37 TOTAL 28 202 46 554 50 650 124 We note the following values: \\(\\sum T_c^2\\) is the sum of squared totals for each condition, here: \\[\\sum T_c^2\\ = (\\sum X_1)^2 + (\\sum X_2)^2 + (\\sum X_3)^2 = 28^2 + 46^2 + 50^2 = 5400\\] \\(\\sum T_s^2\\) is the sum of squared totals for each subject, here: \\[\\sum T_s^2\\ = (\\sum T_{s1})^2 + (\\sum T_{s2})^2 + (\\sum T_{s3})^2 + (\\sum T_{s4})^2 = 23^2 + 32^2 + 32^2 + 37^2 = 3946\\] \\(\\sum X^2\\) is the sum of all squared scores, here: \\[\\sum X^2 = \\sum X_1^2 + \\sum X_2^2 + \\sum X_3^2 = 202 + 554 + 650 = 1406\\] \\((\\sum X)^2\\) is the grand total squared, here: \\[(\\sum X)^2 = (\\sum X_1 + \\sum X_2 + \\sum X_3)^2 = (28 + 46 + 50)^2 = 15376\\] \\(N\\) is the total number of scores (subjects), here \\(N = 12\\), \\(n\\) is the number of subjects in each condition, here \\(n = 4\\), \\(C\\) is the number of conditions (levels), here \\(C = 3\\). From this table, the ANOVA Table must be computed, with the following values: \\(SS_{bet} = \\frac{\\sum T_c^2}{n} - \\frac{(\\sum X)^2}{N}\\) \\(SS_{subj} = \\frac{\\sum T_s^2}{c} - \\frac{(\\sum X)^2}{N}\\) \\(SS_{tot} = \\sum X^2 - \\frac{(\\sum X)^2}{N}\\) \\(SS_{error} = SS_{tot} - SS_{bet} - SS_{subj}\\) \\(df_{bet} = C - 1\\) \\(df_{subj} = n - 1\\) \\(df_{tot} = N - 1\\) \\(df_{error} = df_{tot} - df_{bet} - df_{subj}\\) Replacing with the values gives: \\[SS_{bet} = \\frac{5400}{4} - \\frac{15376}{12} = 68.67\\] \\[SS_{subj} = \\frac{3946}{3} - \\frac{15376}{12} = 34\\] \\[SS_{tot} = 1406 - \\frac{15376}{12} = 124.67\\] \\[SS_{error} = 124.67 - 68.67 - 34 = 22\\] \\[df_{bet} = 3 - 1 = 2\\] \\[df_{subj} = 4 - 1 = 3\\] \\[df_{tot} = 12 - 1 = 11\\] \\[df_{error} = 11 - 3 - 2 = 6\\] Source of Variance \\(SS\\) \\(df\\) \\(MS\\) F ratio Variable A \\(SS_{bet}\\) \\(df_{bet}\\) \\(SS_{bet} / df_{bet}\\) \\(MS_{bet} / MS_{error}\\) Subjects \\(SS_{subj}\\) \\(df_{subj}\\) \\(SS_{subj} / df_{subj}\\) \\(MS_{subj} / MS_{error}\\) Error \\(SS_{error}\\) \\(df_{error}\\) \\(SS_{error} / df_{error}\\) -- TOTAL \\(SS_{tot}\\) \\(df_{tot}\\) -- -- Which gives us: Source of Variance \\(SS\\) \\(df\\) \\(MS\\) F ratio Variable A 68.7 2 34.34 9.38 Subjects 34 3 11.33 3.10 Error 22 6 3.66 -- TOTAL 124.67 11 -- -- We note the value \\(F_{obs} = 9.38\\). In the table(s) I , search for \\(v_1 = df_{bet} = 2\\) and \\(v_2 = df_{error} = 6\\), with a significance level of \\(\\alpha = 0.05\\), that value is \\(F_{crit} = 5.14\\). Because \\(F_{obs} F_{crit}\\), we can reject the null hypothesis (the text size impacts the performance).","title":"One-Way ANOVA (Within)"},{"location":"SHP-stats/#pearson","text":"This test is used to know if there is a correlation between two variables. If the null hypothesis is rejected, than a correlation is found (the strength of that correlation is determined by the coefficient \\(r\\)). An ergonomist wishes to discover whether there is any association between the time twelve typists have spent working at a computer without a break and the number of typing errors made by them in a ten minute interval. The following results are obtained: Subject Time (\\(X\\)) Errors (\\(Y\\)) S1 45 10 S2 61 14 S3 52 13 S4 73 16 S5 46 9 S6 32 6 S7 21 4 S8 19 3 S9 70 18 S10 86 21 S11 53 15 S12 18 5 First start by computing \\(X^2\\), \\(Y^2\\) and \\(X \\cdot Y\\) for each subject, as well as the totals: Subject Time (\\(X\\)) Errors (\\(Y\\)) \\(X^2\\) \\(Y^2\\) \\(X \\cdot Y\\) S1 45 10 2025 100 450 S2 61 14 3721 196 854 S3 52 13 2704 169 676 S4 73 16 5329 256 1168 S5 46 9 2116 81 414 S6 32 6 1024 36 192 S7 21 4 441 16 84 S8 19 3 361 9 57 S9 70 18 4900 324 1260 S10 86 21 7396 441 1806 S11 53 15 2809 225 795 S12 18 5 324 25 90 TOTAL 576 134 33150 1880 7846 Then calculate the coefficient \\(r_{obs}\\) with the following formula: \\[r_{obs} = \\frac{N \\cdot \\sum X \\cdot Y - (\\sum X)\\cdot (\\sum Y)}{\\sqrt{(N \\cdot \\sum X^2 - (\\sum X)^2) \\cdot (N \\cdot \\sum Y^2 - (\\sum Y)^2)}}\\] with \\(N\\) the number of subjects, here \\(N = 12\\). Therefore: \\[r_{obs} = \\frac{12 \\cdot 7846 - 576 \\cdot 134}{\\sqrt{(12 \\cdot 33150 - 576^2) \\cdot (12 \\cdot 1880 - 134^2)}} = 0.973\\] \\(r_{obs}\\) first shows a linear relationship close to 1, which in general means a strong correlation. Let's however find \\(r_{crit}\\) with a significance level of \\(\\alpha = 0.05\\). Calculate the degrees of freedom as \\(df = N - 2 = 12 - 2 = 10\\). In the tale K , search for \\(df = 10\\) with a significance level of \\(\\alpha = 0.05\\), that value is \\(r_{crit} = 0.5760\\). Because \\(r_{obs} r_{crit}\\) we can reject the null hypothesis (the time spent without break and the number of errors are correlated). In addition, \\(r_{obs}^2 = 0.95\\) which means that 95% of the variance in the nuber of errors can be accounted for by the amount of time spent without a break.","title":"Pearson"},{"location":"cognitive-ergonomics-in-design/","text":"Cognitive Ergonomics in Design Introduction and Memory \"Humans are fallible, but humans are brilliant\" Three Mile Island Take a photo of a good and a bad design - put in a slide with explanations why good why bad then submit Bad design: USB stick","title":"Cognitive Ergonomics in Design"},{"location":"cognitive-ergonomics-in-design/#cognitive-ergonomics-in-design","text":"","title":"Cognitive Ergonomics in Design"},{"location":"cognitive-ergonomics-in-design/#introduction-and-memory","text":"\"Humans are fallible, but humans are brilliant\" Three Mile Island Take a photo of a good and a bad design - put in a slide with explanations why good why bad then submit Bad design: USB stick","title":"Introduction and Memory"},{"location":"design-ethnography/","text":"Design Ethnography Introduction lecture Reading material Malinowski, 1922 (https://drive.google.com/file/d/0BzaP9mHcdgckRTltU2JtRlpHcGM/view?usp=sharing) Ethnography TEX youtube Plans and Situated Action: The problem of human machine communication (1987) Coursework Study planning ( 2nd November ), Ethnographic fieldword ( Christmas ) Ethnomethodological analysis ( 18th January ) Topic : Music band interaction ? Tutorial 1 (08/10/2018) Topic ideas: Escape game Band practice Shopping Queuing Studying Meeting/Brainstorm Hair dresser Lecture (11/10/2018) Tutorial 2 (15/10/2018) What confidentiality document can I present to support my request? \"Hello [escape game], My name is Andy Pag\u00e8s and I am a student at the University of Nottingham. I currently study Design Ethnography, which aims to observe and understand basic social interactions. As part of a research project, I am interested in doing an observation session in your establishment. I would like to bring a group of individuals at [escape game] that I will observe while they try to solve a room. To support my research, I will require to record some parts of the session. I understand the confidentiality issues this request may rise, but please be reassured that an official request will be made (if you are interested) backed up with official documents that will ensure that this experiment is conducted in your terms only. In addition, no element regarding the solving of the room will be used in this study, but only the interactions among the participants. Conducting this experiment in your establishment would be a great addition to the ethnographic studies and a significant help for my research. In the hope of a positive response, Best regards, Andy Pag\u00e8s \" Lecture 2 (18/10/2018) Written consent for escape game, very focused environment. To do: Read the slides thoroughly","title":"Design Ethnography"},{"location":"design-ethnography/#design-ethnography","text":"","title":"Design Ethnography"},{"location":"design-ethnography/#introduction-lecture","text":"","title":"Introduction lecture"},{"location":"design-ethnography/#reading-material","text":"Malinowski, 1922 (https://drive.google.com/file/d/0BzaP9mHcdgckRTltU2JtRlpHcGM/view?usp=sharing) Ethnography TEX youtube Plans and Situated Action: The problem of human machine communication (1987)","title":"Reading material"},{"location":"design-ethnography/#coursework","text":"Study planning ( 2nd November ), Ethnographic fieldword ( Christmas ) Ethnomethodological analysis ( 18th January ) Topic : Music band interaction ?","title":"Coursework"},{"location":"design-ethnography/#tutorial-1-08102018","text":"Topic ideas: Escape game Band practice Shopping Queuing Studying Meeting/Brainstorm Hair dresser","title":"Tutorial 1 (08/10/2018)"},{"location":"design-ethnography/#lecture-11102018","text":"","title":"Lecture (11/10/2018)"},{"location":"design-ethnography/#tutorial-2-15102018","text":"What confidentiality document can I present to support my request? \"Hello [escape game], My name is Andy Pag\u00e8s and I am a student at the University of Nottingham. I currently study Design Ethnography, which aims to observe and understand basic social interactions. As part of a research project, I am interested in doing an observation session in your establishment. I would like to bring a group of individuals at [escape game] that I will observe while they try to solve a room. To support my research, I will require to record some parts of the session. I understand the confidentiality issues this request may rise, but please be reassured that an official request will be made (if you are interested) backed up with official documents that will ensure that this experiment is conducted in your terms only. In addition, no element regarding the solving of the room will be used in this study, but only the interactions among the participants. Conducting this experiment in your establishment would be a great addition to the ethnographic studies and a significant help for my research. In the hope of a positive response, Best regards, Andy Pag\u00e8s \"","title":"Tutorial 2 (15/10/2018)"},{"location":"design-ethnography/#lecture-2-18102018","text":"Written consent for escape game, very focused environment. To do: Read the slides thoroughly","title":"Lecture 2 (18/10/2018)"},{"location":"exam2015_methods/","text":"EXAM 2015/2016 Section A 1. (a) The first step to conducting this study is to really understand the objective , and the research goal . In the context of production line in a factory, the goal is performance . By inverstigating this new hydraulic power tool, the factory wants to know if it will be beneficial to replace the old tool with this one. First, the location of the study should be in the factory. While a laboratory is a safer and more controlled environment, this study requires the most realistic conditions to ensure that the results are valid by putting the participants in a real scenario, that would later become real if the study proved the new tool to be beneficial. The sample size, an important factor to consider for optimized results (statisical power), should at least be 10 participants so that significant results are obtained. Of course, every participant should be familiar with the current (old) tool, for proper comparisons. Finally, the study should be conducted in a within-subjects design, as a between-subjects would make less sense: comparing a group that uses the old tool with which they are very familiar, in comparison with a group that uses the new tool and are not yet familiar with it. The study should hence ask \\(N \\geq 10\\) participants to carry out an assembly line once with the old tool, and once with the new one. For each performance, a time should be recorded, with a lower time being better. The independant variable is therefore the hydraulic tool type, and the dependant variable is the performance time. (b) This study would first be valid because being conducted in the factory, just like for any other assembly line task. The study is supposed to measure performance, and in this line of business, time is indicator of performance. Measurement the time took with each tool therefore ensures proper validity regarding the research goal. To be reliable, the study would have to be conducted in very specific conditions: product, line flow, and demands should always be identical so that it could be reproduced. As this study simply measures performance, there are no particular subjective factor that would make it difficult for other researchers to reproduce. Finally, the effects of interest are the differences between the two tools: is one better for performance? if yes, which one? By comparing the exact same assembly line with the two different tools, the data will provide answers, if any significant difference is present. To improve sensitivy and potentially provide more information on performance metrics, other information such as satisfaction rate on each tool could help understanding (correlations?).","title":"EXAM 2015/2016"},{"location":"exam2015_methods/#exam-20152016","text":"Section A","title":"EXAM 2015/2016"},{"location":"exam2015_methods/#1","text":"","title":"1."},{"location":"exam2015_methods/#a","text":"The first step to conducting this study is to really understand the objective , and the research goal . In the context of production line in a factory, the goal is performance . By inverstigating this new hydraulic power tool, the factory wants to know if it will be beneficial to replace the old tool with this one. First, the location of the study should be in the factory. While a laboratory is a safer and more controlled environment, this study requires the most realistic conditions to ensure that the results are valid by putting the participants in a real scenario, that would later become real if the study proved the new tool to be beneficial. The sample size, an important factor to consider for optimized results (statisical power), should at least be 10 participants so that significant results are obtained. Of course, every participant should be familiar with the current (old) tool, for proper comparisons. Finally, the study should be conducted in a within-subjects design, as a between-subjects would make less sense: comparing a group that uses the old tool with which they are very familiar, in comparison with a group that uses the new tool and are not yet familiar with it. The study should hence ask \\(N \\geq 10\\) participants to carry out an assembly line once with the old tool, and once with the new one. For each performance, a time should be recorded, with a lower time being better. The independant variable is therefore the hydraulic tool type, and the dependant variable is the performance time.","title":"(a)"},{"location":"exam2015_methods/#b","text":"This study would first be valid because being conducted in the factory, just like for any other assembly line task. The study is supposed to measure performance, and in this line of business, time is indicator of performance. Measurement the time took with each tool therefore ensures proper validity regarding the research goal. To be reliable, the study would have to be conducted in very specific conditions: product, line flow, and demands should always be identical so that it could be reproduced. As this study simply measures performance, there are no particular subjective factor that would make it difficult for other researchers to reproduce. Finally, the effects of interest are the differences between the two tools: is one better for performance? if yes, which one? By comparing the exact same assembly line with the two different tools, the data will provide answers, if any significant difference is present. To improve sensitivy and potentially provide more information on performance metrics, other information such as satisfaction rate on each tool could help understanding (correlations?).","title":"(b)"},{"location":"simulation-and-digital-human-modeling/","text":"Simulation and Digital Human Modeling Lecture (19/10/2018) Important : Course work 1 is due Friday 9th November 2018 . Read slides from lecture Chosen subjects: Motorcycle manufacturer (simple seat moving right/left + screen to have longer tests ; we don't really want driving experience fidelity, we are interested in a reliable system, providing a decent simulation (just a screen) with minimum sickness effects. We want the testers to be able to stay on the fake bike for a long time to test the comfort. The fake bike would be composed of three key elements: the seat, the handlebar and footplate/gears, all those can be configured to different positions depending on the bike model being simulated). Aviation museum (we want more aspect focused on physical aspects of the historical spitfire plane, a physical fidelity, while giving an entertaining and educational flight experience with minimum SICKNESS (lots of people in museum, sample is large, young, old, fragile..etc) so no motion, only visual). Coursework 1: RESEARCH Motorcycle manufacturer #counter-steering-is-important #highways-are-painful-whatever-the-bike Objective : Improving sports motorbikes physical comfort with the use of a simulator. What defines a comfortable motorbike? : A motorbike that, during and after a prolonged use, does not lead to any form of physical pain or discomfort, and does not affect the general well being of the rider in ways such as chronic pain, back distortion, muscular cramp and many more. What is the population targeted? : Accustomed motorbike riders, familiar with the notion of comfort and discomfort on a bike, capable of judging accordingly to experience in short, medium and long trips with a motorbike as well as general knowledge and interest in the type of motorbike being tested (maximize data). What parts of the bike affect comfort? : The main part of the bike responsible for comfort or discomfort is the seat. The human body is not made for staying still on the same part of the body for prolonged periods of time. Because of decreased supply of blood to the skin, the area that is being compressed for a long time starts to ache (comparison to sub-consciously turn over while sleeping). This principle implies that for a rider to feel comfortable on a motorbike for a long period of time, he needs to be able to move and change positions on the seat. That includes moving forward, backward, widening the legs opening, relieving the weight by pressing the tank..etc. For example, a seat angled forward will push the rider into the lowest spot, always putting pressure on the same parts of his body. Similarly, a seat too soft will seem comfortable at first, but with longer trips the rider might pressure through and hit the plastic base pan of the bike. In addition, the position and size of both the handlebar and the footplate/gears can also affect the rider's comfort. Of course, comfort and discomfort vary a lot from a rider to another one. Weight, size, general morphology, posture and habits will also influence his experience of comfort on a motorbike. Finally, the impact of the vibrations, the road quality, the speed and forces implied in a motorbike experience is significant (suspensions, vibrations suppression). (we consider here that as sports motorcycles, no additional significant weights are added to the rider). What are the relevant aspects for a simulator to achieve comfort evaluation? : If the primary objective is to test the comfort of a bike, the main goal of the simulator is to present the physical configuration of the motorbike being tested (seat, handlebar position, footplate/gears position) with a virtual riding experience entertaining enough to motivate the participant to stay on the bike for a prolonged period of time. That entertaining riding experience does not need to have a high fidelity to the real-world riding, as the goal is only to test the physical comfort of the motorbike (the visual aspect is not indispensable). There are no transfer of training to teach how to ride, or to improve skills, but only a test on the impact of the motorbike on the participant's physical comfort. However, that entertaining-enough riding experience, to last a longer period of time, needs to present minimum sickness effects. In addition, the physical configuration of the motorbike needs to present a very import aspect of riding: the leaning motion. Even with fidelity not being the principal goal, a leaning mechanism is absolutely necessary for any real-world rider to have a minimal replicating experience of a real motorbike. Finally, the leaning motion influences the position of the rider on the motorbike, which influences his comfort. Expecting a rider to test a seat with a motorbike constantly remaining in a straight line would be a complete lack of validity. What resources and materials are necessary? : We assume that manufacturers desire a simulator capable of adapting to any motorcycle model they are designing and/or already selling. A first way to implement such an adaptability is to build a system where an entire motorcycle can be \"locked-in\" and plugged in the rest of the simulator. This implementation raises many issues: compromising an entire motorcycle when most of its parts won't be relevant for the simulation ; every change of model requires to re-do the cabling and sensing systems for the handlebars steering, acceleration grips, and eventually gears if they are integrated to the simulation ; finally it would require that the motorbike is already designed (put together). On the other hand, building a system that only includes the key physical element of a motorbike that actually interact with the rider, will prove more efficient. All cabling/sensing will be definitive, and to change the simulator to a new model would simply require to modify the system's configuration handlebar position, footplate/grip position, gaz tank size/position, and of course a simple replacement of the seat and its position. (additional variables such as weight of the motorcycle, wheel width and more can be manipulated with the motion system and force applied). How can the utility be controlled? : By first targeting experienced riders, the experiment benefit from specific user groups which will maximize and improve the data. Concerning the control over the variables, the key elements of the bike would entirely be configurable to the desired position and setup. As well, software would allow simple manipulation of the force in motion movements (leaning). Of course, the experiment can be repeated as many times as wanted. What is the degree of validity regarding the comfort/discomfort? : Comfort and discomfort vary from the configuration and design of the bike, the characteristics of the rider and the physical impact of driving on the road (vibrations, forces). Regarding the design of the motorbike, the degree of validity is very high as it will present all relevant physical parts of the motorbike. The characteristics of the rider (morphology, physical condition, habits, posture) are entirely identical to the real-world. Finally, the real-world impact of the road can not be represented in any simulator, as it is only a simulator. Only the force of leaning motion and vibrations can be simulated. The overall degree of validity is therefore high, as the design of the motorbike itself is very important in the comfort of the rider. In addition, the important factor of leaning brings more validity as it would allow the rider to be in more diverse and realistic postures. What is the degree of fidelity regarding the general experience of driving? : As the goal of this simulator is not to train, entertain or experiment on driving factors and human behaviors, the fidelity of the general driving experience does not need to be high. To obtain a simulator with such fidelity, cost and technology are aspects to greatly improve: virtual reality, advanced graphics, advanced study of complex driving phenomenons (counter-steering), perfect synchronization to avoid sickness effects, and general software improvements. The fidelity needs to just enough to maintain the participant motivated (entertained, interested and challenged) while avoiding sickness. The fidelity highly relies on how long participants stay on the bike. If they can't stay long enough to test the comfort on extended period of time, fidelity needs to be elevated. What are the costs for such a simulator : Ideally, a first version of the simulator is implemented with only necessary features to offer a basic driving experience with a high validity regarding physical comfort/discomfort. From that version, fidelity has to be improved until the participants are motivated enough to remain longer on the motorbike. Whare are the limitations? : Regarding the comfort/discomfort, this simulator concept lacks the important effect of the road, the suspensions' importance etc. It only takes in account the participant and the motorbike, with basic leaning forces and motion. In addition, the fidelity may be too low on the first versions for prolonged sessions. Aviation museum Objective : Enhance educational impact of museums for the Spitfire plane from WWII flight experience using a simulator. What is the population targeted : No specific population is targeted. This simulator is for the general public: children, adults, elderly, any individual visiting the museum. What is the Spitfire? : Spitfire, also called Supermarine Spitfire, the most widely produced and strategically important British single-seat fighter of World War II. It was designed in response to a 1934 Air Ministry specification calling for a high-performance fighter with an armament of eight wing-mounted 0.303-inch machine guns. It had a stressed-skin aluminum structure and a graceful elliptical wing with a thin airfoil that gave it exceptional performance at high altitudes. It was designed as a short-range, high-performance interceptor aircraft. What elements from the Spitfire are relevant for general public education? : The Spitfire is iconic in the WWII, both for its physical shape that is very famous and the role it played in the war. Educating the general public about how flying with a Spitfire with a simulator implies two aspects: the plane in itself (cockpit, materials, technical elements) and the virtual flying experience (screen, speed, flight simulator, forces). The plane in itself replicated with precision would educated the public on what it felt like to sit in that plane, to have to handle all the different complex commands and gears, while the virtual flying experience would educated people on how it was to actually fly the plane, understand its speed and performance, why it was so useful during the war. What should be integrated into the simulator for maximized education? : If the museum already exposes a model of a Spitfire, it is not necessary to replicate the entire plane, but only the seat and cockpit parts. Generally speaking, the front part of the plane, from pilot seat to nose, is enough for physical fidelity of the plane. While most of the parts of the simulator can use practical and cost-effective materials, the most important part of it is the cockpit. When participants sit inside the plane, they must witness the complexity of the Spitfire cockpit to feel and relate to the pilot's field of view. Of course, among the many elements of the cockpit, only some of them would be actually functional and connected to the simulator. For even more immersion and physical fidelity, no screen to represent the gauges, instead use fake physical gauges. Concerning the nose of the plane and its helices, it is obvious that making them actually turn would be very dangerous to the public. To overcome that problem, a glass could be put to prevent people from reaching to the helices. If possible, turning the helices would really improve fidelity (think of a big fan in front of the plane). Finally, in front of the simulator, a large screen simulating the flight virtually. The cockpit should be able to open/close. The focus here is to really make them feel that they ARE in a spitfire, reproduced but physically recreating a real experience. From this point, there should be a choice between trying the liftoff (complex, requires caution) or be directly set in the air (for younger or older people). A helmet of a pilot should be available for people to really fit in the character of a pilot in the WWII. A question of ethics is raised: should the virtual simulation include elements from the war ? (other planes, explosions etc). For the fidelity, this would greatly improve the educational aspect of the simulator. For younger and fragile it could be different. Once again there should be a mode as option between \"quiet\" (just flying) and \"war\" (explosions, parachutes, storm, attacks).","title":"Simulation and Digital Human Modeling"},{"location":"simulation-and-digital-human-modeling/#simulation-and-digital-human-modeling","text":"","title":"Simulation and Digital Human Modeling"},{"location":"simulation-and-digital-human-modeling/#lecture-19102018","text":"Important : Course work 1 is due Friday 9th November 2018 . Read slides from lecture Chosen subjects: Motorcycle manufacturer (simple seat moving right/left + screen to have longer tests ; we don't really want driving experience fidelity, we are interested in a reliable system, providing a decent simulation (just a screen) with minimum sickness effects. We want the testers to be able to stay on the fake bike for a long time to test the comfort. The fake bike would be composed of three key elements: the seat, the handlebar and footplate/gears, all those can be configured to different positions depending on the bike model being simulated). Aviation museum (we want more aspect focused on physical aspects of the historical spitfire plane, a physical fidelity, while giving an entertaining and educational flight experience with minimum SICKNESS (lots of people in museum, sample is large, young, old, fragile..etc) so no motion, only visual).","title":"Lecture (19/10/2018)"},{"location":"simulation-and-digital-human-modeling/#coursework-1-research","text":"","title":"Coursework 1: RESEARCH"},{"location":"simulation-and-digital-human-modeling/#motorcycle-manufacturer","text":"#counter-steering-is-important #highways-are-painful-whatever-the-bike Objective : Improving sports motorbikes physical comfort with the use of a simulator. What defines a comfortable motorbike? : A motorbike that, during and after a prolonged use, does not lead to any form of physical pain or discomfort, and does not affect the general well being of the rider in ways such as chronic pain, back distortion, muscular cramp and many more. What is the population targeted? : Accustomed motorbike riders, familiar with the notion of comfort and discomfort on a bike, capable of judging accordingly to experience in short, medium and long trips with a motorbike as well as general knowledge and interest in the type of motorbike being tested (maximize data). What parts of the bike affect comfort? : The main part of the bike responsible for comfort or discomfort is the seat. The human body is not made for staying still on the same part of the body for prolonged periods of time. Because of decreased supply of blood to the skin, the area that is being compressed for a long time starts to ache (comparison to sub-consciously turn over while sleeping). This principle implies that for a rider to feel comfortable on a motorbike for a long period of time, he needs to be able to move and change positions on the seat. That includes moving forward, backward, widening the legs opening, relieving the weight by pressing the tank..etc. For example, a seat angled forward will push the rider into the lowest spot, always putting pressure on the same parts of his body. Similarly, a seat too soft will seem comfortable at first, but with longer trips the rider might pressure through and hit the plastic base pan of the bike. In addition, the position and size of both the handlebar and the footplate/gears can also affect the rider's comfort. Of course, comfort and discomfort vary a lot from a rider to another one. Weight, size, general morphology, posture and habits will also influence his experience of comfort on a motorbike. Finally, the impact of the vibrations, the road quality, the speed and forces implied in a motorbike experience is significant (suspensions, vibrations suppression). (we consider here that as sports motorcycles, no additional significant weights are added to the rider). What are the relevant aspects for a simulator to achieve comfort evaluation? : If the primary objective is to test the comfort of a bike, the main goal of the simulator is to present the physical configuration of the motorbike being tested (seat, handlebar position, footplate/gears position) with a virtual riding experience entertaining enough to motivate the participant to stay on the bike for a prolonged period of time. That entertaining riding experience does not need to have a high fidelity to the real-world riding, as the goal is only to test the physical comfort of the motorbike (the visual aspect is not indispensable). There are no transfer of training to teach how to ride, or to improve skills, but only a test on the impact of the motorbike on the participant's physical comfort. However, that entertaining-enough riding experience, to last a longer period of time, needs to present minimum sickness effects. In addition, the physical configuration of the motorbike needs to present a very import aspect of riding: the leaning motion. Even with fidelity not being the principal goal, a leaning mechanism is absolutely necessary for any real-world rider to have a minimal replicating experience of a real motorbike. Finally, the leaning motion influences the position of the rider on the motorbike, which influences his comfort. Expecting a rider to test a seat with a motorbike constantly remaining in a straight line would be a complete lack of validity. What resources and materials are necessary? : We assume that manufacturers desire a simulator capable of adapting to any motorcycle model they are designing and/or already selling. A first way to implement such an adaptability is to build a system where an entire motorcycle can be \"locked-in\" and plugged in the rest of the simulator. This implementation raises many issues: compromising an entire motorcycle when most of its parts won't be relevant for the simulation ; every change of model requires to re-do the cabling and sensing systems for the handlebars steering, acceleration grips, and eventually gears if they are integrated to the simulation ; finally it would require that the motorbike is already designed (put together). On the other hand, building a system that only includes the key physical element of a motorbike that actually interact with the rider, will prove more efficient. All cabling/sensing will be definitive, and to change the simulator to a new model would simply require to modify the system's configuration handlebar position, footplate/grip position, gaz tank size/position, and of course a simple replacement of the seat and its position. (additional variables such as weight of the motorcycle, wheel width and more can be manipulated with the motion system and force applied). How can the utility be controlled? : By first targeting experienced riders, the experiment benefit from specific user groups which will maximize and improve the data. Concerning the control over the variables, the key elements of the bike would entirely be configurable to the desired position and setup. As well, software would allow simple manipulation of the force in motion movements (leaning). Of course, the experiment can be repeated as many times as wanted. What is the degree of validity regarding the comfort/discomfort? : Comfort and discomfort vary from the configuration and design of the bike, the characteristics of the rider and the physical impact of driving on the road (vibrations, forces). Regarding the design of the motorbike, the degree of validity is very high as it will present all relevant physical parts of the motorbike. The characteristics of the rider (morphology, physical condition, habits, posture) are entirely identical to the real-world. Finally, the real-world impact of the road can not be represented in any simulator, as it is only a simulator. Only the force of leaning motion and vibrations can be simulated. The overall degree of validity is therefore high, as the design of the motorbike itself is very important in the comfort of the rider. In addition, the important factor of leaning brings more validity as it would allow the rider to be in more diverse and realistic postures. What is the degree of fidelity regarding the general experience of driving? : As the goal of this simulator is not to train, entertain or experiment on driving factors and human behaviors, the fidelity of the general driving experience does not need to be high. To obtain a simulator with such fidelity, cost and technology are aspects to greatly improve: virtual reality, advanced graphics, advanced study of complex driving phenomenons (counter-steering), perfect synchronization to avoid sickness effects, and general software improvements. The fidelity needs to just enough to maintain the participant motivated (entertained, interested and challenged) while avoiding sickness. The fidelity highly relies on how long participants stay on the bike. If they can't stay long enough to test the comfort on extended period of time, fidelity needs to be elevated. What are the costs for such a simulator : Ideally, a first version of the simulator is implemented with only necessary features to offer a basic driving experience with a high validity regarding physical comfort/discomfort. From that version, fidelity has to be improved until the participants are motivated enough to remain longer on the motorbike. Whare are the limitations? : Regarding the comfort/discomfort, this simulator concept lacks the important effect of the road, the suspensions' importance etc. It only takes in account the participant and the motorbike, with basic leaning forces and motion. In addition, the fidelity may be too low on the first versions for prolonged sessions.","title":"Motorcycle manufacturer"},{"location":"simulation-and-digital-human-modeling/#aviation-museum","text":"Objective : Enhance educational impact of museums for the Spitfire plane from WWII flight experience using a simulator. What is the population targeted : No specific population is targeted. This simulator is for the general public: children, adults, elderly, any individual visiting the museum. What is the Spitfire? : Spitfire, also called Supermarine Spitfire, the most widely produced and strategically important British single-seat fighter of World War II. It was designed in response to a 1934 Air Ministry specification calling for a high-performance fighter with an armament of eight wing-mounted 0.303-inch machine guns. It had a stressed-skin aluminum structure and a graceful elliptical wing with a thin airfoil that gave it exceptional performance at high altitudes. It was designed as a short-range, high-performance interceptor aircraft. What elements from the Spitfire are relevant for general public education? : The Spitfire is iconic in the WWII, both for its physical shape that is very famous and the role it played in the war. Educating the general public about how flying with a Spitfire with a simulator implies two aspects: the plane in itself (cockpit, materials, technical elements) and the virtual flying experience (screen, speed, flight simulator, forces). The plane in itself replicated with precision would educated the public on what it felt like to sit in that plane, to have to handle all the different complex commands and gears, while the virtual flying experience would educated people on how it was to actually fly the plane, understand its speed and performance, why it was so useful during the war. What should be integrated into the simulator for maximized education? : If the museum already exposes a model of a Spitfire, it is not necessary to replicate the entire plane, but only the seat and cockpit parts. Generally speaking, the front part of the plane, from pilot seat to nose, is enough for physical fidelity of the plane. While most of the parts of the simulator can use practical and cost-effective materials, the most important part of it is the cockpit. When participants sit inside the plane, they must witness the complexity of the Spitfire cockpit to feel and relate to the pilot's field of view. Of course, among the many elements of the cockpit, only some of them would be actually functional and connected to the simulator. For even more immersion and physical fidelity, no screen to represent the gauges, instead use fake physical gauges. Concerning the nose of the plane and its helices, it is obvious that making them actually turn would be very dangerous to the public. To overcome that problem, a glass could be put to prevent people from reaching to the helices. If possible, turning the helices would really improve fidelity (think of a big fan in front of the plane). Finally, in front of the simulator, a large screen simulating the flight virtually. The cockpit should be able to open/close. The focus here is to really make them feel that they ARE in a spitfire, reproduced but physically recreating a real experience. From this point, there should be a choice between trying the liftoff (complex, requires caution) or be directly set in the air (for younger or older people). A helmet of a pilot should be available for people to really fit in the character of a pilot in the WWII. A question of ethics is raised: should the virtual simulation include elements from the war ? (other planes, explosions etc). For the fidelity, this would greatly improve the educational aspect of the simulator. For younger and fragile it could be different. Once again there should be a mode as option between \"quiet\" (just flying) and \"war\" (explosions, parachutes, storm, attacks).","title":"Aviation museum"},{"location":"studying-human-performance/","text":"Studying Human Performance Introduction lecture (02/10/2018) Homework for 09/10/28 : Workbook Chapters 2, 3 and 4 Lecture (05/10/2018) Chapter 1 Evaluation of HW Lecture (09/10/2018) Variance (S\u00b2) $$ S^2 = \\frac{\\sum (x - \\bar{x})^2}{N - 1} $$ Standard Deviation (SD) Perfect description of an entire distribution $$ S = \\sqrt{\\frac{\\sum (x - \\bar{x})^2}{N - 1}} $$ Z scores Used to perform calculations on normally distributed data A Z score is the distance from the mean measured in units of Standard Deviation $$ Z = \\frac{x - \\bar{x}}{SD} $$ Homework for 16/10/2018 : Workbook Chapter 5 Lecture (16/10/2018) WB 5.8: Experimental design discussion activity IV : Various pieces of information about the employees ; Task difficulty ; Attitude measure accuracy DV : Attitude score ; Time to complete Task H1 : The training scheme has an impact on the operators' performances H0 : The training scheme has no impact on the operators' performances ST : ANOVA Lecture (19/10/2018) Repeated measures design = Within Simple two-group design = between TO DO : sum up the example questions into r\u00e9sum\u00e9 TO DO : Coursework 1 due 22 November 2018 Lecture (23/10/2018) Wilcoxon Mann-Whitney Lecture (30/10/2018) TODO: program non-parametrics","title":"Studying Human Performance"},{"location":"studying-human-performance/#studying-human-performance","text":"","title":"Studying Human Performance"},{"location":"studying-human-performance/#introduction-lecture-02102018","text":"Homework for 09/10/28 : Workbook Chapters 2, 3 and 4","title":"Introduction lecture (02/10/2018)"},{"location":"studying-human-performance/#lecture-05102018","text":"Chapter 1 Evaluation of HW","title":"Lecture (05/10/2018)"},{"location":"studying-human-performance/#lecture-09102018","text":"","title":"Lecture (09/10/2018)"},{"location":"studying-human-performance/#variance-s2","text":"$$ S^2 = \\frac{\\sum (x - \\bar{x})^2}{N - 1} $$","title":"Variance (S\u00b2)"},{"location":"studying-human-performance/#standard-deviation-sd","text":"Perfect description of an entire distribution $$ S = \\sqrt{\\frac{\\sum (x - \\bar{x})^2}{N - 1}} $$","title":"Standard Deviation (SD)"},{"location":"studying-human-performance/#z-scores","text":"Used to perform calculations on normally distributed data A Z score is the distance from the mean measured in units of Standard Deviation $$ Z = \\frac{x - \\bar{x}}{SD} $$ Homework for 16/10/2018 : Workbook Chapter 5","title":"Z scores"},{"location":"studying-human-performance/#lecture-16102018","text":"","title":"Lecture (16/10/2018)"},{"location":"studying-human-performance/#wb-58-experimental-design-discussion-activity","text":"IV : Various pieces of information about the employees ; Task difficulty ; Attitude measure accuracy DV : Attitude score ; Time to complete Task H1 : The training scheme has an impact on the operators' performances H0 : The training scheme has no impact on the operators' performances ST : ANOVA","title":"WB 5.8: Experimental design discussion activity"},{"location":"studying-human-performance/#lecture-19102018","text":"Repeated measures design = Within Simple two-group design = between TO DO : sum up the example questions into r\u00e9sum\u00e9 TO DO : Coursework 1 due 22 November 2018","title":"Lecture (19/10/2018)"},{"location":"studying-human-performance/#lecture-23102018","text":"","title":"Lecture (23/10/2018)"},{"location":"studying-human-performance/#wilcoxon","text":"","title":"Wilcoxon"},{"location":"studying-human-performance/#mann-whitney","text":"","title":"Mann-Whitney"},{"location":"studying-human-performance/#lecture-30102018","text":"TODO: program non-parametrics","title":"Lecture (30/10/2018)"}]}